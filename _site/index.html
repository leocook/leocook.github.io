<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>leocook</title>
    <meta name="description" content="">

    <link rel="shortcut icon" href="/favicon.ico?" type="image/x-icon">
    <link rel="icon" href="/favicon.ico?" type="image/x-icon">
    <link rel="stylesheet" href=" /css/fontawesome/css/font-awesome.min.css ">
    <link rel="stylesheet" href=" /css/main.css ">
    <link rel="canonical" href="http://leocook.github.io/">
    <link rel="alternate" type="application/rss+xml" title="leocook" href="http://leocook.github.io /feed.xml ">


    <script>
    // 百度统计代码
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?f1ad6f37c7565a0fbaf172ac83132650";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
    </script>


</head>


  <body>

    <header>
    <div class="wrapper">
        <a href="/" class="brand">leocook</a>
        <small>Big-Data Dev Engineer</small>
        <button id="headerMenu" class="menu"><i class="fa fa-bars"></i></button>
        <nav id="headerNav">
            <ul>
                <li>
                    
                    <a class="active" href="/">
                        
                        <i class="fa fa-home"></i>Home
                    </a>
                </li>

                
                    
                    <li>
                        
                        <a href="/archive/">
                        
                            <i class="fa fa-archive"></i>Archives
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/category/">
                        
                            <i class="fa fa-th-list"></i>Categories
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/tag/">
                        
                            <i class="fa fa-tags"></i>Tags
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/collection/">
                        
                            <i class="fa fa-bookmark"></i>Collections
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/about/">
                        
                            <i class="fa fa-heart"></i>About
                        </a>
                    </li>
                    
                
                    
                
                    
                
                    
                
            </ul>
        </nav>
    </div>
</header>


        <div class="page clearfix" index>
    <div class="left">
        <h2>跨界的IT博客 | hadoop | 大数据 | spark</h2>
        <small>这里将会记录着我在互联网行业里走过的每一步。</small>
        <hr>
        <ul>
            
              <li>
                <h2>
                  <a class="post-link" href="/2016/05/09/Tez%E7%B3%BB%E5%88%97%E7%AC%AC%E4%BA%8C%E7%AF%87-hive_on_tez/">Tez系列第二篇 Hive_on_tez</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2016-05-09
                    </div>
                    <div class="label-card">
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#Tez" title="Category: Tez" rel="category">Tez</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#hadoop" title="Tag: hadoop" rel="tag">hadoop</a>&nbsp;
    
        <a href="/tag/#Tez" title="Tag: Tez" rel="tag">Tez</a>&nbsp;
    
        <a href="/tag/#Hive" title="Tag: Hive" rel="tag">Hive</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <ul id="markdown-toc">
  <li><a href="#tez" id="markdown-toc-tez">1.安装配置Tez</a>    <ul>
      <li><a href="#section" id="markdown-toc-section">1.1.环境要求</a></li>
      <li><a href="#section-1" id="markdown-toc-section-1">1.2.集群准备</a></li>
      <li><a href="#section-2" id="markdown-toc-section-2">1.3. 编译环境准备</a></li>
      <li><a href="#nodejsnpm" id="markdown-toc-nodejsnpm">1.4. Nodejs、npm</a></li>
      <li><a href="#git" id="markdown-toc-git">1.5.安装GIT</a></li>
      <li><a href="#protocolbuffer250" id="markdown-toc-protocolbuffer250">1.6. ProtocolBuffer2.5.0</a></li>
      <li><a href="#tez-1" id="markdown-toc-tez-1">1.7.编译&amp;安装Tez</a></li>
    </ul>
  </li>
  <li><a href="#hivetez" id="markdown-toc-hivetez">2. 开始整合Hive和Tez</a>    <ul>
      <li><a href="#section-3" id="markdown-toc-section-3">2.1. 查看编译完成的目标目录结构</a></li>
      <li><a href="#tez-082-minimalhdfs" id="markdown-toc-tez-082-minimalhdfs">2.1. 拷贝tez-0.8.2-minimal目录至HDFS</a></li>
      <li><a href="#jar" id="markdown-toc-jar">2.2. 拷贝对应以来jar</a></li>
      <li><a href="#tez-082" id="markdown-toc-tez-082">2.3. 把tez-0.8.2拷贝到服务器本地部署的目录</a></li>
      <li><a href="#conftez-sitexml" id="markdown-toc-conftez-sitexml">2.4. 进入部署的目录创建conf/tez-site.xml</a></li>
      <li><a href="#tez-2" id="markdown-toc-tez-2">2.4. 把Tez加入到环境变量</a></li>
      <li><a href="#hivetez-1" id="markdown-toc-hivetez-1">2.5. 让Hive把Tez用起来</a></li>
    </ul>
  </li>
  <li><a href="#section-4" id="markdown-toc-section-4">常见的错误</a>    <ul>
      <li><a href="#sudotez" id="markdown-toc-sudotez">1.不要使用sudo权限来编译（编译tez时的错误）</a></li>
      <li><a href="#mavenfrontend-maven-plugintez" id="markdown-toc-mavenfrontend-maven-plugintez">2.maven插件frontend-maven-plugin的版本问题（编译tez时的错误）</a></li>
      <li><a href="#nodetez" id="markdown-toc-nodetez">3.解压Node压缩包时错误（编译tez时的错误）</a></li>
      <li><a href="#nodenpmtez" id="markdown-toc-nodenpmtez">4.node&amp;npm版本不对应（编译tez时的错误）</a></li>
      <li><a href="#mrerror-when-run-hive-on-tez" id="markdown-toc-mrerror-when-run-hive-on-tez">5.缺少MR的依赖包（error when run hive on tez）</a></li>
      <li><a href="#tezhive-on-oozie-" id="markdown-toc-tezhive-on-oozie-">6.tez&amp;hive on oozie 错误</a></li>
    </ul>
  </li>
</ul>

<h2 id="tez">1.安装配置Tez</h2>

<h3 id="section">1.1.环境要求</h3>

<ul>
  <li>CDH5.4.4(hadoop2.6.0)</li>
  <li>编译环境：gcc, gcc-c++, make, build</li>
  <li>Nodejs、npm (Tez-ui需要)</li>
  <li>Git</li>
  <li>pb2.5.0</li>
  <li>maven3</li>
  <li>Tez0.8.2</li>
</ul>

<h3 id="section-1">1.2.集群准备</h3>
<p>以及安装完成的cdh5.4.4集群。</p>

<h3 id="section-2">1.3. 编译环境准备</h3>
<p>安装gcc, gcc-c++, make, build  <br />
<code class="highlighter-rouge">
yum install gcc gcc-c++ libstdc++-devel make build
</code></p>

<h3 id="nodejsnpm">1.4. Nodejs、npm</h3>
<p>下载源码:  <br />
<code class="highlighter-rouge">
wget http://nodejs.org/dist/v0.8.14/node-v0.8.14.tar.gz
</code></p>

<p>解压后编译:  <br />
<code class="highlighter-rouge">
./configure
make &amp;&amp; make install
</code></p>

<p>查看nodejs和npm的版本:  <br />
<code class="highlighter-rouge">
node --version
npm --version
</code></p>

<p>笔者的安装环境里，node的版本是v0.12.9，npm的版本是2.14.9</p>

<h3 id="git">1.5.安装GIT</h3>
<p>下载git:  <br />
<code class="highlighter-rouge">
https://git-scm.com/download    
笔者选择的是1.7.3版本
</code></p>

<p>解压后编译:  <br />
<code class="highlighter-rouge">
./configure
make
make install
</code></p>

<h3 id="protocolbuffer250">1.6. ProtocolBuffer2.5.0</h3>
<ul>
  <li>下载pb源码  <br />
<code class="highlighter-rouge">
https://github.com/google/protobuf/releases/tag/v2.5.0
</code></li>
  <li>
    <p>编译安装pb  <br />
<code class="highlighter-rouge">
./configure -prefix=/opt/protoc/
make &amp;&amp; make install
</code></p>
  </li>
  <li>配置环境变量 <br />
<code class="highlighter-rouge">
export PROTOC_HOME=/opt/protoc
export PATH=$PATH:$PROTOC_HOME/bin
</code></li>
  <li>检验  <br />
<code class="highlighter-rouge">
protoc --version
</code>  <br />
查看pb的版本是不是2.5.0，笔者这里显示为 <strong>libprotoc 2.5.0</strong></li>
</ul>

<h3 id="tez-1">1.7.编译&amp;安装Tez</h3>
<ul>
  <li>下载Tez <br />
Tez所有版本列表在者：</li>
</ul>

<div class="highlighter-rouge"><pre class="highlight"><code>http://tez.apache.org/releases/index.html
</code></pre>
</div>
<p>笔者这里下载的是0.8.2版本。</p>

<ul>
  <li>解压修改配置</li>
</ul>

<p>vi pom.xml</p>

<div class="highlighter-rouge"><pre class="highlight"><code>&lt;hadoop.version&gt;2.6.0-cdh5.4.4&lt;/hadoop.version&gt;
</code></pre>
</div>

<p>vi tez-ui/pom.xml</p>

<div class="highlighter-rouge"><pre class="highlight"><code>&lt;nodeVersion&gt;v0.12.9&lt;/nodeVersion&gt;
&lt;npmVersion&gt;2.14.9&lt;/npmVersion&gt;
</code></pre>
</div>

<ul>
  <li>开始编译</li>
</ul>

<div class="highlighter-rouge"><pre class="highlight"><code>mvn clean package -DskipTests=true -Dmaven.javadoc.skip=true  -Dfrontend-maven-plugin.version=0.0.23
</code></pre>
</div>

<blockquote>
  <p>编译的过程中可能会发生错误，我这边由于网络故障，经常会出现node.gz.tar文件下载失败。最后还是编译成功了。</p>
</blockquote>

<h2 id="hivetez">2. 开始整合Hive和Tez</h2>

<h3 id="section-3">2.1. 查看编译完成的目标目录结构</h3>
<div class="highlighter-rouge"><pre class="highlight"><code>[wulin@lf-R710-29 target]$ ls
archive-tmp  maven-archiver  tez-0.8.2  tez-0.8.2-minimal  tez-0.8.2-minimal.tar.gz  tez-0.8.2.tar.gz  tez-dist-0.8.2-tests.jar
</code></pre>
</div>

<h3 id="tez-082-minimalhdfs">2.1. 拷贝tez-0.8.2-minimal目录至HDFS</h3>
<div class="highlighter-rouge"><pre class="highlight"><code>hdfs dfs -put tez-0.8.2-minimal /tmp/tez-dir/
</code></pre>
</div>
<p>先拷贝到tmp目录做测试，成功运行后在拷贝到正式目录。</p>

<h3 id="jar">2.2. 拷贝对应以来jar</h3>
<p>把hadoop-mapreduce-client-common-2.6.0-cdh5.4.4.jar到hdfs的/tmp/tez-dir/tez-0.8.2-minimal目录</p>

<h3 id="tez-082">2.3. 把tez-0.8.2拷贝到服务器本地部署的目录</h3>
<div class="highlighter-rouge"><pre class="highlight"><code>cp -r tez-0.8.2 /opt/
</code></pre>
</div>

<h3 id="conftez-sitexml">2.4. 进入部署的目录创建conf/tez-site.xml</h3>
<div class="highlighter-rouge"><pre class="highlight"><code><span class="cp">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span>
<span class="cp">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span>
<span class="nt">&lt;configuration&gt;</span>
 <span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>tez.lib.uris<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>${fs.defaultFS}/tmp/tez-dir/tez-0.8.2-minimal,${fs.defaultFS}/tmp/tez-dir/tez-0.8.2-minimal/lib<span class="nt">&lt;/value&gt;</span>
 <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>tez.use.cluster.hadoop-libs<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
 <span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
</code></pre>
</div>
<blockquote>
  <p>注：tez.lib.uris参数值，是之前上传到hdfs目录的tez包，它必须是tez-0.8.2-minimal目录，而不能是tez-0.8.2目录。（如果有谁使用tez-0.8.2目录部署成功的话，可以告诉我，谢谢！）。根据官网的说明，使用tez-0.8.2-minimal包的时候，务必设置tez.use.cluster.hadoop-libs属性为true。</p>
</blockquote>

<h3 id="tez-2">2.4. 把Tez加入到环境变量</h3>

<div class="highlighter-rouge"><pre class="highlight"><code>export TEZ_JARS=/opt/tez-0.8.2-minimal
export TEZ_CONF_DIR=$TEZ_JARS/conf
export HADOOP_CLASSPATH=${TEZ_CONF_DIR}:${TEZ_JARS}/*:${TEZ_JARS}/lib/*
</code></pre>
</div>
<blockquote>
  <p>注：经笔者的测试，TEZ_JARS指向tez-0.8.2-minimal目录或者tez-0.8.2目录都是可以的。</p>
</blockquote>

<h3 id="hivetez-1">2.5. 让Hive把Tez用起来</h3>
<ul>
  <li>配置整合
临时配置</li>
</ul>

<div class="highlighter-rouge"><pre class="highlight"><code>hive&gt;set hive.execution.engine=tez;
</code></pre>
</div>
<p>或者修改hive-site.xml（长期配置）</p>

<div class="highlighter-rouge"><pre class="highlight"><code> &lt;property&gt;
   &lt;name&gt;hive.execution.engine&lt;/name&gt;
   &lt;value&gt;tez&lt;/value&gt;
 &lt;/property&gt;
</code></pre>
</div>
<ul>
  <li>执行hive，验证查看</li>
</ul>

<div class="highlighter-rouge"><pre class="highlight"><code>hive&gt; select count(*) from dual;
Query ID = wulin_20160406152121_6fd704e7-a437-4345-9958-2fbd1cccb057
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=&lt;number&gt;
In order to set a constant number of reducers:
  set mapreduce.job.reduces=&lt;number&gt;
Starting Job = job_1457012272029_352465, Tracking URL = http://lfh-R710-165:8088/proxy/application_1457012272029_352465/
Kill Command = /opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/bin/hadoop job  -kill job_1457012272029_352465
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2016-04-06 15:21:29,925 Stage-1 map = 0%,  reduce = 0%
2016-04-06 15:21:38,274 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.05 sec
2016-04-06 15:21:45,611 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.86 sec
MapReduce Total cumulative CPU time: 3 seconds 860 msec
Ended Job = job_1457012272029_352465
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.86 sec   HDFS Read: 6138 HDFS Write: 2 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 860 msec
OK
1
Time taken: 35.934 seconds, Fetched: 1 row(s)
hive&gt; set hive.execution.engine=tez;
hive&gt; select count(*) from dual;
Query ID = wulin_20160406152222_426dd505-1f6a-4d02-ae95-5a4d0e6bbc76
Total jobs = 1
Launching Job 1 out of 1
Status: Running (Executing on YARN cluster with App id application_1457012272029_352467)
--------------------------------------------------------------------------------
        VERTICES      STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED
--------------------------------------------------------------------------------
Map 1 ..........   SUCCEEDED      1          1        0        0       0       0
Reducer 2 ......   SUCCEEDED      1          1        0        0       0       0
--------------------------------------------------------------------------------
VERTICES: 02/02  [==========================&gt;&gt;] 100%  ELAPSED TIME: 9.64 s     
--------------------------------------------------------------------------------
OK
1
Time taken: 22.211 seconds, Fetched: 1 row(s)
hive&gt; 
</code></pre>
</div>
<p>如下图：
<img src="http://7xriy2.com1.z0.glb.clouddn.com/tez-ok.png" alt="Hive on Tez" title="Hive on Tez" /></p>

<p>到此，hive on tez，整合完毕！</p>

<h2 id="section-4">常见的错误</h2>

<h3 id="sudotez">1.不要使用sudo权限来编译（编译tez时的错误）</h3>
<div class="highlighter-rouge"><pre class="highlight"><code>[ERROR] Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.3.2:exec (Bower install) on project tez-ui: Command execution failed. Process exited with an error: 1 (Exit value: 1) -&gt; [Help 1]
</code></pre>
</div>
<p>解决办法：
不要使用root用户，也不要使用sudo来编译.</p>

<h3 id="mavenfrontend-maven-plugintez">2.maven插件frontend-maven-plugin的版本问题（编译tez时的错误）</h3>

<div class="highlighter-rouge"><pre class="highlight"><code>[ERROR] Failed to execute goal com.github.eirslett:frontend-maven-plugin:0.0.22:install-node-and-npm (install node and npm) on project tez-ui: Execution install node and npm of goal com.github.eirslett:frontend-maven-plugin:0.0.22:install-node-and-npm failed: A required class was missing while executing com.github.eirslett:frontend-maven-plugin:0.0.22:install-node-and-npm: org/slf4j/helpers/MarkerIgnoringBase
</code></pre>
</div>
<p>解决办法：强制执行编译时frontend-maven-plugin插件的版本（mvn clean package -DskipTests=true -Dmaven.javadoc.skip=true  -Dfrontend-maven-plugin.version=0.0.XX）
如果maven的版本低于3.1，frontend-maven-plugin版本应该 &lt;= 0.0.22；
如果maven的版本大于或等于3.1，frontend-maven-plugin版本应该 &gt;= 0.0.23.</p>

<h3 id="nodetez">3.解压Node压缩包时错误（编译tez时的错误）</h3>

<div class="highlighter-rouge"><pre class="highlight"><code>[ERROR] Failed to execute goal com.github.eirslett:frontend-maven-plugin:0.0.22:install-node-and-npm (install node and npm) on project tez-ui: Could not extract the Node archive: Could not extract archive: '/home/.../tez/tez-ui/src/main/webapp/node_tmp/node.tar.gz': EOFException -&gt; [Help 1]
</code></pre>
</div>
<p>解决版本：检查第二个问题，并重新执行。如果还失败，可以多执行几次，可能和网络有关系。</p>

<h3 id="nodenpmtez">4.node&amp;npm版本不对应（编译tez时的错误）</h3>

<div class="highlighter-rouge"><pre class="highlight"><code>[ERROR] npm WARN engine hoek@2.16.3: wanted: {"node":"&gt;=0.10.40"} (current: {"node":"v0.10.18","npm":"1.3.8"})
[ERROR] npm WARN engine boom@2.10.1: wanted: {"node":"&gt;=0.10.40"} (current: {"node":"v0.10.18","npm":"1.3.8"})
[ERROR] npm WARN engine cryptiles@2.0.5: wanted: {"node":"&gt;=0.10.40"} (current: {"node":"v0.10.18","npm":"1.3.8"})
</code></pre>
</div>
<p>解决办法：
1. 安装正确版本的nodeJs；
2. 修改tez-ui/pom.xml中的nodeVersion和npmVersion标签值为系统环境的值。可使用下面命令，查看系统里的node和npm版本：
<code class="highlighter-rouge">
node --version
npm --version
</code></p>

<h3 id="mrerror-when-run-hive-on-tez">5.缺少MR的依赖包（error when run hive on tez）</h3>
<div class="highlighter-rouge"><pre class="highlight"><code>Vertex failed, vertexName=Map 1, vertexId=vertex_1457012272029_352429_1_00, diagnostics=[Vertex vertex_1457012272029_352429_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: dual initializer failed, vertex=vertex_1457012272029_352429_1_00 [Map 1], java.lang.NoClassDefFoundError: org/apache/hadoop/mapred/MRVersion
        at org.apache.hadoop.hive.shims.Hadoop23Shims.isMR2(Hadoop23Shims.java:843)
        at org.apache.hadoop.hive.shims.Hadoop23Shims.getHadoopConfNames(Hadoop23Shims.java:914)
        at org.apache.hadoop.hive.conf.HiveConf$ConfVars.&lt;clinit&gt;(HiveConf.java:356)
        at org.apache.hadoop.hive.ql.exec.Utilities.getBaseWork(Utilities.java:371)
        at org.apache.hadoop.hive.ql.exec.Utilities.getMapWork(Utilities.java:296)
        at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:106)
        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:278)
        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:269)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:269)
        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:253)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.mapred.MRVersion
        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
        ... 17 more
]
Vertex killed, vertexName=Reducer 2, vertexId=vertex_1457012272029_352429_1_01, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1457012272029_352429_1_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]
DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask
</code></pre>
</div>
<p>这个错误是在配置完之后，运行hive时才会出现的。
解决办法：
拷贝mr依赖包至tez的hdfs目录中。笔者的环境是CDH5.4.4，所以把hadoop-mapreduce-client-common-2.6.0-cdh5.4.4.jar拷贝到hdfs的/tmp/tez-dir/tez-0.8.2-minimal目录，就解决问题了。</p>

<h3 id="tezhive-on-oozie-">6.tez&amp;hive on oozie 错误</h3>

<div class="highlighter-rouge"><pre class="highlight"><code>Status: Running (Executing on YARN cluster with App id application_1461470184587_0770)

Map 1: -/-	Reducer 2: 0/1	
Status: Failed
Vertex failed, vertexName=Map 1, vertexId=vertex_1461470184587_0770_1_00, diagnostics=[Vertex vertex_1461470184587_0770_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: wl_manager_core_assembly initializer failed, vertex=vertex_1461470184587_0770_1_00 [Map 1], java.lang.IllegalArgumentException: Illegal Capacity: -1
	at java.util.ArrayList.&lt;init&gt;(ArrayList.java:142)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:330)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:306)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:408)
	at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:129)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:278)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:269)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:269)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:253)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
]
Vertex killed, vertexName=Reducer 2, vertexId=vertex_1461470184587_0770_1_01, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1461470184587_0770_1_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]
DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask
Intercepting System.exit(2)
Failing Oozie Launcher, Main class [org.apache.oozie.action.hadoop.HiveMain], exit code [2]
</code></pre>
</div>

<p>参考链接：
http://m.oschina.net/blog/421764  <br />
http://duguyiren3476.iteye.com/blog/2214549
https://cwiki.apache.org/confluence/display/TEZ/Build+errors+and+solutions</p>


                </div>
                <div class="read-all">
                    <a  href="/2016/05/09/Tez%E7%B3%BB%E5%88%97%E7%AC%AC%E4%BA%8C%E7%AF%87-hive_on_tez/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2016/04/01/Tez%E7%B3%BB%E5%88%97%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E5%B8%B8%E8%AF%86/">Tez系列第一篇 基础常识</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2016-04-01
                    </div>
                    <div class="label-card">
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#Tez" title="Category: Tez" rel="category">Tez</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#hadoop" title="Tag: hadoop" rel="tag">hadoop</a>&nbsp;
    
        <a href="/tag/#Tez" title="Tag: Tez" rel="tag">Tez</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <ul id="markdown-toc">
  <li><a href="#tez" id="markdown-toc-tez">1.Tez是什么</a>    <ul>
      <li><a href="#section" id="markdown-toc-section">1.1.介绍</a></li>
      <li><a href="#section-1" id="markdown-toc-section-1">1.2.两大优势</a></li>
    </ul>
  </li>
  <li><a href="#tez-1" id="markdown-toc-tez-1">2.为什么要有Tez</a>    <ul>
      <li><a href="#yarnam" id="markdown-toc-yarnam">2.1.YARN的AM</a></li>
      <li><a href="#yarn" id="markdown-toc-yarn">2.2.YARN的资源无法重用</a></li>
      <li><a href="#yarndag" id="markdown-toc-yarndag">2.3.YARN的DAG中间计算结果读写效率低下</a></li>
    </ul>
  </li>
  <li><a href="#tez-2" id="markdown-toc-tez-2">3.Tez能解决什么问题</a>    <ul>
      <li><a href="#amamampoolserver" id="markdown-toc-amamampoolserver">3.1.使用AM缓冲池实现AM的复用，AMPoolServer</a></li>
      <li><a href="#container" id="markdown-toc-container">3.2.Container预启动</a></li>
      <li><a href="#container-1" id="markdown-toc-container-1">3.3.Container重用</a></li>
    </ul>
  </li>
</ul>

<p>本文主要围绕着这么几个问题来展开：Tez是什么？为什么要有Tez？Tez能解决什么问题？</p>

<h2 id="tez">1.Tez是什么</h2>

<h3 id="section">1.1.介绍</h3>
<p>Tez目标是用来构建复杂的有向五环图数据处理程序。Tez项目目前是构建在YARN之上的。详情可以查看Tez的官网：http://tez.apache.org/</p>

<h3 id="section-1">1.2.两大优势</h3>
<p><strong>用户体验</strong>  <br />
- 使用API来自定义数据流  <br />
- 灵活的Input-Processor-Output运行模式  <br />
- 与计算的数据类型无关  <br />
- 简单的部署流程</p>

<p><strong>计算性能</strong>  <br />
- 性能高于MapReduce  <br />
- 资源管理更加优化  <br />
- 运行时配置预加载  <br />
- 物理数据流动态运行</p>

<p><strong>举例</strong>  <br />
下图是一个基于MR的Hive/Pig的DAG数据流处理过程:  <br />
<img src="http://7xriy2.com1.z0.glb.clouddn.com/tez01-PigHiveQueryOnMR.png" alt="Hive/Pig" title="Hive/Pig的DAG" /></p>

<p>下图是一个基于Tez的Hive/Pig的DAG数据流处理过程:  <br />
<img src="http://7xriy2.com1.z0.glb.clouddn.com/tez02-PigHiveQueryOnTez.png" alt="Hive/Pig" title="Hive/Pig的DAG" /></p>

<h2 id="tez-1">2.为什么要有Tez</h2>

<h3 id="yarnam">2.1.YARN的AM</h3>

<p>YARN的每个作业在执行前都会先创建一个AM，然后才会开始正真的计算。这样处理小作业的时候，会有较大的延迟，而且还会造成极大的性能浪费。</p>

<h3 id="yarn">2.2.YARN的资源无法重用</h3>
<p>在MR1中，用户可以开启JVM重用，用来降低作业延迟。
但是在YARN中，每个作业的AM会先向RM申请资源（Container），申请到资源之后开始运行作业，作业处理完成后释放资源，期间没有资源重新利用的环节。这样会使作业大大的延迟。</p>

<h3 id="yarndag">2.3.YARN的DAG中间计算结果读写效率低下</h3>
<p>可以查看1.2中的图“<strong>基于MR的Hive/Pig的DAG数据流处理过程</strong>”，可以看出图中的每一节点都是把结果写到一个中间存储（HDFS/S3）中，下个节点从中间存储读取数据，再来继续接下来的计算。可见中间存储的读写性能对整个DAG的性能影响是很大的。  <br />
如果使用Tez，则可以省去中间存储的读写，上个节点的输出可以直接重定向到下个节点的输入。</p>

<h2 id="tez-2">3.Tez能解决什么问题</h2>

<h3 id="amamampoolserver">3.1.使用AM缓冲池实现AM的复用，AMPoolServer</h3>

<p>使用Tez后，yarn的作业不是先提交给RM了，而是提交给AMPS。AMPS在启动后，会预先创建若干个AM，作为AM资源池，当作业被提交到AMPS的时候，AMPS会把该作业直接提交到AM上，这样就避免每个作业都创建独立的AM，大大的提高了效率。</p>

<h3 id="container">3.2.Container预启动</h3>
<p>AM缓冲池中的每个AM在启动时都会预先创建若干个container，以此来减少因创建container所话费的时间。</p>

<h3 id="container-1">3.3.Container重用</h3>
<p>每个任务运行完之后，AM不会立马释放Container，而是将它分配给其它未执行的任务。  <br />
看到这里， Tez是什么？为什么要有Tez？Tez能解决什么问题？应该都知道了吧！下一篇来开始讲解正式环境中的使用。</p>


                </div>
                <div class="read-all">
                    <a  href="/2016/04/01/Tez%E7%B3%BB%E5%88%97%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E5%B8%B8%E8%AF%86/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2016/03/13/hadoop%E4%BC%98%E5%8C%96-yarn/">Hadoop优化 Yarn</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2016-03-13
                    </div>
                    <div class="label-card">
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#hadoop" title="Category: hadoop" rel="category">hadoop</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#hadoop" title="Tag: hadoop" rel="tag">hadoop</a>&nbsp;
    
        <a href="/tag/#集群优化" title="Tag: 集群优化" rel="tag">集群优化</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <ul id="markdown-toc">
  <li><a href="#section" id="markdown-toc-section">1.之前的集群存在的问题</a>    <ul>
      <li><a href="#section-1" id="markdown-toc-section-1">1.1.问题一：作业的执行速率不同步</a></li>
      <li><a href="#section-2" id="markdown-toc-section-2">1.2.问题二：资源利用倾斜</a></li>
      <li><a href="#section-3" id="markdown-toc-section-3">1.3.问题三：集群资源并没有真正的参与计算</a></li>
    </ul>
  </li>
  <li><a href="#section-4" id="markdown-toc-section-4">2.集群资源重新划分的过程</a>    <ul>
      <li><a href="#section-5" id="markdown-toc-section-5">2.1.拿到集群中所有机器的硬件资源列表</a></li>
      <li><a href="#section-6" id="markdown-toc-section-6">2.2.根据集群资源，分组的大概情况如截图：</a></li>
      <li><a href="#section-7" id="markdown-toc-section-7">2.3.资源划分的策略</a></li>
      <li><a href="#section-8" id="markdown-toc-section-8">2.4.具体的划分策略</a>        <ul>
          <li><a href="#section-9" id="markdown-toc-section-9">2.4.1.内存划分策略</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#section-10" id="markdown-toc-section-10">4.成果</a>    <ul>
      <li><a href="#section-11" id="markdown-toc-section-11">4.1.集群表现情况</a></li>
    </ul>
  </li>
  <li><a href="#section-12" id="markdown-toc-section-12">5.总结</a></li>
</ul>

<p>集群优化这块一直是一个比较麻烦的事情，目前由于集群的资源分配问题，已经出现了几次作业故障，有必要好这块的东西重新梳理一下。经过３天的测试，最终找到了对于目前环境相对适合的参数，目前集群已经11*24无节点故障了，先在这里做一些简单的分享吧。</p>

<h1 id="section">1.之前的集群存在的问题</h1>

<h2 id="section-1">1.1.问题一：作业的执行速率不同步</h2>
<ul>
  <li>问题表现
部分任务跑的慢，部分任务跑得快。</li>
  <li>问题的原因
集群资源分配不合理，出现有低配的机器运行作业数较多，高配机器运行作业数较少的情况。</li>
  <li>解决办法
重新分配角色组，使得低配机器参与相对较少的计算，高配机器参与相对较多的计算。</li>
</ul>

<h2 id="section-2">1.2.问题二：资源利用倾斜</h2>
<ul>
  <li>问题表现
部分机器资源利用率极高，部分机器资源利用率级低；</li>
  <li>问题的原因
<strong>同【问题一】</strong></li>
  <li>解决办法
<strong>同【问题一】</strong></li>
</ul>

<h2 id="section-3">1.3.问题三：集群资源并没有真正的参与计算</h2>
<ul>
  <li>问题表现
作业个数较多的时候，出现集群资源分配完了，但是集群负载极低，作业执行极缓慢。</li>
  <li>问题的原因
咱们的ETL结果报表使用的是单节点mysql，大量的小文件写操作使得磁盘的IO成为了严重的性能瓶颈，所以每个导数据的任务执行的较缓慢，导数据的作业长时间占用计算资源，计算任务执行的较为缓慢。</li>
  <li>解决办法
a). 把mysql中的数据库存到不同的磁盘上的，降低单个磁盘的负载。
b). 减少单个任务的资源占用，提高集群的并行度。</li>
</ul>

<h1 id="section-4">2.集群资源重新划分的过程</h1>

<h2 id="section-5">2.1.拿到集群中所有机器的硬件资源列表</h2>
<p>感谢运维同学的帮助！</p>

<h2 id="section-6">2.2.根据集群资源，分组的大概情况如截图：</h2>
<p><img src="http://7xriy2.com1.z0.glb.clouddn.com/%E5%88%86%E7%BB%84%E5%88%97%E8%A1%A8.png" alt="服务器分组情况" title="根据机器硬件资源情况，服务器分组情况" />
分组命名规则：
<strong><em>NM</em></strong>: NodeManager；  <br />
<strong><em>G01</em></strong>: Group01；  <br />
<strong><em>C08</em></strong>: cpu是8核；  <br />
<strong><em>M48</em></strong>: 内存是48GB。  <br />
<strong><em>ZK</em></strong>: 机器上安装了ZK，如果没有这一项，默认该机器上只安装了HbaseRegion Server、HDFS DataNode、Impala Daemon和Yarn NodeManager这四个角色。（如果某台机器上只安装了一个测试的zk，则可忽略该角色的资源占用，若该角色占用资源较多，那么就应该把这台机器单独拿出来分组）</p>

<h2 id="section-7">2.3.资源划分的策略</h2>
<p>根据机器上安装的服务，大概给服务做了如下的划分：
- 安装有重要服务的机器，可参与计算
例如安装了OOZIE、ResourcesManager、NodeManager的节点，当它们故障时，对集群来说，将可能会是一场灾难，所以不让这些机器参与计算，保证这些服务的稳定。</p>

<ul>
  <li>
    <p>安装有重要服务的机器，不参与计算
例如安装了FLUME、KAFKA或ZK的节点，由于它们本身就是可以配置分布式执行的，当其中一个服务出现故障时，对业务的影响是较小的，甚至没有。所以允许这些节点和参与计算的节点安装在同一台主机上。</p>
  </li>
  <li>
    <p>只安装了存储和计算的机器
例如HbaseRegion Server、HDFS DataNode、Impala Daemon和Yarn NodeManager，不会因为一台机器的故障导致集群出现灾难。</p>
  </li>
</ul>

<h2 id="section-8">2.4.具体的划分策略</h2>

<h3 id="section-9">2.4.1.内存划分策略</h3>
<ul>
  <li>yarn容器可直接管理的资源
主机中内存*0.8 - 7GB（Hbase）- 7GB（HDFS），具体根据集群规模，hdfs、hbase的环境来定。有的几点还安装了其它服务，具体需要观察集群环境。</li>
  <li>单个任务可使用的任务资源
map任务划分1GB，reduce任务划分2GB，JVM虚拟机分别设置为他们70%。
### 2.4.2.内存划分策略</li>
  <li>yarn容器可直接管理的资源
对于只安装了HRS、DN、ID、NM的节点，vcore总数设置为（cpu核数-1）的2倍，具体根据cpu的计算性能来定（减一时预留给系统的）。如果节点上安装了一些会消耗CPU的服务，那么就设置vcore总数为cpu核数/2。如果安装了一些对CPU消耗不是非常大的服务，例如ZK，那么就设置vcore总是为（cpu的核数-1）。</li>
  <li>单个任务可使用的任务资源
1个vcore。
# 3.mysql调优的过程
不同的库，挂载在不同的磁盘上，减小单块盘的压力。</li>
</ul>

<h1 id="section-10">4.成果</h1>

<h2 id="section-11">4.1.集群表现情况</h2>
<p>到目前为止，已超过72小时NodeManager未出现过故障了，待考察一周。
## 4.2.mysql表现情况
优化前的负载情况入下图：
<img src="http://7xriy2.com1.z0.glb.clouddn.com/mysql_befor.png" alt="优化前" title="优化前负载情况" />
优化后的负载情况入下图：
<img src="http://7xriy2.com1.z0.glb.clouddn.com/mysql.png" alt="优化后" title="优化后负载情况" /></p>

<blockquote>
  <p>后端导数据速度有明显加快，但是SDA盘的负载还是明显略高于SDB的负载。</p>
</blockquote>

<h1 id="section-12">5.总结</h1>
<p>在摸索这个问题上花费了比较多的时间，目前的优化方案满足现在的业务场景。在集群优化这方边，在个人现在能看到的未来，还有很多可以优化的项。例如：
- Impala的资源管理未使用yarn，所以一直还没有开始使用；
- OOZIE未做HA配置；
- HDFS数据平衡效果不是很好；
- CPU一个线程做两个vcore使用，压力还是比较大的。</p>

<p>在接下，将会按照优先级逐一解决。</p>


                </div>
                <div class="read-all">
                    <a  href="/2016/03/13/hadoop%E4%BC%98%E5%8C%96-yarn/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2015/02/23/hadoop%E4%BC%98%E5%8C%96-%E6%A6%82%E8%BF%B0/">Hadoop优化 概述</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2015-02-23
                    </div>
                    <div class="label-card">
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#hadoop" title="Category: hadoop" rel="category">hadoop</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#集群优化" title="Tag: 集群优化" rel="tag">集群优化</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <ul id="markdown-toc">
  <li><a href="#section" id="markdown-toc-section">1.应用程序角度进行优化</a>    <ul>
      <li><a href="#reduce" id="markdown-toc-reduce">1.1.减少不必要的reduce任务</a></li>
      <li><a href="#section-1" id="markdown-toc-section-1">1.2.外部文件引用</a></li>
      <li><a href="#combiner" id="markdown-toc-combiner">1.3.使用Combiner</a></li>
      <li><a href="#writable" id="markdown-toc-writable">1.4.使用合适的Writable类型</a></li>
      <li><a href="#java" id="markdown-toc-java">1.5.尽可能的少创建新的Java对象</a></li>
    </ul>
  </li>
  <li><a href="#linux" id="markdown-toc-linux">2. Linux系统层面上的配置调优</a>    <ul>
      <li><a href="#section-2" id="markdown-toc-section-2">2.1. 文件系统的配置</a></li>
      <li><a href="#linux-1" id="markdown-toc-linux-1">2.2. Linux文件系统预读缓冲区大小</a></li>
      <li><a href="#raidlvm" id="markdown-toc-raidlvm">2.3. 去除RAID和LVM</a></li>
      <li><a href="#section-3" id="markdown-toc-section-3">2.4. 增大同时打开的文件数和网络连接数</a></li>
      <li><a href="#swap" id="markdown-toc-swap">2.5. 关闭swap分区</a></li>
      <li><a href="#io" id="markdown-toc-io">2.6. I/O调度器选择</a></li>
    </ul>
  </li>
  <li><a href="#hadoop" id="markdown-toc-hadoop">3. Hadoop平台内参数调优</a>    <ul>
      <li><a href="#section-4" id="markdown-toc-section-4">3.1. 计算资源优化</a></li>
      <li><a href="#section-5" id="markdown-toc-section-5">3.2. 节点间的通信优化</a></li>
      <li><a href="#section-6" id="markdown-toc-section-6">3.3. 磁盘块的配置优化</a></li>
      <li><a href="#rpc-handlerhttp" id="markdown-toc-rpc-handlerhttp">3.4. RPC Handler个数和Http线程数优化</a></li>
      <li><a href="#section-7" id="markdown-toc-section-7">3.5. 选择合适的压缩算法</a></li>
      <li><a href="#section-8" id="markdown-toc-section-8">3.6. 启用批量任务调度(现在新版本都默认支持了)</a></li>
      <li><a href="#apache" id="markdown-toc-apache">3.7. 启用预读机制(Apache暂时没有)</a></li>
      <li><a href="#hdfs" id="markdown-toc-hdfs">3.8.HDFS相关参数优化</a></li>
    </ul>
  </li>
  <li><a href="#section-9" id="markdown-toc-section-9">4.系统实现角度调优</a>    <ul>
      <li><a href="#section-10" id="markdown-toc-section-10">4.1. 调度延迟</a></li>
      <li><a href="#section-11" id="markdown-toc-section-11">4.2. 可移植性</a></li>
      <li><a href="#prefetchingpreshuffling" id="markdown-toc-prefetchingpreshuffling">4.3. 优化策略：Prefetching与preshuffling</a></li>
    </ul>
  </li>
</ul>

<p>这篇文章是我开始涉及做集群相关优化时的第一篇笔记，内容比较浅显易懂，适合想了解集群优化的朋友阅读。</p>

<h2 id="section">1.应用程序角度进行优化</h2>

<h3 id="reduce">1.1.减少不必要的reduce任务</h3>
<p>若对于同一份数据需要多次处理，可以尝试先排序、分区，然后自定义InputSplit将某一个分区作为一个Map的输入，在Map中处理数据，将Reduce的个数设置为空。</p>

<h3 id="section-1">1.2.外部文件引用</h3>
<p>如字典、配置文件等需要在Task之间共享的数据，可使用分布式缓存DistributedCache或者使用-files</p>

<h3 id="combiner">1.3.使用Combiner</h3>
<p>combiner是发生在map端的，作用是归并Map端输出的文件，这样Map端输出的数据量就小了，减少了Map端和reduce端间的数据传输。需要注意的是，Combiner不能影响作业的结果;不是每个MR都可以使用Combiner的，需要根据具体业务来定;Combiner是发生在Map端的，不能垮Map来执行（只有Reduce可以接收多个Map任务的输出数据）</p>

<h3 id="writable">1.4.使用合适的Writable类型</h3>
<p>尽可能使用二进制的Writable类型，例如：IntWritable， FloatWritable等，而不是Text。因为在一个批处理系统中将数值转换为文本时低效率的。使用二进制的Writable类型可以降低cpu资源的消耗，也可以减少Map端中间数据、结果数据占用的空间。</p>

<h3 id="java">1.5.尽可能的少创建新的Java对象</h3>
<p>a)需要注意的Writable对象，例如下面的写法：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>public void map(...) {
…
for (String word : words) {
    output.collect(new Text(word), new IntWritable(1));
} }
</code></pre>
</div>

<p>这样会冲去创建对象new Text(word)和new IntWritable(1))，这样可能会产生海量的短周期对象。更高效的写法见下：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>class MyMapper … {
Text wordText = new Text();
IntWritable one = new IntWritable(1);
public void map(...) {
    for (String word: words) {
    wordText.set(word);
        output.collect(wordText, one);
    }
} }
</code></pre>
</div>

<p>b)对于可变字符串，使用StringBuffer而不是String</p>

<p>String类是经过final修饰的，那么每次对它的修改都会产生临时对象，而SB则不会。</p>

<h2 id="linux">2. Linux系统层面上的配置调优</h2>

<h3 id="section-2">2.1. 文件系统的配置</h3>
<p>a) 关闭文件在被操作时会记下时间戳:noatime和nodiratime <br />
b) 选择I/O性能较好的文件系统（Hadoop比较依赖本地的文件系统）</p>

<h3 id="linux-1">2.2. Linux文件系统预读缓冲区大小</h3>
<p>命令:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>blockdev
</code></pre>
</div>

<h3 id="raidlvm">2.3. 去除RAID和LVM</h3>

<h3 id="section-3">2.4. 增大同时打开的文件数和网络连接数</h3>
<p>ulimit net.core.somaxconn</p>

<div class="highlighter-rouge"><pre class="highlight"><code>ulimit net.core.somaxconn
</code></pre>
</div>

<h3 id="swap">2.5. 关闭swap分区</h3>
<p>在Hadoop中，对于每个作业处理的数据量和每个Task中用到的各种缓冲，用户都是完全可控的。</p>

<div class="highlighter-rouge"><pre class="highlight"><code>/etc/sysctl.conf
</code></pre>
</div>

<h3 id="io">2.6. I/O调度器选择</h3>
<p>详情见AMD的白皮书</p>

<h2 id="hadoop">3. Hadoop平台内参数调优</h2>
<p>Hadoop相关可配置参数共有几百个，但是其中只有三十个左右会对其性能产生显著影响。</p>

<h3 id="section-4">3.1. 计算资源优化</h3>
<p>a) 设置合理的slot（资源槽位）</p>

<div class="highlighter-rouge"><pre class="highlight"><code>mapred.tasktracker.map.tasks.maximum / mapred.tasktracker.reduce.tasks.maximum
</code></pre>
</div>

<p>参数说明：每个TaskTracker上可并发执行的Map Task和Reduce Task数目 <br />
默认值：都是2 <br />
推荐值：根据具体的节点资源来看，推荐值是(core_per_node)/2~2*(cores_per_node) <br />
单位：无</p>

<h3 id="section-5">3.2. 节点间的通信优化</h3>
<p><strong>a) TaskTracker和JobTracker之间的心跳间隔</strong> <br />
这个值太小的话，在一个大集群中会造成JobTracker需要处理高并发心跳，可能会有很大的压力。 <br />
建议集群规模小于300时，使用默认值3秒，在此基础上，集群规模每增加100台，会加1秒。 <br />
<strong>b) 启用带外心跳(out-of-band heartbeat)</strong></p>

<div class="highlighter-rouge"><pre class="highlight"><code>mapreduce.tasktracker.outofband.heartbeat
</code></pre>
</div>

<p>参数说明：主要是为了减少任务分配延迟。它与常规心跳不同，一般的心跳是一定时间间隔发送的，而带外心跳是在任务运行结束或是失败时发送，这样就能在TaskTracker节点出现空闲资源的时候能第一时间通知JobTracker。</p>

<h3 id="section-6">3.3. 磁盘块的配置优化</h3>
<p>a) 作业相关的磁盘配置</p>

<div class="highlighter-rouge"><pre class="highlight"><code>mapred.local.dir
</code></pre>
</div>

<p>参数说明：map本地计算时所用到的目录，建议配置在多块硬盘上 <br />
b) 存储相关的磁盘配置（HDFS数据存储） <br />
dfs.data.dir <br />
参数说明：HDFS的数据存储目录，建议配置在多块硬盘上，可提高整体IO性能 <br />
例如：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>&lt;property&gt;
 &lt;name&gt;dfs.name.dir&lt;/name&gt;
 &lt;value&gt;/data1/hadoopdata/mapred/jt/,/data2/hadoopdata/mapred/jt/&lt;/value&gt;
&lt;/property&gt;
</code></pre>
</div>

<p>c) 存储相关的磁盘配置（HDFS元数据存储）</p>

<div class="highlighter-rouge"><pre class="highlight"><code>dfs.name.dir
</code></pre>
</div>

<p>参数说明：HDFS的元数据存储目录，建议设置多目录，每个多目录都可保存元数据的一个备份 <br />
注：要想提升hadoop整体IO性能，对于hadoop中用到的所有文件目录，都需要评估它磁盘IO的负载，对于IO负载可能会高的目录，最好都配置到多个磁盘上，以提示IO性能</p>

<h3 id="rpc-handlerhttp">3.4. RPC Handler个数和Http线程数优化</h3>
<p>a) RPC Handler个数</p>

<div class="highlighter-rouge"><pre class="highlight"><code>mapred.job.tracker.handler.count
</code></pre>
</div>

<p>参数说明：JobTracker需要并发的处理来自各个TaskTracker的RPC请求，可根据集群规模和并发数来调整RPC Handler的个数。 <br />
默认值：10 <br />
推荐值：60-70，最少要是TaskTracker个数的4% <br />
单位：无 <br />
b) Http线程数
   <br />
tasktracker.http.threads</p>

<p>在Shuffle阶段，Reduce Task会通过Http请求从各个TaskTracker上读取Map Task的结果，TaskTracker是使用Jetty Server来提供服务的，这里可适量调整Jetty Server的工作线程以提高它的并发处理能力。 <br />
默认值：40 <br />
推荐值：50-80+</p>

<h3 id="section-7">3.5. 选择合适的压缩算法</h3>
<div class="highlighter-rouge"><pre class="highlight"><code>mapred.compress.map.output / Mapred.output.compress
</code></pre>
</div>

<p>map输出的中间结果时需要进行压缩的，指定压缩方式<strong>（Mapred.compress.map.output.codec/ Mapred.output.compress.codec）</strong>。推荐使用LZO压缩。</p>

<h3 id="section-8">3.6. 启用批量任务调度(现在新版本都默认支持了)</h3>
<p>a) Fair Scheduler</p>

<div class="highlighter-rouge"><pre class="highlight"><code>mapred.fairscheduler.assignmultiple
</code></pre>
</div>

<p>b) Capacity Scheduler</p>

<h3 id="apache">3.7. 启用预读机制(Apache暂时没有)</h3>
<p>Hadoop是顺序读，所以预读机制可以很明显的提高HDFS的读性能。
HDFS预读：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>dfs.datanode.readahead ：true
dfs.datanode.readahead.bytes ：4MB
</code></pre>
</div>

<p>shuffle预读</p>

<div class="highlighter-rouge"><pre class="highlight"><code>mapred.tasktracker.shuffle.fadvise : true
mapred.tasktracker.shuffle.readahead.bytes : 4MB
</code></pre>
</div>

<h3 id="hdfs">3.8.HDFS相关参数优化</h3>
<p>1) dfs.replication <br />
参数说明：hdfs文件副本数 <br />
默认值：3 <br />
推荐值：3-5（对于IO较为密集的场景可适量增大） <br />
单位：无 <br />
2) dfs.blocksize <br />
参数说明： <br />
默认值：67108864(64MB) <br />
推荐值：稍大型集群建议设为128MB(134217728)或256MB(268435456) <br />
单位：无 <br />
3) dfs.datanode.handler.count <br />
参数说明：DateNode上的服务线程数 <br />
默认值：10 <br />
推荐值： <br />
单位：无 <br />
4) fs.trash.interval <br />
参数说明：HDFS文件删除后会移动到垃圾箱，该参数时清理垃圾箱的时间 <br />
默认值：0 <br />
推荐值：1440(1day) <br />
单位：无 <br />
5) io.sort.factor <br />
参数说明：当一个map task执行完之后，本地磁盘上(mapred.local.dir)有若干个spill文件，map task最后做的一件事就是执行merge sort，把这些spill文件合成一个文件（partition）。执行merge sort的时候，每次同时打开多少个spill文件由该参数决定。打开的文件越多，不一定merge sort就越快，所以要根据数据情况适当的调整。 <br />
默认值：10 <br />
推荐值： <br />
单位：无 <br />
6) mapred.child.java.opts <br />
参数说明：JVM堆的最大可用内存 <br />
默认值：-Xmx200m <br />
推荐值：-Xmx1G | -Xmx4G | -Xmx8G <br />
单位：-Xmx8589934592也行，单位不固定 <br />
7) io.sort.mb <br />
参数说明：Map Task的输出结果和元数据在内存中占的buffer总大小，当buffer达到一定阀值时，会启动一个后台进程来对buffer里的内容进行排序，然后写入本地磁盘，形成一个split小文件 <br />
默认值：100 <br />
推荐值：200 | 800 <br />
单位：兆 <br />
8) io.sort.spill.percent <br />
参数说明：即io.sort.mb中所说的阀值 <br />
默认值：0.8 <br />
推荐值：0.8 <br />
单位：无 <br />
9) io.sort.record <br />
参数说明：io.sort.mb中分类给元数据的空间占比 <br />
默认值：0.05 <br />
推荐值：0.05 <br />
单位：无 <br />
10) Mapred.reduce.parallel <br />
参数说明：Reduce shuffle阶段copier线程数。默认是5，对于较大集群，可调整为16~25 <br />
默认值：5 <br />
推荐值：16~25 <br />
单位：无</p>

<h2 id="section-9">4.系统实现角度调优</h2>
<p>https://www.xiaohui.org/archives/944.html</p>

<p>主要针对HDFS进行优化，HDFS性能低下的两个原因：调度延迟和可移植性</p>

<h3 id="section-10">4.1. 调度延迟</h3>
<p>关于调度延迟主要是发生在两个阶段： <br />
a) tasktracker上出现空余的slot到该tasktracker接收到新的task； <br />
b) tasktracker获取到了新的Task后，到连接上了datanode，并且可以读写数据。 <br />
之所以说这两个阶段不够高效，因为一个分布式计算系统需要解决的是计算问题，如果把过多的时间花费在其它上，就显得很不合适，例如线程等待、高负荷的数据传输。 <br />
下面解释下会经历上边两个阶段发生的过程： <br />
a) 当tasktracker上出现slot时，他会调用heartbeat方法向jobtracker发送心跳包（默认时间间隔是3秒，集群很大时可适量调整）来告知它，假设此时有准备需要执行的task，那么jobtracker会采用某种调度机制（调度机制很重要，是一个可以深度研究的东东）选择一个Task，然后通过调用heartbeat方法发送心跳包告知tasktracker。在该过程中，HDFS一直处于等待状态，这就使得资源利用率不高。 <br />
b) 这个过程中所发生的操作都是串行化的 <br />
tasktracker会连接到namenode上获取到自己需要的数据在datanode上的存储情况，然后再从datanode上读数据，在该过程中，HDFS一直处于等待状态，这就使得资源利用率不高。 <br />
若能减短hdfs的等待时间;在执行task之前就开始把数据读到将要执行该task的tasktracker上，减少数据传输时间，那么将会显得高效很多。未解决此类问题，有这样几种解决方案：重叠I/O和CPU阶段（pipelining），task预取（task prefetching），数据预取（data prefetching）等。</p>

<h3 id="section-11">4.2. 可移植性</h3>
<p>Hadoop是Java写的，所以可移植性相对较高。由于它屏蔽了底层文件系统，所以无法使用底层api来优化数据的读写。在活跃度较高的集群里（例如共享集群），大量并发读写会增加磁盘的随机寻道时间，这会降低读写效率;在大并发写的场景下，还会增加大量的磁盘碎片，这样将会大大的增加了读数据的成本，hdfs更适合文件顺序读取。
对于上述问题，可以尝试使用下面的解决方案：</p>

<blockquote>
  <p>tasktracker现在的线程模型是：one thread per client，即每个client连接都是由一个线程处理的（包括接受请求、处理请求，返回结果）。那么这一块一个拆分成两个部分来做，一组线程来处理和client的通信（Client Threads），一组用于数据的读写（Disk Threads）。</p>
</blockquote>

<p>想要解决上述两个问题，暂时没有十全十美的办法，只能尽可能的权衡保证调度延迟相对较低+可移植性相对较高。</p>

<h3 id="prefetchingpreshuffling">4.3. 优化策略：Prefetching与preshuffling</h3>
<ul>
  <li>
    <p>a) Prefetching包括Block-intra prefetching和Block-inter prefetching <br />
<strong>Block-intra prefetching：</strong>对block内部数据处理方式进行了优化，即一边进行计算，一边预读将要用到的数据。这种方式需要解决两个难题：一个是计算和预取同步，另一个是确定合适的预取率。前者可以使用进度条（processing bar）的概念，进度条主要是记录计算数据和预读数据的进度，当同步被打破时发出同步失效的通知。后者是要根据实际情况来设定，可采用重复试验的方法来确定。 <br />
<strong>Block-inter prefetching：</strong>在block层面上预读数据，在某个Task正在处理数据块A1的时候，预测器能预测接下来将要读取的数据块A2、A3、A4，然后把数据块A2、A3、A4预读到Task所在的rack上。</p>
  </li>
  <li>
    <p>b) preshuffling<br />
数据被map task处理之前，由预测器判断每条记录将要被哪个reduce task处理，将这些数据交给靠近reduce task的map task来处理。</p>
  </li>
</ul>

<p><strong>参考资料：</strong></p>

<ul>
  <li>cloudera官方文档 <br />
http://blog.cloudera.com/blog/2009/12/7-tips-for-improving-mapreduce-performance/</li>
  <li>
    <p>AMD白皮书(较为实用) <br />
http://www.admin-magazine.com/HPC/content/download/9408/73372/file/Hadoop_Tuning_Guide-Version5.pdf</p>
  </li>
  <li>国内博客（大部分内容都是AMD白皮书上的翻译）： <br />
http://dongxicheng.org/mapreduce/hadoop-optimization-0/
http://dongxicheng.org/mapreduce/hadoop-optimization-1/</li>
</ul>

                </div>
                <div class="read-all">
                    <a  href="/2015/02/23/hadoop%E4%BC%98%E5%8C%96-%E6%A6%82%E8%BF%B0/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
        </ul>



        <!-- Pagination links -->
        <div class="pagination">
          
            <span class="previous disable"><i class="fa fa-angle-double-left"></i></span>
            <span class="previous disable"><i class="fa fa-angle-left"></i></span>
          
          <span class="page_number ">1/1</span>
          
            <span class="next disable"><i class="fa fa-angle-right"></i></span>
            <span class="next disable"><i class="fa fa-angle-double-right"></i></span>
          
        </div>
    </div>
    <!-- <button class="anchor"><i class="fa fa-anchor"></i></button> -->
    <div class="right">
        <div class="wrap">
            <div class="side">
                <div>
                    <i class="fa fa-pencil-square-o" aria-hidden="true"></i>
                    Recent Posts
                </div>
                <ul class="content-ul" recent>
                    
                        <li><a href="/2016/05/09/Tez%E7%B3%BB%E5%88%97%E7%AC%AC%E4%BA%8C%E7%AF%87-hive_on_tez/">Tez系列第二篇 Hive_on_tez</a></li>
                    
                        <li><a href="/2016/04/01/Tez%E7%B3%BB%E5%88%97%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E5%B8%B8%E8%AF%86/">Tez系列第一篇 基础常识</a></li>
                    
                        <li><a href="/2016/03/13/hadoop%E4%BC%98%E5%8C%96-yarn/">Hadoop优化 Yarn</a></li>
                    
                        <li><a href="/2015/02/23/hadoop%E4%BC%98%E5%8C%96-%E6%A6%82%E8%BF%B0/">Hadoop优化 概述</a></li>
                    
                </ul>
            </div>

            <!-- Content -->
            <div class="side ">
                <div>
                    <i class="fa fa-th-list"></i>
                    Categories
                </div>
                <ul class="content-ul" cate>
                    
                    <li>
                        <a href="/category/#hadoop" class="categories-list-item" cate="hadoop">
                            <span class="name">
                                hadoop
                            </span>
                            <span class="badge">2</span>
                        </a>
                    </li>
                    
                    <li>
                        <a href="/category/#Tez" class="categories-list-item" cate="Tez">
                            <span class="name">
                                Tez
                            </span>
                            <span class="badge">2</span>
                        </a>
                    </li>
                    
                </ul>
            </div>
            <!-- 其他div框放到这里 -->
            <div class="side">
                <div>
                    <i class="fa fa-tags"></i>
                    Tags
                </div>
                <div class="tags-cloud">
                    
                    
                    
                    
                    

                    
                      
                      
                      
                      
                      
                      <a href="/tag/#集群优化" style="font-size: 13.5pt; color: #444;">集群优化</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#hadoop" style="font-size: 18pt; color: #000;">hadoop</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#Tez" style="font-size: 13.5pt; color: #444;">Tez</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#Hive" style="font-size: 9pt; color: #999;">Hive</a>
                    
                </div>
            </div>

            <!-- <div class="side">
                <div>
                    <i class="fa fa-external-link"></i>
                    Links
                </div>
                <ul  class="content-ul">

                </ul>
            </div> -->
        </div>
    </div>
</div>
<!-- <script src="/js/scroll.min.js " charset="utf-8"></script> -->
<!-- <script src="/js/pageContent.js " charset="utf-8"></script> -->


    <footer class="site-footer">


  <div class="wrapper">
      <p class="description">
          
          小鸟的成长记录.
          
      </p>
        <p class="contact">
            Contact me at:
            
            <a href="https://github.com/leocook"><i class="fa fa-github" aria-hidden="true"></i></a>
            

            
            <a href="mailto:leocook@163.com"><i class="fa fa-envelope-o" aria-hidden="true"></i></a>
            

            
            <a href="http://weibo.com/wulinjq"><i class="fa fa-weibo" aria-hidden="true"></i></a>
            

            

            

            
        </p>
        <p class="power">
            <span>
                Site powered by <a href="https://jekyllrb.com/">Jekyll</a> & <a href="https://pages.github.com/">Github Pages</a>.
            </span>
            <span>
                Theme designed by <a href="https://github.com/Gaohaoyang">HyG</a>.
            </span>
        </p>

  </div>
</footer>
<script src="/js/main.js " charset="utf-8"></script>


  </body>

</html>
