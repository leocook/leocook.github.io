<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>leocook</title>
    <description></description>
    <link>http://leocook.github.io/</link>
    <atom:link href="http://leocook.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 13 Oct 2016 14:48:07 +0800</pubDate>
    <lastBuildDate>Thu, 13 Oct 2016 14:48:07 +0800</lastBuildDate>
    <generator>Jekyll v3.2.1</generator>
    
      <item>
        <title>Spark streaming最佳实践 概述</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#section&quot; id=&quot;markdown-toc-section&quot;&gt;1.内存溢出&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-1&quot; id=&quot;markdown-toc-section-1&quot;&gt;2.外部系统连接数过多&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-2&quot; id=&quot;markdown-toc-section-2&quot;&gt;3.资源分配的浪费&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Spark streaming是基于Spark core的，天然的具备易扩展、高吞吐量，以及自动容错等特性。支持的主流数据源有Kafka、Flume、HDFS、Twitter、TCP socket等等，Spark的数据输出在spark streaming中都支持，对于有spark基础的开发人员来说，开发spark streaming应用成本将会少很多。本文是一篇概述型的文章，相关详细的配置会在后边逐渐补上。&lt;/p&gt;

&lt;p&gt;Spark streaming在企业实战中经常回到这么几类问题：内存溢出、外部系统连接数过多、分配的资源超过了程序所需要的资源，造成资源浪费。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;1.内存溢出&lt;/h2&gt;

&lt;p&gt;Spark内存大概分为了两类：Execution Memory和Storage Memory，前者主要用来做buffer的，例如joins、shuffle、sort等等；后者主要用来做cache，例如RDD的数据存储、广播变量、task结果数据等等。从1.6版本之前这两部分内存是不能共享的，从1.6开始之后这两部分内存就可以共享了。&lt;/p&gt;

&lt;p&gt;我遇到的内存溢出问题大概有三类，通常是单个分区处理的数据量过多：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a).数据倾斜引；&lt;/li&gt;
  &lt;li&gt;b).数据未倾斜，分区数过少；&lt;/li&gt;
  &lt;li&gt;c).某个分区中产生了一个较大的内存集合，例如大的List、Set，或者Map。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这类问题在spark core中也是经常会出现的，关于数据倾斜的问题以后会详细讲解。入手一个新的业务时，应该对该业务的数据量有个大概的预估，这样给这个应用分配多少节点，每个节点会处理多大的数据量，这样就能很好的预估出每个节点分配多少资源比较合理了。&lt;/p&gt;

&lt;p&gt;为了避免内存溢出，可以从下面几个方向来做：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a).避免数据倾斜；&lt;/li&gt;
  &lt;li&gt;b).评估每个分区的数据量，给每个分区分配合理的资源；&lt;/li&gt;
  &lt;li&gt;c).控制好List、Set等集合的大小；&lt;/li&gt;
  &lt;li&gt;d).控制Spark streaming读取源数据的最大速度（spark.streaming.kafka.maxRatePerPartition），实时数据流量有高峰和低谷，不同时间处理的数据量是不一样的，为防止数据高峰的时候内存溢出，这里有必要做配置；&lt;/li&gt;
  &lt;li&gt;e).配置Spark streaming可动态控制读取数据源的速度（spark.streaming.backpressure.enabled）。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-1&quot;&gt;2.外部系统连接数过多&lt;/h2&gt;

&lt;p&gt;在使用spark streaming解决问题的时候，经常会对外部数据源进行读写操作，切记每次读写完成之后需要关闭网络连接。在我们以往的编程经验中，这些思维都是一直持有的。 &lt;br /&gt;
在往hbase写入数据的时候，如果你使用了RDD.saveAsHadoopDataset方法，就需要注意了，org.apache.hadoop.hbase.mapred.TableOutputFormat类存在bug：不能释放zookeeper连接，导致在往hbase写数据的时候，zookeeper的连接数不停得增长。&lt;/p&gt;

&lt;p&gt;推荐使用下面这种写法：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
import org.apache.hadoop.mapreduce.Job
import org.apache.hadoop.hbase.mapreduce.TableOutputFormat

val conf = HBaseConfiguration.create()
conf.set(&quot;hbase.zookeeper.quorum&quot;, zk_hosts)
conf.set(&quot;hbase.zookeeper.property.clientPort&quot;, zk_port)

conf.set(TableOutputFormat.OUTPUT_TABLE, &quot;TABLE_NAME&quot;)
val job = Job.getInstance(conf)
job.setOutputFormatClass(classOf[TableOutputFormat[String]])

formatedLines.map{
  case (a,b, c) =&amp;gt; {
    val row = Bytes.toBytes(a)

    val put = new Put(row)
    put.setDurability(Durability.SKIP_WAL)

    put.addColumn(Bytes.toBytes(&quot;cf&quot;), Bytes.toBytes(&quot;node&quot;), Bytes.toBytes(b))
    put.addColumn(Bytes.toBytes(&quot;cf&quot;), Bytes.toBytes(&quot;topic&quot;), Bytes.toBytes(c))

    (new ImmutableBytesWritable(row), put)
  }
}.saveAsNewAPIHadoopDataset(job.getConfiguration)

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;section-2&quot;&gt;3.资源分配的浪费&lt;/h2&gt;

&lt;p&gt;如果实时数据在每天的某个时间点有着平时的几倍的数据量，如果给该作业分配过多的资源，那么在绝大多数，这些资源都是闲置浪费的。这里可以启用动态资源分配。&lt;/p&gt;

&lt;p&gt;关于配置介绍可以查看官方文档：http://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation&lt;/p&gt;

&lt;p&gt;如果启用该配置，需要做如下配置：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;1.在spark应用中配置spark.dynamicAllocation.enabled=true &lt;br /&gt;
2.每个节点启动外部shuffle服务，并在spark应用中配置spark.shuffle.service.enabled=true&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;关于外部shuffle服务，在standalone、Mesos，yarn中的配置是不一样的。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;standalone&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;启动worker的时候指定spark.shuffle.service.enabled=true&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Mesos&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在所有节点上配置spark.shuffle.service.enabled=true，然后执行$SPARK_HOME/sbin/start-mesos-shuffle-service.sh&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;yarn&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;a.添加yarn的配置文件，重新编译spark。如果使用官方编译好的安装包，可以忽略这一步。
b.找到spark-&lt;version&gt;-yarn-shuffle.jar。如果自己编译spark的话，在目录$SPARK_HOME/common/network-yarn/target/scala-&lt;version&gt;下；如果是使用官方编译好的spark，在lib目录下寻找。
c.添加到spark-&lt;version&gt;-yarn-shuffle.jar到yarn所有NodeManager的classpath下。
d.配置所有NodeManager的yarn-site.xml文件如下：&lt;/version&gt;&lt;/version&gt;&lt;/version&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;yarn.nodemanager.aux-services&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;spark_shuffle&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;yarn.nodemanager.aux-services.spark_shuffle.class&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;org.apache.spark.network.yarn.YarnShuffleService&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;e.重启所有的NodeManager&lt;/p&gt;

&lt;p&gt;spark on yarn配置了外部shuffle之后，&lt;code&gt;--num-executors&lt;/code&gt;配置将不再生效。&lt;/p&gt;

</description>
        <pubDate>Wed, 12 Oct 2016 00:00:00 +0800</pubDate>
        <link>http://leocook.github.io/2016/10/12/spark-streaming%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5-%E6%A6%82%E8%BF%B0/</link>
        <guid isPermaLink="true">http://leocook.github.io/2016/10/12/spark-streaming%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5-%E6%A6%82%E8%BF%B0/</guid>
        
        <category>spark</category>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>幸福的双休日</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#section&quot; id=&quot;markdown-toc-section&quot;&gt;扯几句&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-1&quot; id=&quot;markdown-toc-section-1&quot;&gt;周六&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-2&quot; id=&quot;markdown-toc-section-2&quot;&gt;周日&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;自从离开北京到现在也有两个月了，中间在上海待了一个多月，最后还是来了杭州。来了杭州之后才知道，这是一个幸福感爆棚的准一线城市！&lt;/p&gt;

&lt;p&gt;离开北京后，一直在奔波。北京和上海这两个城市，留给外地的年轻人更多的是想象，房价、落户等等都是遥不可及的。北京离家比较远，如果不能买房留在那也没啥意思。上海离家近一些，还是那个问题，买房和落户的问题，经过和女朋友三年异地的历程，不想把宝贵的时间耗费在距离上，爱情、亲情都是这样。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;扯几句&lt;/h2&gt;

&lt;p&gt;生活和工作上，杭州这总城市整体上没有北京和上海那么浮躁。没遇到过大家为了上个地铁或者公交，挤得你死我活，关于挤地铁这个事情，个人感觉上海那边比北京要显得糟糕点。在杭州则不一样，主城区到郊区距离也不是很远，大多数同事到公司的距离都在5公里以内吧，自行车和电动车都是比较好的代步工具。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;周六&lt;/h2&gt;

&lt;p&gt;按照惯例，周六会好好的睡一觉，一直睡到自然醒。上了一个星期的班，补充一下睡眠。中午有朋友在这，一起吃了个饭，下午一起逛逛街，去超市买点东西就回家了。晚餐后（媳妇做的晚餐，超棒！），看看书，学习学习新的技术，十点不到就睡着了。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;周日&lt;/h2&gt;

&lt;p&gt;六点多就醒了，微信上月朋友一起跑了两公里多，早上不需要跑太久，跑久了一天都会有疲惫感。回来的路上买了点早餐，到家后和朋友，还有媳妇一起开始了早餐。油条、小笼包，还有自己煮的白米粥，粗茶淡饭，健康的生活。吃完了早饭，朋友离去后才八点多。哇~早起会给你增添这么多时间！&lt;/p&gt;

&lt;p&gt;随后继续看书、学习技术。午饭后玩了玩游戏，傍晚和朋友一起去买菜，三个男人买了二十块钱的菜，花了一个多小时的时间做好了晚餐，朋友住的地方到我住的地方步行也就3分钟，才做好了，喊了媳妇来吃饭（现在都是男人下厨房有木有？）。晚餐后3哥们在一块吹吹牛，媳妇去洗碗了（俺媳妇还是比较贤惠滴）。&lt;/p&gt;

&lt;p&gt;晚餐散后回到住处看了会资料，写下这篇文字，记录一下这个双休。家里有个贤惠漂亮的媳妇、身边好友三两，不用考虑太多关于生存的话题。可以把时间花费在自己感兴趣的事情上，例如技术追求和厨艺。&lt;/p&gt;

&lt;p&gt;最后，杭州是一个适合年轻人奋斗和生活的地方。&lt;/p&gt;

</description>
        <pubDate>Sun, 21 Aug 2016 00:00:00 +0800</pubDate>
        <link>http://leocook.github.io/2016/08/21/%E5%B9%B8%E7%A6%8F%E7%9A%84%E5%8F%8C%E4%BC%91%E6%97%A5/</link>
        <guid isPermaLink="true">http://leocook.github.io/2016/08/21/%E5%B9%B8%E7%A6%8F%E7%9A%84%E5%8F%8C%E4%BC%91%E6%97%A5/</guid>
        
        <category>双休日</category>
        
        
        <category>生活</category>
        
      </item>
    
      <item>
        <title>Lua_on_nginx</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#section&quot; id=&quot;markdown-toc-section&quot;&gt;1.系统环境&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#lua&quot; id=&quot;markdown-toc-lua&quot;&gt;2.Lua的运行时环境配置&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#lua-1&quot; id=&quot;markdown-toc-lua-1&quot;&gt;2.1.下载Lua的运行环境&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-1&quot; id=&quot;markdown-toc-section-1&quot;&gt;2.2.编译安装&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#nginxlua&quot; id=&quot;markdown-toc-nginxlua&quot;&gt;3.下载Nginx的lua模块&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#nginx&quot; id=&quot;markdown-toc-nginx&quot;&gt;4.Nginx配置&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#nginx-1&quot; id=&quot;markdown-toc-nginx-1&quot;&gt;4.1.下载nginx&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-2&quot; id=&quot;markdown-toc-section-2&quot;&gt;4.2.编译安装&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-3&quot; id=&quot;markdown-toc-section-3&quot;&gt;4.3.配置&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#nginx-2&quot; id=&quot;markdown-toc-nginx-2&quot;&gt;4.4.启动nginx&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-4&quot; id=&quot;markdown-toc-section-4&quot;&gt;4.5.验证&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#faq&quot; id=&quot;markdown-toc-faq&quot;&gt;5.FAQ&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#pcre3&quot; id=&quot;markdown-toc-pcre3&quot;&gt;5.1.缺少pcre3&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#libssl&quot; id=&quot;markdown-toc-libssl&quot;&gt;5.2.缺少libssl&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nginx的高并发是它的一大显著优势，Lua则是一门较为轻便的脚本语言。把他们组合在一起，则极大的增强了Nginx的能力（灵活性，扩展性）。
Nginx-Lua模块是由淘宝开发的第三方模块，使用它可以把Lua内嵌到Nginx中。&lt;/p&gt;

&lt;p&gt;nginx  地址：http://www.nginx.org
luajit 地址：http://luajit.org/download.html
HttpLuaModule 地址：http://wiki.nginx.org/HttpLuaModule&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;1.系统环境&lt;/h2&gt;

&lt;p&gt;必须的编译环境，需要提前准备好。我这里的环境是ubuntu14.04，用的apt source是官方的源，使用163 source的时候有问题。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apt-get install make
apt-get install gcc
apt-get install libpcre3 libpcre3-dev
apt-get install libssl-dev
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;lua&quot;&gt;2.Lua的运行时环境配置&lt;/h2&gt;

&lt;h3 id=&quot;lua-1&quot;&gt;2.1.下载Lua的运行环境&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd /opt/

wget http://luajit.org/download/LuaJIT-2.0.4.tar.gz

tar zxvf LuaJIT-2.0.4.tar.gz
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;section-1&quot;&gt;2.2.编译安装&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd /opt/LuaJIT-2.0.4
make
make install
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;nginxlua&quot;&gt;3.下载Nginx的lua模块&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd /opt/

wget https://github.com/openresty/lua-nginx-module/archive/v0.10.5.tar.gz

tar zxvf v0.10.5.tar.gz
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;nginx&quot;&gt;4.Nginx配置&lt;/h2&gt;

&lt;h3 id=&quot;nginx-1&quot;&gt;4.1.下载nginx&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd /opt/

wget http://nginx.org/download/nginx-1.10.1.tar.gz

tar zxvf nginx-1.10.1.tar.gz
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;section-2&quot;&gt;4.2.编译安装&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# 导入环境变
export LUAJIT_LIB=/usr/local/lib
export LUAJIT_INC=/usr/local/include/luajit-2.0

# 安装到/usr/local/nginx-1.10.1目录下
./configure --prefix=/usr/local/nginx-1.10.1 --add-module=../lua-nginx-module-0.10.5

make -j2
make install
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;section-3&quot;&gt;4.3.配置&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd /usr/local/nginx-1.10.1
vi conf/nginx.conf
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;然后在http -&amp;gt; server下加入配置：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;location /lua_test {
    default_type 'text/plain';
    content_by_lua 'ngx.say(&quot;hello, ttlsa lua&quot;)';
}

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;这个配置后，访问http://[hostname]:[port]/lua_test，就能访问你所定义的代码块了。&lt;/p&gt;

&lt;p&gt;如果你还想修改nginx的http端口，修改一下&lt;strong&gt;http.server&lt;/strong&gt;中的&lt;strong&gt;listen&lt;/strong&gt;值，就可以了。&lt;/p&gt;

&lt;h3 id=&quot;nginx-2&quot;&gt;4.4.启动nginx&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/usr/local/nginx-1.10.1/sbin/nginx
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;section-4&quot;&gt;4.5.验证&lt;/h3&gt;

&lt;p&gt;查看端口的运行情况：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@ubuntu:~# netstat -anp|grep 4002
tcp        0      0 0.0.0.0:4002            0.0.0.0:*               LISTEN      1736/nginx
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;或者直接请求4002端口。（我这里使用的是4002端口，你可以根据需要，设置为你需要的。）&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-`&quot;&gt;root@ubuntu:/usr/local/nginx-1.10.1/sbin# curl http://192.168.1.160:4002/lua_test
hello, ttlsa lua
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;faq&quot;&gt;5.FAQ&lt;/h2&gt;

&lt;h3 id=&quot;pcre3&quot;&gt;5.1.缺少pcre3&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;错误log&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./configure: error: the HTTP rewrite module requires the PCRE library.
You can either disable the module by using --without-http_rewrite_module
option, or install the PCRE library into the system, or build the PCRE library
statically from the source with nginx by using --with-pcre=&amp;lt;path&amp;gt; option.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;解决办法&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apt-get install libpcre3 libpcre3-dev
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;libssl&quot;&gt;5.2.缺少libssl&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;错误log&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./configure: error: the HTTP gzip module requires the zlib library.
You can either disable the module by using --without-http_gzip_module
option, or install the zlib library into the system, or build the zlib library
statically from the source with nginx by using --with-zlib=&amp;lt;path&amp;gt; option.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;解决办法&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apt-get install libssl-dev
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;参考地址：
http://www.ttlsa.com/nginx/nginx-modules-ngx_lua/&lt;/p&gt;

</description>
        <pubDate>Tue, 26 Jul 2016 00:00:00 +0800</pubDate>
        <link>http://leocook.github.io/2016/07/26/Lua_on_Nginx/</link>
        <guid isPermaLink="true">http://leocook.github.io/2016/07/26/Lua_on_Nginx/</guid>
        
        <category>nginx</category>
        
        <category>lua</category>
        
        
        <category>nginx</category>
        
      </item>
    
      <item>
        <title>Install_hadoop_cluster_on_cm</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#cloudera-manager&quot; id=&quot;markdown-toc-cloudera-manager&quot;&gt;1.Cloudera Manager安装前准备&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#section&quot; id=&quot;markdown-toc-section&quot;&gt;1.1. 操作系统的优化&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-1&quot; id=&quot;markdown-toc-section-1&quot;&gt;1.2. 数据的存放&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#cloudera-manager-1&quot; id=&quot;markdown-toc-cloudera-manager-1&quot;&gt;2.开始安装Cloudera Manager&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#cm&quot; id=&quot;markdown-toc-cm&quot;&gt;2.1.下载CM&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-2&quot; id=&quot;markdown-toc-section-2&quot;&gt;2.2.配置私有软件仓库(如果不使用私有仓库，这里可以直接跳过)&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#section-3&quot; id=&quot;markdown-toc-section-3&quot;&gt;2.2.1.创建一个临时可以使用的远程仓库&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#cm-1&quot; id=&quot;markdown-toc-cm-1&quot;&gt;2.2.2.配置安装CM所需要的私有仓库&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#cm-2&quot; id=&quot;markdown-toc-cm-2&quot;&gt;2.2.3.配置使用CM安装节点时会用到的仓库&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#jdk&quot; id=&quot;markdown-toc-jdk&quot;&gt;2.2.4.配置节点的JDK&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#cm-3&quot; id=&quot;markdown-toc-cm-3&quot;&gt;2.3.开始安装CM&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#cm-4&quot; id=&quot;markdown-toc-cm-4&quot;&gt;2.4.使用CM安装集群&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-4&quot; id=&quot;markdown-toc-section-4&quot;&gt;3.注意事项&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#host&quot; id=&quot;markdown-toc-host&quot;&gt;3.1.主机的Host配置不能出差错&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#cm-agent&quot; id=&quot;markdown-toc-cm-agent&quot;&gt;3.2.cm-agent安装失败重试时&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-5&quot; id=&quot;markdown-toc-section-5&quot;&gt;4.会用到的一些地址总结&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#mysql&quot; id=&quot;markdown-toc-mysql&quot;&gt;4.1.Mysql的相关配置&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-6&quot; id=&quot;markdown-toc-section-6&quot;&gt;4.2.创建本地仓库&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#parcel&quot; id=&quot;markdown-toc-parcel&quot;&gt;4.3.parcel的下载地址&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#cm-5&quot; id=&quot;markdown-toc-cm-5&quot;&gt;4.4.cm的下载地址&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-7&quot; id=&quot;markdown-toc-section-7&quot;&gt;5.附件&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于Hadoop这个复杂的大系统，我们期望能有一个平台，可以对Hadoop的每一个部件都能够进行安装部署，以及细颗粒度的监控。Apache发行版的Hadoop可以使用Ambari；Cloudera公司的CDH版本Hadoop则可以使用Cloudera Manager（后面简称为CM）来统一管理和部署。咱们这里的操作系统使用的是ubuntu14.04.&lt;/p&gt;

&lt;h2 id=&quot;cloudera-manager&quot;&gt;1.Cloudera Manager安装前准备&lt;/h2&gt;

&lt;h3 id=&quot;section&quot;&gt;1.1. 操作系统的优化&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;打开的最大文件数
修改当前的session配置(临时)：&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ulimit -SHn 65535
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;永久修改（需要重启服务器）：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo echo &quot;ulimit -SHn 65535&quot; &amp;gt;&amp;gt; /etc/rc.local
sudo chmod +x /etc/rc.local
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;打开的组大文件句柄数
临时配置&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# 一般可不做修改，这是临时配置
sudo echo 2000000 &amp;gt; /proc/sys/fs/file-max
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;永久配置&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;echo &quot;echo 2000000 &amp;gt; /proc/sys/fs/file-max&quot; &amp;gt;&amp;gt; /etc/rc.local
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;或者&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo echo &quot;fs.file-max = 2000000&quot; &amp;gt;&amp;gt;/etc/sysctl.conf #推荐
#使文件生效sudo /sbin/sysctl -p
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;打开的最大网络连接数&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo echo &quot;net.core.somaxconn = 2048&quot; &amp;gt;&amp;gt;/etc/sysctl.conf
sudo /sbin/sysctl -p
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;关闭selinux&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ubuntu默认是不安装的selinux的，所以这里可以直接忽略。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;配置ntp&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;安装&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apt-get install ntp
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;如果只需要保证集群内部的各个server之间时间保持同步，只需要在需要同步的机器上配置：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;0 */12 * * * ntpdate dt-vt-154
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;如果需要时间和互联网的时间保持一致，那么就需要在提供ntp server的机器上配置上层ntpserver:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;server [ntpserver_01]
server [ntpserver_02]
server [ntpserver_03]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;关闭防火墙&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo ufw disable #关闭防火墙
apt-get remove iptables #卸载防火墙
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;配置好hosts映射文件&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vi /etc/hosts

192.168.1.151   dt-vt-151
192.168.1.152   dt-vt-152
192.168.1.153   dt-vt-153
192.168.1.154   dt-vt-154
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Java环境配置&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;安装&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mkdir /opt/java
cd /opt/java
#下载到安装文件到这个目录
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;环境配置&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vi /etc/profile

export JAVA_HOME=/opt/java/jdk1.7.0_79
export CLASSPATH=.:$JAVA_HOME/lib
export PATH=$JAVA_HOME/bin:$PATH
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;执行下面命令，使当前的session生效：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;source /etc/profile
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;section-1&quot;&gt;1.2. 数据的存放&lt;/h3&gt;
&lt;p&gt;在使用CM来管理集群的时候，会涉及到大量的数据存储，例如Hadoop的主机列表信息，主机的配置信息，负载信息，各个模块的运行时状态等等。
咱们这里使用mysql来作为CM的数据存储，这里不仅是CM，Hadoop中的Hive等模块的元数据，都来使用mysql存储。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;安装mysql&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install mysql-server
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;我这里安装完后是5.5.49.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;关闭mysql，备份配置文件&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/etc/init.d/mysql stop
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;把/var/lib/mysql/ib_logfile0和/var/lib/mysql/ib_logfile1拷贝至某个配置目录中，例如：/var/lib/mysql/bak&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;配置InnoDB引擎&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;务必使用InnoDB引擎引擎，若是使用MyISAM引擎CM将启动不了。在Mysql的命令行中运行下面的命令，来查看你的Mysql使用了哪个引擎。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mysql&amp;gt; show table status;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;配置mysql的innodb刷写模式&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;innodb_flush_method=O_DIRECT
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;即：配置Innodb的刷写模式为异步写模式。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;修改mysql的最大连接数&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;max_connections=1550
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;在这里，你应该会考虑配置该数值为多少比较合适。
当集群规模&lt;strong&gt;小于50台&lt;/strong&gt;的时候，假设该库中有N个数据库是用来服务于Hadoop的，那么max_connections可以设置为100*N+50。例如：Cloudera Manager Server, Activity Monitor, Reports Manager, Cloudera Navigator, 和 Hive metastore都是使用mysql的，那么就配置max_connections为550.
当集群规模&lt;strong&gt;大于50台&lt;/strong&gt;的时候，建议每个数据库只存放在一台机器上。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;配置文件样例&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[mysqld]
transaction-isolation = READ-COMMITTED
# Disabling symbolic-links is recommended to prevent assorted security risks;
# to do so, uncomment this line:
# symbolic-links = 0

key_buffer = 16M
key_buffer_size = 32M
max_allowed_packet = 32M
thread_stack = 256K
thread_cache_size = 64
query_cache_limit = 8M
query_cache_size = 64M
query_cache_type = 1

max_connections = 1550
#expire_logs_days = 10
#max_binlog_size = 100M


# InnoDB settings
innodb_file_per_table = 1
innodb_flush_log_at_trx_commit  = 2
innodb_log_buffer_size = 64M
innodb_buffer_pool_size = 4G
innodb_thread_concurrency = 8
innodb_flush_method = O_DIRECT
innodb_log_file_size = 512M

[mysqld_safe]
log-error=/var/log/mysqld.log
pid-file=/var/run/mysqld/mysqld.pid

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;启动mysql&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/etc/init.d/mysql start
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;打开开机自启&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apt-get install sysv-rc-conf
sysv-rc-conf mysql on
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;安装mysql-jdbc驱动&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apt-get install libmysql-java
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;mysql远程连接&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vi /etc/mysql/my.cnf

bind-address = 0.0.0.0
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;給相关服务创建mysql的数据库&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;相关列表如下：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Role&lt;/th&gt;
      &lt;th&gt;Database&lt;/th&gt;
      &lt;th&gt;User&lt;/th&gt;
      &lt;th&gt;Password&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Activity Monitor&lt;/td&gt;
      &lt;td&gt;amon&lt;/td&gt;
      &lt;td&gt;amon&lt;/td&gt;
      &lt;td&gt;amon_password&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Reports Manager&lt;/td&gt;
      &lt;td&gt;rman&lt;/td&gt;
      &lt;td&gt;rman&lt;/td&gt;
      &lt;td&gt;rman_password&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Hive Metastore Server&lt;/td&gt;
      &lt;td&gt;hive_metastore&lt;/td&gt;
      &lt;td&gt;hive&lt;/td&gt;
      &lt;td&gt;hive_password&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Sentry Server&lt;/td&gt;
      &lt;td&gt;sentry&lt;/td&gt;
      &lt;td&gt;sentry&lt;/td&gt;
      &lt;td&gt;sentry_password&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Cloudera Navigator Audit Server&lt;/td&gt;
      &lt;td&gt;nav&lt;/td&gt;
      &lt;td&gt;nav&lt;/td&gt;
      &lt;td&gt;nav_password&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Cloudera Navigator Metadata Server&lt;/td&gt;
      &lt;td&gt;navms&lt;/td&gt;
      &lt;td&gt;navms&lt;/td&gt;
      &lt;td&gt;navms_password&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;OOZIE&lt;/td&gt;
      &lt;td&gt;oozie&lt;/td&gt;
      &lt;td&gt;oozie&lt;/td&gt;
      &lt;td&gt;oozie&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;建表语句格式如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;create database database DEFAULT CHARACTER SET utf8;
grant all on database.* TO 'user'@'%' IDENTIFIED BY 'password';
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;建表语句如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;create database amon DEFAULT CHARACTER SET utf8;
grant all on amon.* TO 'amon'@'%' IDENTIFIED BY 'amon';

create database rmon DEFAULT CHARACTER SET utf8;
grant all on rmon.* TO 'rmon'@'%' IDENTIFIED BY 'rmon';

create database hive_metastore DEFAULT CHARACTER SET utf8;
grant all on hive_metastore.* TO 'hive'@'%' IDENTIFIED BY 'hive';

create database sentry DEFAULT CHARACTER SET utf8;
grant all on sentry.* TO 'sentry'@'%' IDENTIFIED BY 'sentry';

create database nav DEFAULT CHARACTER SET utf8;
grant all on nav.* TO 'nav'@'%' IDENTIFIED BY 'nav';

create database navms DEFAULT CHARACTER SET utf8;
grant all on navms.* TO 'navms'@'%' IDENTIFIED BY 'navms';

create database oozie DEFAULT CHARACTER SET utf8;
grant all on oozie.* TO 'oozie'@'%' IDENTIFIED BY 'oozie';
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;cloudera-manager-1&quot;&gt;2.开始安装Cloudera Manager&lt;/h2&gt;

&lt;h3 id=&quot;cm&quot;&gt;2.1.下载CM&lt;/h3&gt;

&lt;p&gt;可以下载安装最新版本的CM：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget http://archive.cloudera.com/cm5/installer/latest/cloudera-manager-installer.bin
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;当然，如果你想选择安装其它版本，可以访问下面的地址，并选择下载你所需要的版本(cm5+)：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;http://archive.cloudera.com/cm5/installer/
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;我这里使用的是版本是5.4.10，从release note上看，目前cdh5.4.10是最稳当的版本。我下载到了本地路径如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/opt/cm/cloudera-manager-installer.bin
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;修改权限，使其可以被执行：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;chmod u+x cloudera-manager-installer.bin
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;section-2&quot;&gt;2.2.配置私有软件仓库(如果不使用私有仓库，这里可以直接跳过)&lt;/h3&gt;

&lt;h4 id=&quot;section-3&quot;&gt;2.2.1.创建一个临时可以使用的远程仓库&lt;/h4&gt;
&lt;p&gt;这个配置是在安装cloudera-manager-server的时候才会用的。这里的仓库是使用传统的http协议，通过网络传输数据的。可以去http://archive.cloudera.com/cm5/repo-as-tarball/下载你所需要的cdh包。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;解压安装包&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下载完安装包后，解压到某个目录下，我这里下载的是cm5.4.10-ubuntu14-04.tar.gz这个版本。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tar -zxvf cm5.4.10-ubuntu14-04.tar.gz
chmod -R ugo+rX /opt/cm/local_resp/cm
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;解压后的目录是/opt/cm/local_resp/cm&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;启动Http server&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;启动一个Http服务，使可以通过网络来访问仓库中的数据。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd /opt/cm/
python -m SimpleHTTPServer 8900
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;我这里使用的是8900，你可以根据需要，使用指定的端口。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;验证&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可在浏览器中访问地址http://server:8900/cm，如果可以正常访问，并且能看到相对应的文件列表，则表示正常启动。&lt;/p&gt;

&lt;h4 id=&quot;cm-1&quot;&gt;2.2.2.配置安装CM所需要的私有仓库&lt;/h4&gt;

&lt;p&gt;在目录&lt;strong&gt;/etc/apt/sources.list.d/&lt;/strong&gt;下创建文件&lt;strong&gt;my-private-cloudera-repo.list&lt;/strong&gt;，并写入配置把这个文件和上面新建的仓库关联到一起：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vi /etc/apt/sources.list.d/my-private-cloudera-repo.list

deb [arch=amd64] http://192.168.1.154:8900/cm/ trusty-cm5.4.10 contrib
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;执行下面的命令，使得上面的配置生效：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get update
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;cm-2&quot;&gt;2.2.3.配置使用CM安装节点时会用到的仓库&lt;/h3&gt;
&lt;p&gt;可以下载地址：http://archive.cloudera.com/cm5/ubuntu/trusty/amd64/cm/ 里的所有内容到本地，然后使用&lt;strong&gt;2.2.1&lt;/strong&gt;中的方式来启动一个Http server。&lt;/p&gt;

&lt;h3 id=&quot;jdk&quot;&gt;2.2.4.配置节点的JDK&lt;/h3&gt;
&lt;p&gt;这一步是可选的，如果你不想每台机器都去手动安装，也可以在后边使用CM来批量安装。&lt;/p&gt;

&lt;h3 id=&quot;cm-3&quot;&gt;2.3.开始安装CM&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;连接互联网安装&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo ./cloudera-manager-installer.bin
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;或者&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;使用本地仓库安装&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo ./cloudera-manager-installer.bin --skip_repo_package=1
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;然后就是一路的YES &amp;amp; NEXT，最后安装完成，CM默认的端口是7180，账户名和密码都是7180.&lt;/p&gt;

&lt;h3 id=&quot;cm-4&quot;&gt;2.4.使用CM安装集群&lt;/h3&gt;

&lt;p&gt;首次登陆CM管理界面的时候，会出现一个集群安装向导。我这里选择的是免费版本。然后大概有如下几步：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;使用ip或者hostname来搜索主机，搜索到之后，go to next step.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;看到如下图片的时候，如果你已经在前面的主机中安装好了JDK，那么这里可以不选，如果没有，则必须选择。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://7xriy2.com1.z0.glb.clouddn.com/cm-02.png&quot; alt=&quot;CM JDK&quot; title=&quot;cm jdk&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;选择是否使用单用户模式（Single User Mode）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;不使用该模式的话，HDFS服务会使用“hdfs”账户来启动，Hbase的Region Server会使用“hbase”账户来启动。使用了该模式之后，所有的服务都是使用同一个账户去启动的。
这里主要看集群的使用场景，如果其中涉及到不同的模块是由不同人员来运维管理的话，我建议还是不要使用单用户模式了。但如果集群是统一由一个人员来管理，那么选择使用单用户模式可能会方便很多。
我这里没有使用单用户模式。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;配置好SSH登录&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://7xriy2.com1.z0.glb.clouddn.com/cm-03.png&quot; alt=&quot;配置ssh登录信息&quot; title=&quot;输入密码&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;配置好SSH后开始连入主机，安装jdk和cm-agent
&lt;img src=&quot;http://7xriy2.com1.z0.glb.clouddn.com/cm-04.png&quot; alt=&quot;install cm agent&quot; title=&quot;install cm agent&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;安装完成，此时cm-agent就已经安装好了，随时都可以使用cm-server控制安装Hadoop的相关组件,完成后先不要点击进入下一步，继续看下面。
&lt;img src=&quot;http://7xriy2.com1.z0.glb.clouddn.com/cm-05.png&quot; alt=&quot;cm agent ok&quot; title=&quot;cm agent ok&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;分发Hadoop的安装包&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;去地址：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;http://archive.cloudera.com/cdh5/parcels/5.4.10/
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;下载：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CDH-5.4.10-1.cdh5.4.10.p0.16-trusty.parcel
CDH-5.4.10-1.cdh5.4.10.p0.16-trusty.parcel.sha1
manifest.json
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;下载完成后：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;把&quot;CDH-5.4.10-1.cdh5.4.10.p0.16-trusty.parcel.sha1&quot;重命名为&quot;CDH-5.4.10-1.cdh5.4.10.p0.16-trusty.parcel.sha&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;并移到目录：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/opt/cloudera/parcel-repo
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;然后在cm-serve点击进入下一页，将会看到如下图：
&lt;img src=&quot;http://7xriy2.com1.z0.glb.clouddn.com/cm-06.png&quot; alt=&quot;cm parcel distributed&quot; title=&quot;cm parcel distributed&quot; /&gt;
因为你已经把安装包下载好，并且放入到/opt/cloudera/parcel-repo（默认的目录）里面，所以这里的&lt;strong&gt;Download&lt;/strong&gt;自然就是100%，&lt;strong&gt;Distributed&lt;/strong&gt;是把安装包从cm-server往集群中各个节点分发的过程，&lt;strong&gt;Unpacked&lt;/strong&gt;是表示各个节点的上安装包的解压情况进度，前面都OK后，&lt;strong&gt;Activated&lt;/strong&gt;自然就可以了，表示安装包已经部署好了，可以随时进行安装。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;选择安装Hadoop的那些组件之后，会在这里显示各个组件部署的主机地址。
&lt;img src=&quot;http://7xriy2.com1.z0.glb.clouddn.com/cm-07.png&quot; alt=&quot;hosts choose&quot; title=&quot;hosts choose&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;这里配置的是一些组件使用的mysql信息：
&lt;img src=&quot;http://7xriy2.com1.z0.glb.clouddn.com/cm-08.png&quot; alt=&quot;hadoop mysql config&quot; title=&quot;hadoop mysql config&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;然后可以配置一些服务的的具体参数
&lt;img src=&quot;http://7xriy2.com1.z0.glb.clouddn.com/cm-09.png&quot; alt=&quot;hadoop config&quot; title=&quot;hadoop config&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;参数配置都没问题后，开始执行安装。
&lt;img src=&quot;http://7xriy2.com1.z0.glb.clouddn.com/cm-10.png&quot; alt=&quot;hadoop install&quot; title=&quot;hadoop install&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;安装完成后。
&lt;img src=&quot;http://7xriy2.com1.z0.glb.clouddn.com/cm-11.jpeg&quot; alt=&quot;install success&quot; title=&quot;install success&quot; /&gt;
PS：我这里的Kafka是后来安装上去的，默认Hadoop的parcel包是没有kafka的。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-4&quot;&gt;3.注意事项&lt;/h2&gt;

&lt;h3 id=&quot;host&quot;&gt;3.1.主机的Host配置不能出差错&lt;/h3&gt;
&lt;p&gt;我就配置错过host，期间走了一些弯路，主要表现在添加完主机之后，Hosts下的主机名都是localhost。&lt;/p&gt;

&lt;h3 id=&quot;cm-agent&quot;&gt;3.2.cm-agent安装失败重试时&lt;/h3&gt;
&lt;p&gt;如果在重试的过程中出现了一直等待，或者cm-agent端口被占用的情况，大概有下面的几种情况：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;锁文件没有删除&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo rm /tmp/.scm_prepare_node.lock
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;端口被占用 &lt;strong&gt;9000/9001&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这种情况是部分进程没有关闭成功，找到端口对应的进程号，然后然后使用&lt;strong&gt;kill -9停止进程&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;netstat -nap | grep 9000
netstat -nap | grep 9001
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;主要是&lt;strong&gt;supervisor&lt;/strong&gt;和&lt;strong&gt;cm-agent&lt;/strong&gt;进程&lt;/p&gt;

&lt;h2 id=&quot;section-5&quot;&gt;4.会用到的一些地址总结&lt;/h2&gt;

&lt;h3 id=&quot;mysql&quot;&gt;4.1.Mysql的相关配置&lt;/h3&gt;
&lt;p&gt;http://www.cloudera.com/documentation/enterprise/5-4-x/topics/cm_ig_mysql.html&lt;/p&gt;

&lt;h3 id=&quot;section-6&quot;&gt;4.2.创建本地仓库&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;For CM&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;http://www.cloudera.com/documentation/enterprise/5-4-x/topics/cm_ig_create_local_package_repo.html&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For parcel&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;http://www.cloudera.com/documentation/enterprise/5-4-x/topics/cm_ig_create_local_parcel_repo.html&lt;/p&gt;

&lt;h3 id=&quot;parcel&quot;&gt;4.3.parcel的下载地址&lt;/h3&gt;

&lt;p&gt;http://archive.cloudera.com/cdh5/parcels/
http://archive.cloudera.com/kafka/parcels/&lt;/p&gt;

&lt;h3 id=&quot;cm-5&quot;&gt;4.4.cm的下载地址&lt;/h3&gt;

&lt;p&gt;加压后可以直接运行
http://archive.cloudera.com/cm5/cm/5/&lt;/p&gt;

&lt;h2 id=&quot;section-7&quot;&gt;5.附件&lt;/h2&gt;

&lt;p&gt;博文中的图片是压缩后的，清晰度比较低，原图可以访问下面的地址：&lt;/p&gt;

&lt;p&gt;链接: http://pan.baidu.com/s/1cmiBCE 密码: rnmw&lt;/p&gt;
</description>
        <pubDate>Sat, 23 Jul 2016 00:00:00 +0800</pubDate>
        <link>http://leocook.github.io/2016/07/23/Install_hadoop_cluster_on_CM/</link>
        <guid isPermaLink="true">http://leocook.github.io/2016/07/23/Install_hadoop_cluster_on_CM/</guid>
        
        <category>cm</category>
        
        <category>hadoop</category>
        
        
        <category>hadoop</category>
        
      </item>
    
      <item>
        <title>Elk安装配置介绍</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#section&quot; id=&quot;markdown-toc-section&quot;&gt;版本列表&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#logstash&quot; id=&quot;markdown-toc-logstash&quot;&gt;1.logstash配置&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#jdk&quot; id=&quot;markdown-toc-jdk&quot;&gt;1.1.Jdk安装&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#logstash-1&quot; id=&quot;markdown-toc-logstash-1&quot;&gt;1.2.下载logstash&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-1&quot; id=&quot;markdown-toc-section-1&quot;&gt;1.3.启动&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-2&quot; id=&quot;markdown-toc-section-2&quot;&gt;1.4.验证&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#es&quot; id=&quot;markdown-toc-es&quot;&gt;2.ES&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#section-3&quot; id=&quot;markdown-toc-section-3&quot;&gt;2.1.下载&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-4&quot; id=&quot;markdown-toc-section-4&quot;&gt;2.2.配置&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-5&quot; id=&quot;markdown-toc-section-5&quot;&gt;2.3.启动&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-6&quot; id=&quot;markdown-toc-section-6&quot;&gt;2.4.插件安装&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-7&quot; id=&quot;markdown-toc-section-7&quot;&gt;2.5.验证&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#kibana&quot; id=&quot;markdown-toc-kibana&quot;&gt;3. Kibana&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#section-8&quot; id=&quot;markdown-toc-section-8&quot;&gt;3.1.下载&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-9&quot; id=&quot;markdown-toc-section-9&quot;&gt;3.2.配置&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-10&quot; id=&quot;markdown-toc-section-10&quot;&gt;3.3.启动&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-11&quot; id=&quot;markdown-toc-section-11&quot;&gt;3.4.验证&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#elk&quot; id=&quot;markdown-toc-elk&quot;&gt;4.使ELK整体协作起来&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#section-12&quot; id=&quot;markdown-toc-section-12&quot;&gt;4.1.原理&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#el&quot; id=&quot;markdown-toc-el&quot;&gt;4.2.E和L的连接&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#kibana-1&quot; id=&quot;markdown-toc-kibana-1&quot;&gt;4.3.Kibana的配置&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#es-1&quot; id=&quot;markdown-toc-es-1&quot;&gt;4.4.ES&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-13&quot; id=&quot;markdown-toc-section-13&quot;&gt;5.小结&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;es是接触的比较早，在13年就做过相关开发，后来使用过ELK来做一些数据统计。最近打算从头来梳理一下这块的东西，今天就先从安装和配置开始吧。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;版本列表&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;project&lt;/th&gt;
      &lt;th&gt;version&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;es&lt;/td&gt;
      &lt;td&gt;2.3.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;logstash&lt;/td&gt;
      &lt;td&gt;2.3.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;kibana&lt;/td&gt;
      &lt;td&gt;4.5.3&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;logstash&quot;&gt;1.logstash配置&lt;/h2&gt;

&lt;h3 id=&quot;jdk&quot;&gt;1.1.Jdk安装&lt;/h3&gt;
&lt;p&gt;http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html&lt;/p&gt;

&lt;h3 id=&quot;logstash-1&quot;&gt;1.2.下载logstash&lt;/h3&gt;
&lt;p&gt;https://www.elastic.co/downloads/logstash&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;1.3.启动&lt;/h3&gt;

&lt;p&gt;解压后可直接启动，不增加额外的配置也是能够启动成功的。启动方式有多种，这里举例说明。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;使用-e指定启动的参数&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./logstash -e 'input { stdin { } } output { stdout {} }'
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;这里设定stdin为输入，stdout为输出。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;使用配置文件启动&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cat logstash-simple.conf
input { stdin { } }
output {
   stdout { codec=&amp;gt; rubydebug }
}

./logstash agent -f logstash-simple.conf
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;section-2&quot;&gt;1.4.验证&lt;/h3&gt;

&lt;p&gt;启动后才命令行输入”hello World”，如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@dt-vt-153:/opt/logstash/logstash-2.3.4/bin# ./logstash agent -f logstash-simple.conf

Settings: Default pipeline workers: 1
Pipeline main started
hello World
{
       &quot;message&quot; =&amp;gt; &quot;hello World&quot;,
       &quot;@version&quot; =&amp;gt; &quot;1&quot;,
       &quot;@timestamp&quot; =&amp;gt; &quot;2016-07-18T07:20:20.526Z&quot;,
       &quot;host&quot; =&amp;gt; &quot;0.0.0.0&quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;打印出来的message部分显示为输入内容。&lt;/p&gt;

&lt;h2 id=&quot;es&quot;&gt;2.ES&lt;/h2&gt;

&lt;h3 id=&quot;section-3&quot;&gt;2.1.下载&lt;/h3&gt;
&lt;p&gt;https://www.elastic.co/downloads/elasticsearch&lt;/p&gt;

&lt;h3 id=&quot;section-4&quot;&gt;2.2.配置&lt;/h3&gt;
&lt;p&gt;配置一下主机的地址,这里不配置的话，只能在安装服务的宿主机上使用localhost来访问ES了。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vi config/elasticsearch.yml
network.host: 0.0.0.0
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;section-5&quot;&gt;2.3.启动&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/opt/elasticsearch/elasticsearch-2.3.4/bin/elasticsearch
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;section-6&quot;&gt;2.4.插件安装&lt;/h3&gt;

&lt;p&gt;kopf插件安装：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/opt/elasticsearch/elasticsearch-2.3.4/bin/plugin install lmenezes/elasticsearch-kopf
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;安装完之后，可以访问web页面http://[hostname]:9200/_plugin/kopf查看。&lt;/p&gt;

&lt;h3 id=&quot;section-7&quot;&gt;2.5.验证&lt;/h3&gt;

&lt;p&gt;es默认使用的9200端口，可使用下面的命令来查看该端口是否已经被监听：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;netstat -anp |grep :9200
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;或者使用浏览器访问端口9200.&lt;/p&gt;

&lt;h2 id=&quot;kibana&quot;&gt;3. Kibana&lt;/h2&gt;

&lt;h3 id=&quot;section-8&quot;&gt;3.1.下载&lt;/h3&gt;
&lt;p&gt;https://www.elastic.co/downloads/kibana&lt;/p&gt;

&lt;h3 id=&quot;section-9&quot;&gt;3.2.配置&lt;/h3&gt;

&lt;p&gt;配置一下es的地址。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vi config/kibana.yml

elasticsearch.url: &quot;http://[hostname]:9200&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;这里默认是使用ES的数据。&lt;/p&gt;

&lt;h3 id=&quot;section-10&quot;&gt;3.3.启动&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/opt/kibana/kibana-4.5.3-linux-x64/bin/kibana
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;section-11&quot;&gt;3.4.验证&lt;/h3&gt;

&lt;p&gt;访问：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;http://[hostname]:5601/
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;点击”create”创建索引名称。&lt;/p&gt;

&lt;h2 id=&quot;elk&quot;&gt;4.使ELK整体协作起来&lt;/h2&gt;

&lt;h3 id=&quot;section-12&quot;&gt;4.1.原理&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;logstash&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;logstash主要用作收集数据使用，可以自由的定义数据的入口和出口，兼容多种数据源。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;elasticsearch&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;es和solr比较类似，都是基于lucene的来提供的搜索服务。但是在高并发的表现上，ES的负载均衡效果是优于solr的。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;kibana&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;kibana是一个可以可以用来查看ES里数据的Web。在早期logstash有一个logstash-web，但是功能比较简单。咱们这里说的kibana严格意义上说是kibana4，是在2015年重构完成的一个版本。&lt;/p&gt;

&lt;h3 id=&quot;el&quot;&gt;4.2.E和L的连接&lt;/h3&gt;
&lt;p&gt;其实就是把logstash收集到的数据写入es中，这里只要在logstash的启动参数上做配置就可以了，具体的配置文件如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vi logstash-indexer.conf

input {
  file {
    type =&amp;gt;&quot;syslog&quot;
    path =&amp;gt; [ &quot;/var/log/syslog&quot; ]
  }
  syslog {
    type =&amp;gt;&quot;syslog&quot;
    port =&amp;gt;&quot;5544&quot;
  }
}
output {
  stdout { codec=&amp;gt; rubydebug }
  elasticsearch {hosts =&amp;gt; &quot;localhost&quot; }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;配置介绍&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;logstash的配置里，一定要有一个input和一个output。
file: 这里配置输入的文件信息。
syslog：把logstash配置为一个可接收syslog服务器来接收file里变化的数据。
output里定义了两处输出，分别是&lt;strong&gt;stdout&lt;/strong&gt;命令行和&lt;strong&gt;elasticsearch&lt;/strong&gt;。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;启动&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;nohup ./logstash agent -f logstash-indexer.conf &amp;gt; nohup &amp;amp;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;kibana-1&quot;&gt;4.3.Kibana的配置&lt;/h3&gt;
&lt;p&gt;只需要修改kibana.yml中es的地址就可以了。&lt;/p&gt;

&lt;h3 id=&quot;es-1&quot;&gt;4.4.ES&lt;/h3&gt;
&lt;p&gt;ES在这个架构中作为数据存储和索引的系统。无额外的特殊配置。&lt;/p&gt;

&lt;h3 id=&quot;section-13&quot;&gt;5.小结&lt;/h3&gt;
&lt;p&gt;ELK架构在处理运维系统的日志分析以及一些数据量不是很大的场景还是很实用的。快速、简单、易扩展，企业中使用可以考虑使用hdfs作为es的数据存储来使用，具体性能需要根据实际的业务复杂度来衡量，复杂度不是很高的海量数据统计，可优先考虑使用elk方案。&lt;/p&gt;

&lt;p&gt;参考地址：
http://baidu.blog.51cto.com/71938/1676798
https://www.gitbook.com/book/chenryn/kibana-guide-cn/details&lt;/p&gt;

</description>
        <pubDate>Wed, 20 Jul 2016 00:00:00 +0800</pubDate>
        <link>http://leocook.github.io/2016/07/20/ELK%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%E4%BB%8B%E7%BB%8D/</link>
        <guid isPermaLink="true">http://leocook.github.io/2016/07/20/ELK%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%E4%BB%8B%E7%BB%8D/</guid>
        
        <category>es</category>
        
        <category>logstash</category>
        
        <category>kibana</category>
        
        <category>ELK</category>
        
        
        <category>ELK</category>
        
      </item>
    
      <item>
        <title>Tez系列第三篇 Tez和oozie整合</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#section&quot; id=&quot;markdown-toc-section&quot;&gt;1.完成上一篇的基础的相关配置&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#tezjarooziehdfs&quot; id=&quot;markdown-toc-tezjarooziehdfs&quot;&gt;2.拷贝Tez的依赖Jar包到OOZIE的HDFS共享目录下&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#jar&quot; id=&quot;markdown-toc-jar&quot;&gt;3.修改Jar的权限&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-1&quot; id=&quot;markdown-toc-section-1&quot;&gt;4.是配置生效&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#workflowtez&quot; id=&quot;markdown-toc-workflowtez&quot;&gt;5.在workflow里使用Tez&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#workflowhivetez&quot; id=&quot;markdown-toc-workflowhivetez&quot;&gt;5.1.使单个workflow里的hive都用tez&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#workflowhivetez-1&quot; id=&quot;markdown-toc-workflowhivetez-1&quot;&gt;5.2.使单个workflow里的单个hive都用tez&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section&quot;&gt;1.完成上一篇的基础的相关配置&lt;/h2&gt;

&lt;h2 id=&quot;tezjarooziehdfs&quot;&gt;2.拷贝Tez的依赖Jar包到OOZIE的HDFS共享目录下&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hadoop fs -copyFromLocal *.jar /user/oozie/share/lib/lib_20150722203343/hive/
hadoop fs -copyFromLocal /usr/lib/tez/lib/*.jar /user/oozie/share/lib/lib_20150722203343/hive/
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;jar&quot;&gt;3.修改Jar的权限&lt;/h2&gt;
&lt;p&gt;保证oozie有权限读取、使用Jar包:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hadoop fs -chown oozie:oozie /user/oozie/share/lib/lib_20150722203343/hive/*.jar
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;section-1&quot;&gt;4.是配置生效&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;oozie admin -sharelibupdate
oozie admin -shareliblist hive
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;或者重启oozie也可以&lt;/p&gt;

&lt;h2 id=&quot;workflowtez&quot;&gt;5.在workflow里使用Tez&lt;/h2&gt;
&lt;p&gt;这里咱们只是让oozie处理hive作业时使用Tez引擎，具体配置如下.&lt;/p&gt;

&lt;h3 id=&quot;workflowhivetez&quot;&gt;5.1.使单个workflow里的hive都用tez&lt;/h3&gt;
&lt;p&gt;在作业流的hive-site.xml中加入下面的配置，即可使整个作业里的hive都使用Tez引擎：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;hive.execution.engine&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;tez&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;tez.lib.uris&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;${nameNode}/tmp/apps/tez-0.8.2/,${nameNode}/tmp/apps/tez-0.8.2/lib&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;tez.use.cluster.hadoop-libs&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;workflowhivetez-1&quot;&gt;5.2.使单个workflow里的单个hive都用tez&lt;/h3&gt;
&lt;p&gt;上面的配置不用加，在workflow.xml里的hive节点添加如下配置:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;hive.execution.engine&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;tez&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;tez.lib.uris&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;${nameNode}/tmp/apps/tez-0.8.2/,${nameNode}/tmp/apps/tez-0.8.2/lib&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;tez.use.cluster.hadoop-libs&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;hive.execution.engine属性可以不添加，在hive的脚本中的第一行添加:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;set hive.execution.engine=tez;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
        <pubDate>Tue, 17 May 2016 00:00:00 +0800</pubDate>
        <link>http://leocook.github.io/2016/05/17/Tez%E7%B3%BB%E5%88%97%E7%AC%AC%E4%B8%89%E7%AF%87-Tez%E5%92%8Coozie%E6%95%B4%E5%90%88/</link>
        <guid isPermaLink="true">http://leocook.github.io/2016/05/17/Tez%E7%B3%BB%E5%88%97%E7%AC%AC%E4%B8%89%E7%AF%87-Tez%E5%92%8Coozie%E6%95%B4%E5%90%88/</guid>
        
        <category>hadoop</category>
        
        <category>tez</category>
        
        <category>oozie</category>
        
        <category>hive</category>
        
        
        <category>tez</category>
        
      </item>
    
      <item>
        <title>Tez系列第二篇 Hive_on_tez</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#tez&quot; id=&quot;markdown-toc-tez&quot;&gt;1.安装配置Tez&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#section&quot; id=&quot;markdown-toc-section&quot;&gt;1.1.环境要求&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-1&quot; id=&quot;markdown-toc-section-1&quot;&gt;1.2.集群准备&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-2&quot; id=&quot;markdown-toc-section-2&quot;&gt;1.3. 编译环境准备&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#nodejsnpm&quot; id=&quot;markdown-toc-nodejsnpm&quot;&gt;1.4. Nodejs、npm&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#git&quot; id=&quot;markdown-toc-git&quot;&gt;1.5.安装GIT&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#protocolbuffer250&quot; id=&quot;markdown-toc-protocolbuffer250&quot;&gt;1.6. ProtocolBuffer2.5.0&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#tez-1&quot; id=&quot;markdown-toc-tez-1&quot;&gt;1.7.编译&amp;amp;安装Tez&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#hivetez&quot; id=&quot;markdown-toc-hivetez&quot;&gt;2. 开始整合Hive和Tez&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#section-3&quot; id=&quot;markdown-toc-section-3&quot;&gt;2.1. 查看编译完成的目标目录结构&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#tez-082-minimalhdfs&quot; id=&quot;markdown-toc-tez-082-minimalhdfs&quot;&gt;2.1. 拷贝tez-0.8.2-minimal目录至HDFS&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#jar&quot; id=&quot;markdown-toc-jar&quot;&gt;2.2. 拷贝对应以来jar&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#tez-082&quot; id=&quot;markdown-toc-tez-082&quot;&gt;2.3. 把tez-0.8.2拷贝到服务器本地部署的目录&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#conftez-sitexml&quot; id=&quot;markdown-toc-conftez-sitexml&quot;&gt;2.4. 进入部署的目录创建conf/tez-site.xml&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#tez-2&quot; id=&quot;markdown-toc-tez-2&quot;&gt;2.4. 把Tez加入到环境变量&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#hivetez-1&quot; id=&quot;markdown-toc-hivetez-1&quot;&gt;2.5. 让Hive把Tez用起来&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-4&quot; id=&quot;markdown-toc-section-4&quot;&gt;常见的错误&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#sudotez&quot; id=&quot;markdown-toc-sudotez&quot;&gt;1.不要使用sudo权限来编译（编译tez时的错误）&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#mavenfrontend-maven-plugintez&quot; id=&quot;markdown-toc-mavenfrontend-maven-plugintez&quot;&gt;2.maven插件frontend-maven-plugin的版本问题（编译tez时的错误）&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#nodetez&quot; id=&quot;markdown-toc-nodetez&quot;&gt;3.解压Node压缩包时错误（编译tez时的错误）&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#nodenpmtez&quot; id=&quot;markdown-toc-nodenpmtez&quot;&gt;4.node&amp;amp;npm版本不对应（编译tez时的错误）&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#mrerror-when-run-hive-on-tez&quot; id=&quot;markdown-toc-mrerror-when-run-hive-on-tez&quot;&gt;5.缺少MR的依赖包（error when run hive on tez）&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#tezhive-on-oozie-&quot; id=&quot;markdown-toc-tezhive-on-oozie-&quot;&gt;6.tez&amp;amp;hive on oozie 错误&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;本文主要描述Tez的安装配置，以及使用Tez作为Hive的计算引擎时的相关配置。&lt;/p&gt;

&lt;h2 id=&quot;tez&quot;&gt;1.安装配置Tez&lt;/h2&gt;

&lt;h3 id=&quot;section&quot;&gt;1.1.环境要求&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;CDH5.4.4(hadoop2.6.0)&lt;/li&gt;
  &lt;li&gt;编译环境：gcc, gcc-c++, make, build&lt;/li&gt;
  &lt;li&gt;Nodejs、npm (Tez-ui需要)&lt;/li&gt;
  &lt;li&gt;Git&lt;/li&gt;
  &lt;li&gt;pb2.5.0&lt;/li&gt;
  &lt;li&gt;maven3&lt;/li&gt;
  &lt;li&gt;Tez0.8.2&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-1&quot;&gt;1.2.集群准备&lt;/h3&gt;
&lt;p&gt;以及安装完成的cdh5.4.4集群。&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;1.3. 编译环境准备&lt;/h3&gt;
&lt;p&gt;安装gcc, gcc-c++, make, build&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;yum install gcc gcc-c++ libstdc++-devel make build
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;nodejsnpm&quot;&gt;1.4. Nodejs、npm&lt;/h3&gt;
&lt;p&gt;下载源码:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget http://nodejs.org/dist/v0.8.14/node-v0.8.14.tar.gz
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;解压后编译:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./configure
make &amp;amp;&amp;amp; make install
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;查看nodejs和npm的版本:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;node --version
npm --version
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;笔者的安装环境里，node的版本是v0.12.9，npm的版本是2.14.9&lt;/p&gt;

&lt;h3 id=&quot;git&quot;&gt;1.5.安装GIT&lt;/h3&gt;
&lt;p&gt;下载git:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;https://git-scm.com/download    
笔者选择的是1.7.3版本
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;解压后编译:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./configure
make
make install
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;protocolbuffer250&quot;&gt;1.6. ProtocolBuffer2.5.0&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;下载pb源码
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;https://github.com/google/protobuf/releases/tag/v2.5.0
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;编译安装pb
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./configure -prefix=/opt/protoc/
make &amp;amp;&amp;amp; make install
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;配置环境变量
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export PROTOC_HOME=/opt/protoc
export PATH=$PATH:$PROTOC_HOME/bin
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;检验
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;protoc --version
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
    &lt;p&gt;查看pb的版本是不是2.5.0，笔者这里显示为 &lt;strong&gt;libprotoc 2.5.0&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;tez-1&quot;&gt;1.7.编译&amp;amp;安装Tez&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;下载Tez &lt;br /&gt;
Tez所有版本列表在者：&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;http://tez.apache.org/releases/index.html
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;笔者这里下载的是0.8.2版本。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;解压修改配置&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;vi pom.xml&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;hadoop.version&amp;gt;2.6.0-cdh5.4.4&amp;lt;/hadoop.version&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;vi tez-ui/pom.xml&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;nodeVersion&amp;gt;v0.12.9&amp;lt;/nodeVersion&amp;gt;
&amp;lt;npmVersion&amp;gt;2.14.9&amp;lt;/npmVersion&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;开始编译&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mvn clean package -DskipTests=true -Dmaven.javadoc.skip=true  -Dfrontend-maven-plugin.version=0.0.23
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;编译的过程中可能会发生错误，我这边由于网络故障，经常会出现node.gz.tar文件下载失败。最后还是编译成功了。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;hivetez&quot;&gt;2. 开始整合Hive和Tez&lt;/h2&gt;

&lt;h3 id=&quot;section-3&quot;&gt;2.1. 查看编译完成的目标目录结构&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[wulin@lf-R710-29 target]$ ls
archive-tmp  maven-archiver  tez-0.8.2  tez-0.8.2-minimal  tez-0.8.2-minimal.tar.gz  tez-0.8.2.tar.gz  tez-dist-0.8.2-tests.jar
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;tez-082-minimalhdfs&quot;&gt;2.1. 拷贝tez-0.8.2-minimal目录至HDFS&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hdfs dfs -put tez-0.8.2-minimal /tmp/tez-dir/
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;先拷贝到tmp目录做测试，成功运行后在拷贝到正式目录。&lt;/p&gt;

&lt;h3 id=&quot;jar&quot;&gt;2.2. 拷贝对应以来jar&lt;/h3&gt;
&lt;p&gt;把hadoop-mapreduce-client-common-2.6.0-cdh5.4.4.jar到hdfs的/tmp/tez-dir/tez-0.8.2-minimal目录&lt;/p&gt;

&lt;h3 id=&quot;tez-082&quot;&gt;2.3. 把tez-0.8.2拷贝到服务器本地部署的目录&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cp -r tez-0.8.2 /opt/
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;conftez-sitexml&quot;&gt;2.4. 进入部署的目录创建conf/tez-site.xml&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;&lt;/span&gt;
&lt;span class=&quot;cp&quot;&gt;&amp;lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;configuration&amp;gt;&lt;/span&gt;
 &lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
   &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;tez.lib.uris&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
   &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;${fs.defaultFS}/tmp/tez-dir/tez-0.8.2-minimal,${fs.defaultFS}/tmp/tez-dir/tez-0.8.2-minimal/lib&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
 &lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
 &lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
   &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;tez.use.cluster.hadoop-libs&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
   &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;true&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
 &lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/configuration&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;注：tez.lib.uris参数值，是之前上传到hdfs目录的tez包，它必须是tez-0.8.2-minimal目录，而不能是tez-0.8.2目录。（如果有谁使用tez-0.8.2目录部署成功的话，可以告诉我，谢谢！）。根据官网的说明，使用tez-0.8.2-minimal包的时候，务必设置tez.use.cluster.hadoop-libs属性为true。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;tez-2&quot;&gt;2.4. 把Tez加入到环境变量&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export TEZ_JARS=/opt/tez-0.8.2-minimal
export TEZ_CONF_DIR=$TEZ_JARS/conf
export HADOOP_CLASSPATH=${TEZ_CONF_DIR}:${TEZ_JARS}/*:${TEZ_JARS}/lib/*
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;注：经笔者的测试，TEZ_JARS指向tez-0.8.2-minimal目录或者tez-0.8.2目录都是可以的。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;hivetez-1&quot;&gt;2.5. 让Hive把Tez用起来&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;配置整合
临时配置&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hive&amp;gt;set hive.execution.engine=tez;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;或者修改hive-site.xml（长期配置）&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &amp;lt;property&amp;gt;
   &amp;lt;name&amp;gt;hive.execution.engine&amp;lt;/name&amp;gt;
   &amp;lt;value&amp;gt;tez&amp;lt;/value&amp;gt;
 &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;执行hive，验证查看&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hive&amp;gt; select count(*) from dual;
Query ID = wulin_20160406152121_6fd704e7-a437-4345-9958-2fbd1cccb057
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=&amp;lt;number&amp;gt;
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=&amp;lt;number&amp;gt;
In order to set a constant number of reducers:
  set mapreduce.job.reduces=&amp;lt;number&amp;gt;
Starting Job = job_1457012272029_352465, Tracking URL = http://lfh-R710-165:8088/proxy/application_1457012272029_352465/
Kill Command = /opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/bin/hadoop job  -kill job_1457012272029_352465
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2016-04-06 15:21:29,925 Stage-1 map = 0%,  reduce = 0%
2016-04-06 15:21:38,274 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.05 sec
2016-04-06 15:21:45,611 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.86 sec
MapReduce Total cumulative CPU time: 3 seconds 860 msec
Ended Job = job_1457012272029_352465
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.86 sec   HDFS Read: 6138 HDFS Write: 2 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 860 msec
OK
1
Time taken: 35.934 seconds, Fetched: 1 row(s)
hive&amp;gt; set hive.execution.engine=tez;
hive&amp;gt; select count(*) from dual;
Query ID = wulin_20160406152222_426dd505-1f6a-4d02-ae95-5a4d0e6bbc76
Total jobs = 1
Launching Job 1 out of 1
Status: Running (Executing on YARN cluster with App id application_1457012272029_352467)
--------------------------------------------------------------------------------
        VERTICES      STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED
--------------------------------------------------------------------------------
Map 1 ..........   SUCCEEDED      1          1        0        0       0       0
Reducer 2 ......   SUCCEEDED      1          1        0        0       0       0
--------------------------------------------------------------------------------
VERTICES: 02/02  [==========================&amp;gt;&amp;gt;] 100%  ELAPSED TIME: 9.64 s     
--------------------------------------------------------------------------------
OK
1
Time taken: 22.211 seconds, Fetched: 1 row(s)
hive&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;如下图：
&lt;img src=&quot;http://7xriy2.com1.z0.glb.clouddn.com/tez-ok.png&quot; alt=&quot;Hive on Tez&quot; title=&quot;Hive on Tez&quot; /&gt;&lt;/p&gt;

&lt;p&gt;到此，hive on tez，整合完毕！&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;常见的错误&lt;/h2&gt;

&lt;h3 id=&quot;sudotez&quot;&gt;1.不要使用sudo权限来编译（编译tez时的错误）&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[ERROR] Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.3.2:exec (Bower install) on project tez-ui: Command execution failed. Process exited with an error: 1 (Exit value: 1) -&amp;gt; [Help 1]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;解决办法：
不要使用root用户，也不要使用sudo来编译.&lt;/p&gt;

&lt;h3 id=&quot;mavenfrontend-maven-plugintez&quot;&gt;2.maven插件frontend-maven-plugin的版本问题（编译tez时的错误）&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[ERROR] Failed to execute goal com.github.eirslett:frontend-maven-plugin:0.0.22:install-node-and-npm (install node and npm) on project tez-ui: Execution install node and npm of goal com.github.eirslett:frontend-maven-plugin:0.0.22:install-node-and-npm failed: A required class was missing while executing com.github.eirslett:frontend-maven-plugin:0.0.22:install-node-and-npm: org/slf4j/helpers/MarkerIgnoringBase
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;解决办法：强制执行编译时frontend-maven-plugin插件的版本（mvn clean package -DskipTests=true -Dmaven.javadoc.skip=true  -Dfrontend-maven-plugin.version=0.0.XX）
如果maven的版本低于3.1，frontend-maven-plugin版本应该 &amp;lt;= 0.0.22；
如果maven的版本大于或等于3.1，frontend-maven-plugin版本应该 &amp;gt;= 0.0.23.&lt;/p&gt;

&lt;h3 id=&quot;nodetez&quot;&gt;3.解压Node压缩包时错误（编译tez时的错误）&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[ERROR] Failed to execute goal com.github.eirslett:frontend-maven-plugin:0.0.22:install-node-and-npm (install node and npm) on project tez-ui: Could not extract the Node archive: Could not extract archive: '/home/.../tez/tez-ui/src/main/webapp/node_tmp/node.tar.gz': EOFException -&amp;gt; [Help 1]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;解决版本：检查第二个问题，并重新执行。如果还失败，可以多执行几次，可能和网络有关系。&lt;/p&gt;

&lt;h3 id=&quot;nodenpmtez&quot;&gt;4.node&amp;amp;npm版本不对应（编译tez时的错误）&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[ERROR] npm WARN engine hoek@2.16.3: wanted: {&quot;node&quot;:&quot;&amp;gt;=0.10.40&quot;} (current: {&quot;node&quot;:&quot;v0.10.18&quot;,&quot;npm&quot;:&quot;1.3.8&quot;})
[ERROR] npm WARN engine boom@2.10.1: wanted: {&quot;node&quot;:&quot;&amp;gt;=0.10.40&quot;} (current: {&quot;node&quot;:&quot;v0.10.18&quot;,&quot;npm&quot;:&quot;1.3.8&quot;})
[ERROR] npm WARN engine cryptiles@2.0.5: wanted: {&quot;node&quot;:&quot;&amp;gt;=0.10.40&quot;} (current: {&quot;node&quot;:&quot;v0.10.18&quot;,&quot;npm&quot;:&quot;1.3.8&quot;})
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;解决办法：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;安装正确版本的nodeJs；&lt;/li&gt;
  &lt;li&gt;修改tez-ui/pom.xml中的nodeVersion和npmVersion标签值为系统环境的值。可使用下面命令，查看系统里的node和npm版本：
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;node --version
npm --version
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;mrerror-when-run-hive-on-tez&quot;&gt;5.缺少MR的依赖包（error when run hive on tez）&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Vertex failed, vertexName=Map 1, vertexId=vertex_1457012272029_352429_1_00, diagnostics=[Vertex vertex_1457012272029_352429_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: dual initializer failed, vertex=vertex_1457012272029_352429_1_00 [Map 1], java.lang.NoClassDefFoundError: org/apache/hadoop/mapred/MRVersion
        at org.apache.hadoop.hive.shims.Hadoop23Shims.isMR2(Hadoop23Shims.java:843)
        at org.apache.hadoop.hive.shims.Hadoop23Shims.getHadoopConfNames(Hadoop23Shims.java:914)
        at org.apache.hadoop.hive.conf.HiveConf$ConfVars.&amp;lt;clinit&amp;gt;(HiveConf.java:356)
        at org.apache.hadoop.hive.ql.exec.Utilities.getBaseWork(Utilities.java:371)
        at org.apache.hadoop.hive.ql.exec.Utilities.getMapWork(Utilities.java:296)
        at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:106)
        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:278)
        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:269)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:269)
        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:253)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.mapred.MRVersion
        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
        ... 17 more
]
Vertex killed, vertexName=Reducer 2, vertexId=vertex_1457012272029_352429_1_01, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1457012272029_352429_1_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]
DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;这个错误是在配置完之后，运行hive时才会出现的。
解决办法：
拷贝mr依赖包至tez的hdfs目录中。笔者的环境是CDH5.4.4，所以把hadoop-mapreduce-client-common-2.6.0-cdh5.4.4.jar拷贝到hdfs的/tmp/tez-dir/tez-0.8.2-minimal目录，就解决问题了。&lt;/p&gt;

&lt;h3 id=&quot;tezhive-on-oozie-&quot;&gt;6.tez&amp;amp;hive on oozie 错误&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Status: Running (Executing on YARN cluster with App id application_1461470184587_0770)

Map 1: -/-	Reducer 2: 0/1	
Status: Failed
Vertex failed, vertexName=Map 1, vertexId=vertex_1461470184587_0770_1_00, diagnostics=[Vertex vertex_1461470184587_0770_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: wl_manager_core_assembly initializer failed, vertex=vertex_1461470184587_0770_1_00 [Map 1], java.lang.IllegalArgumentException: Illegal Capacity: -1
	at java.util.ArrayList.&amp;lt;init&amp;gt;(ArrayList.java:142)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:330)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:306)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:408)
	at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:129)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:278)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:269)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:269)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:253)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
]
Vertex killed, vertexName=Reducer 2, vertexId=vertex_1461470184587_0770_1_01, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1461470184587_0770_1_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]
DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask
Intercepting System.exit(2)
Failing Oozie Launcher, Main class [org.apache.oozie.action.hadoop.HiveMain], exit code [2]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;参考链接：
http://m.oschina.net/blog/421764  &lt;br /&gt;
http://duguyiren3476.iteye.com/blog/2214549
https://cwiki.apache.org/confluence/display/TEZ/Build+errors+and+solutions&lt;/p&gt;

</description>
        <pubDate>Mon, 09 May 2016 00:00:00 +0800</pubDate>
        <link>http://leocook.github.io/2016/05/09/Tez%E7%B3%BB%E5%88%97%E7%AC%AC%E4%BA%8C%E7%AF%87-hive_on_tez/</link>
        <guid isPermaLink="true">http://leocook.github.io/2016/05/09/Tez%E7%B3%BB%E5%88%97%E7%AC%AC%E4%BA%8C%E7%AF%87-hive_on_tez/</guid>
        
        <category>hadoop</category>
        
        <category>tez</category>
        
        <category>hive</category>
        
        
        <category>tez</category>
        
      </item>
    
      <item>
        <title>Tez系列第一篇 基础常识</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#tez&quot; id=&quot;markdown-toc-tez&quot;&gt;1.Tez是什么&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#section&quot; id=&quot;markdown-toc-section&quot;&gt;1.1.介绍&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-1&quot; id=&quot;markdown-toc-section-1&quot;&gt;1.2.两大优势&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#tez-1&quot; id=&quot;markdown-toc-tez-1&quot;&gt;2.为什么要有Tez&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#yarnam&quot; id=&quot;markdown-toc-yarnam&quot;&gt;2.1.YARN的AM&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#yarn&quot; id=&quot;markdown-toc-yarn&quot;&gt;2.2.YARN的资源无法重用&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#yarndag&quot; id=&quot;markdown-toc-yarndag&quot;&gt;2.3.YARN的DAG中间计算结果读写效率低下&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#tez-2&quot; id=&quot;markdown-toc-tez-2&quot;&gt;3.Tez能解决什么问题&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#amamampoolserver&quot; id=&quot;markdown-toc-amamampoolserver&quot;&gt;3.1.使用AM缓冲池实现AM的复用，AMPoolServer&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#container&quot; id=&quot;markdown-toc-container&quot;&gt;3.2.Container预启动&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#container-1&quot; id=&quot;markdown-toc-container-1&quot;&gt;3.3.Container重用&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;本文主要围绕着这么几个问题来展开：Tez是什么？为什么要有Tez？Tez能解决什么问题？&lt;/p&gt;

&lt;h2 id=&quot;tez&quot;&gt;1.Tez是什么&lt;/h2&gt;

&lt;h3 id=&quot;section&quot;&gt;1.1.介绍&lt;/h3&gt;
&lt;p&gt;Tez目标是用来构建复杂的有向五环图数据处理程序。Tez项目目前是构建在YARN之上的。详情可以查看Tez的官网：http://tez.apache.org/&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;1.2.两大优势&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;用户体验&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;使用API来自定义数据流&lt;/li&gt;
  &lt;li&gt;灵活的Input-Processor-Output运行模式&lt;/li&gt;
  &lt;li&gt;与计算的数据类型无关&lt;/li&gt;
  &lt;li&gt;简单的部署流程&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;计算性能&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;性能高于MapReduce&lt;/li&gt;
  &lt;li&gt;资源管理更加优化&lt;/li&gt;
  &lt;li&gt;运行时配置预加载&lt;/li&gt;
  &lt;li&gt;物理数据流动态运行&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;举例&lt;/strong&gt;  &lt;br /&gt;
下图是一个基于MR的Hive/Pig的DAG数据流处理过程:  &lt;br /&gt;
&lt;img src=&quot;http://7xriy2.com1.z0.glb.clouddn.com/tez01-PigHiveQueryOnMR.png&quot; alt=&quot;Hive/Pig&quot; title=&quot;Hive/Pig的DAG&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下图是一个基于Tez的Hive/Pig的DAG数据流处理过程:  &lt;br /&gt;
&lt;img src=&quot;http://7xriy2.com1.z0.glb.clouddn.com/tez02-PigHiveQueryOnTez.png&quot; alt=&quot;Hive/Pig&quot; title=&quot;Hive/Pig的DAG&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tez-1&quot;&gt;2.为什么要有Tez&lt;/h2&gt;

&lt;h3 id=&quot;yarnam&quot;&gt;2.1.YARN的AM&lt;/h3&gt;

&lt;p&gt;YARN的每个作业在执行前都会先创建一个AM，然后才会开始正真的计算。这样处理小作业的时候，会有较大的延迟，而且还会造成极大的性能浪费。&lt;/p&gt;

&lt;h3 id=&quot;yarn&quot;&gt;2.2.YARN的资源无法重用&lt;/h3&gt;
&lt;p&gt;在MR1中，用户可以开启JVM重用，用来降低作业延迟。
但是在YARN中，每个作业的AM会先向RM申请资源（Container），申请到资源之后开始运行作业，作业处理完成后释放资源，期间没有资源重新利用的环节。这样会使作业大大的延迟。&lt;/p&gt;

&lt;h3 id=&quot;yarndag&quot;&gt;2.3.YARN的DAG中间计算结果读写效率低下&lt;/h3&gt;
&lt;p&gt;可以查看1.2中的图“&lt;strong&gt;基于MR的Hive/Pig的DAG数据流处理过程&lt;/strong&gt;”，可以看出图中的每一节点都是把结果写到一个中间存储（HDFS/S3）中，下个节点从中间存储读取数据，再来继续接下来的计算。可见中间存储的读写性能对整个DAG的性能影响是很大的。  &lt;br /&gt;
如果使用Tez，则可以省去中间存储的读写，上个节点的输出可以直接重定向到下个节点的输入。&lt;/p&gt;

&lt;h2 id=&quot;tez-2&quot;&gt;3.Tez能解决什么问题&lt;/h2&gt;

&lt;h3 id=&quot;amamampoolserver&quot;&gt;3.1.使用AM缓冲池实现AM的复用，AMPoolServer&lt;/h3&gt;

&lt;p&gt;使用Tez后，yarn的作业不是先提交给RM了，而是提交给AMPS。AMPS在启动后，会预先创建若干个AM，作为AM资源池，当作业被提交到AMPS的时候，AMPS会把该作业直接提交到AM上，这样就避免每个作业都创建独立的AM，大大的提高了效率。&lt;/p&gt;

&lt;h3 id=&quot;container&quot;&gt;3.2.Container预启动&lt;/h3&gt;
&lt;p&gt;AM缓冲池中的每个AM在启动时都会预先创建若干个container，以此来减少因创建container所话费的时间。&lt;/p&gt;

&lt;h3 id=&quot;container-1&quot;&gt;3.3.Container重用&lt;/h3&gt;
&lt;p&gt;每个任务运行完之后，AM不会立马释放Container，而是将它分配给其它未执行的任务。  &lt;br /&gt;
看到这里， Tez是什么？为什么要有Tez？Tez能解决什么问题？应该都知道了吧！下一篇来开始讲解正式环境中的使用。&lt;/p&gt;

</description>
        <pubDate>Fri, 01 Apr 2016 00:00:00 +0800</pubDate>
        <link>http://leocook.github.io/2016/04/01/Tez%E7%B3%BB%E5%88%97%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E5%B8%B8%E8%AF%86/</link>
        <guid isPermaLink="true">http://leocook.github.io/2016/04/01/Tez%E7%B3%BB%E5%88%97%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E5%B8%B8%E8%AF%86/</guid>
        
        <category>hadoop</category>
        
        <category>tez</category>
        
        
        <category>tez</category>
        
      </item>
    
      <item>
        <title>Hadoop优化 Yarn</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#section&quot; id=&quot;markdown-toc-section&quot;&gt;1.之前的集群存在的问题&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#section-1&quot; id=&quot;markdown-toc-section-1&quot;&gt;1.1.问题一：作业的执行速率不同步&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-2&quot; id=&quot;markdown-toc-section-2&quot;&gt;1.2.问题二：资源利用倾斜&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-3&quot; id=&quot;markdown-toc-section-3&quot;&gt;1.3.问题三：集群资源并没有真正的参与计算&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-4&quot; id=&quot;markdown-toc-section-4&quot;&gt;2.集群资源重新划分的过程&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#section-5&quot; id=&quot;markdown-toc-section-5&quot;&gt;2.1.拿到集群中所有机器的硬件资源列表&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-6&quot; id=&quot;markdown-toc-section-6&quot;&gt;2.2.根据集群资源，分组的大概情况如截图：&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-7&quot; id=&quot;markdown-toc-section-7&quot;&gt;2.3.资源划分的策略&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-8&quot; id=&quot;markdown-toc-section-8&quot;&gt;2.4.具体的划分策略&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#section-9&quot; id=&quot;markdown-toc-section-9&quot;&gt;2.4.1.内存划分策略&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#section-10&quot; id=&quot;markdown-toc-section-10&quot;&gt;2.4.2.内存划分策略&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#mysql&quot; id=&quot;markdown-toc-mysql&quot;&gt;3.mysql调优的过程&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-11&quot; id=&quot;markdown-toc-section-11&quot;&gt;4.成果&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#section-12&quot; id=&quot;markdown-toc-section-12&quot;&gt;4.1.集群表现情况&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#mysql-1&quot; id=&quot;markdown-toc-mysql-1&quot;&gt;4.2.mysql表现情况&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-13&quot; id=&quot;markdown-toc-section-13&quot;&gt;5.总结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;集群优化这块一直是一个比较麻烦的事情，目前由于集群的资源分配问题，已经出现了几次作业故障，有必要好这块的东西重新梳理一下。经过３天的测试，最终找到了对于目前环境相对适合的参数，目前集群已经11*24无节点故障了，先在这里做一些简单的分享吧。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;1.之前的集群存在的问题&lt;/h1&gt;

&lt;h2 id=&quot;section-1&quot;&gt;1.1.问题一：作业的执行速率不同步&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;问题表现
部分任务跑的慢，部分任务跑得快。&lt;/li&gt;
  &lt;li&gt;问题的原因
集群资源分配不合理，出现有低配的机器运行作业数较多，高配机器运行作业数较少的情况。&lt;/li&gt;
  &lt;li&gt;解决办法
重新分配角色组，使得低配机器参与相对较少的计算，高配机器参与相对较多的计算。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-2&quot;&gt;1.2.问题二：资源利用倾斜&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;问题表现
部分机器资源利用率极高，部分机器资源利用率级低；&lt;/li&gt;
  &lt;li&gt;问题的原因
&lt;strong&gt;同【问题一】&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;解决办法
&lt;strong&gt;同【问题一】&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-3&quot;&gt;1.3.问题三：集群资源并没有真正的参与计算&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;问题表现
作业个数较多的时候，出现集群资源分配完了，但是集群负载极低，作业执行极缓慢。&lt;/li&gt;
  &lt;li&gt;问题的原因
咱们的ETL结果报表使用的是单节点mysql，大量的小文件写操作使得磁盘的IO成为了严重的性能瓶颈，所以每个导数据的任务执行的较缓慢，导数据的作业长时间占用计算资源，计算任务执行的较为缓慢。&lt;/li&gt;
  &lt;li&gt;解决办法
a). 把mysql中的数据库存到不同的磁盘上的，降低单个磁盘的负载。
b). 减少单个任务的资源占用，提高集群的并行度。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-4&quot;&gt;2.集群资源重新划分的过程&lt;/h1&gt;

&lt;h2 id=&quot;section-5&quot;&gt;2.1.拿到集群中所有机器的硬件资源列表&lt;/h2&gt;
&lt;p&gt;感谢运维同学的帮助！&lt;/p&gt;

&lt;h2 id=&quot;section-6&quot;&gt;2.2.根据集群资源，分组的大概情况如截图：&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;http://7xriy2.com1.z0.glb.clouddn.com/%E5%88%86%E7%BB%84%E5%88%97%E8%A1%A8.png&quot; alt=&quot;服务器分组情况&quot; title=&quot;根据机器硬件资源情况，服务器分组情况&quot; /&gt;
分组命名规则：
&lt;strong&gt;&lt;em&gt;NM&lt;/em&gt;&lt;/strong&gt;: NodeManager；  &lt;br /&gt;
&lt;strong&gt;&lt;em&gt;G01&lt;/em&gt;&lt;/strong&gt;: Group01；  &lt;br /&gt;
&lt;strong&gt;&lt;em&gt;C08&lt;/em&gt;&lt;/strong&gt;: cpu是8核；  &lt;br /&gt;
&lt;strong&gt;&lt;em&gt;M48&lt;/em&gt;&lt;/strong&gt;: 内存是48GB。  &lt;br /&gt;
&lt;strong&gt;&lt;em&gt;ZK&lt;/em&gt;&lt;/strong&gt;: 机器上安装了ZK，如果没有这一项，默认该机器上只安装了HbaseRegion Server、HDFS DataNode、Impala Daemon和Yarn NodeManager这四个角色。（如果某台机器上只安装了一个测试的zk，则可忽略该角色的资源占用，若该角色占用资源较多，那么就应该把这台机器单独拿出来分组）&lt;/p&gt;

&lt;h2 id=&quot;section-7&quot;&gt;2.3.资源划分的策略&lt;/h2&gt;
&lt;p&gt;根据机器上安装的服务，大概给服务做了如下的划分：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;安装有重要服务的机器，可参与计算
例如安装了OOZIE、ResourcesManager、NodeManager的节点，当它们故障时，对集群来说，将可能会是一场灾难，所以不让这些机器参与计算，保证这些服务的稳定。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;安装有重要服务的机器，不参与计算
例如安装了FLUME、KAFKA或ZK的节点，由于它们本身就是可以配置分布式执行的，当其中一个服务出现故障时，对业务的影响是较小的，甚至没有。所以允许这些节点和参与计算的节点安装在同一台主机上。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;只安装了存储和计算的机器
例如HbaseRegion Server、HDFS DataNode、Impala Daemon和Yarn NodeManager，不会因为一台机器的故障导致集群出现灾难。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-8&quot;&gt;2.4.具体的划分策略&lt;/h2&gt;

&lt;h3 id=&quot;section-9&quot;&gt;2.4.1.内存划分策略&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;yarn容器可直接管理的资源
主机中内存*0.8 - 7GB（Hbase）- 7GB（HDFS），具体根据集群规模，hdfs、hbase的环境来定。有的几点还安装了其它服务，具体需要观察集群环境。&lt;/li&gt;
  &lt;li&gt;单个任务可使用的任务资源
map任务划分1GB，reduce任务划分2GB，JVM虚拟机分别设置为他们70%。
    &lt;h3 id=&quot;section-10&quot;&gt;2.4.2.内存划分策略&lt;/h3&gt;
  &lt;/li&gt;
  &lt;li&gt;yarn容器可直接管理的资源
对于只安装了HRS、DN、ID、NM的节点，vcore总数设置为（cpu核数-1）的2倍，具体根据cpu的计算性能来定（减一时预留给系统的）。如果节点上安装了一些会消耗CPU的服务，那么就设置vcore总数为cpu核数/2。如果安装了一些对CPU消耗不是非常大的服务，例如ZK，那么就设置vcore总是为（cpu的核数-1）。&lt;/li&gt;
  &lt;li&gt;单个任务可使用的任务资源
1个vcore。
    &lt;h1 id=&quot;mysql&quot;&gt;3.mysql调优的过程&lt;/h1&gt;
    &lt;p&gt;不同的库，挂载在不同的磁盘上，减小单块盘的压力。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-11&quot;&gt;4.成果&lt;/h1&gt;

&lt;h2 id=&quot;section-12&quot;&gt;4.1.集群表现情况&lt;/h2&gt;
&lt;p&gt;到目前为止，已超过72小时NodeManager未出现过故障了，待考察一周。&lt;/p&gt;
&lt;h2 id=&quot;mysql-1&quot;&gt;4.2.mysql表现情况&lt;/h2&gt;
&lt;p&gt;优化前的负载情况入下图：
&lt;img src=&quot;http://7xriy2.com1.z0.glb.clouddn.com/mysql_befor.png&quot; alt=&quot;优化前&quot; title=&quot;优化前负载情况&quot; /&gt;
优化后的负载情况入下图：
&lt;img src=&quot;http://7xriy2.com1.z0.glb.clouddn.com/mysql.png&quot; alt=&quot;优化后&quot; title=&quot;优化后负载情况&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;后端导数据速度有明显加快，但是SDA盘的负载还是明显略高于SDB的负载。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;section-13&quot;&gt;5.总结&lt;/h1&gt;
&lt;p&gt;在摸索这个问题上花费了比较多的时间，目前的优化方案满足现在的业务场景。在集群优化这方边，在个人现在能看到的未来，还有很多可以优化的项。例如：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Impala的资源管理未使用yarn，所以一直还没有开始使用；&lt;/li&gt;
  &lt;li&gt;OOZIE未做HA配置；&lt;/li&gt;
  &lt;li&gt;HDFS数据平衡效果不是很好；&lt;/li&gt;
  &lt;li&gt;CPU一个线程做两个vcore使用，压力还是比较大的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在接下，将会按照优先级逐一解决。&lt;/p&gt;

</description>
        <pubDate>Sun, 13 Mar 2016 00:00:00 +0800</pubDate>
        <link>http://leocook.github.io/2016/03/13/hadoop%E4%BC%98%E5%8C%96-yarn/</link>
        <guid isPermaLink="true">http://leocook.github.io/2016/03/13/hadoop%E4%BC%98%E5%8C%96-yarn/</guid>
        
        <category>hadoop</category>
        
        <category>集群优化</category>
        
        
        <category>hadoop</category>
        
      </item>
    
      <item>
        <title>Hadoop优化 概述</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#section&quot; id=&quot;markdown-toc-section&quot;&gt;1.应用程序角度进行优化&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#reduce&quot; id=&quot;markdown-toc-reduce&quot;&gt;1.1.减少不必要的reduce任务&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-1&quot; id=&quot;markdown-toc-section-1&quot;&gt;1.2.外部文件引用&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#combiner&quot; id=&quot;markdown-toc-combiner&quot;&gt;1.3.使用Combiner&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#writable&quot; id=&quot;markdown-toc-writable&quot;&gt;1.4.使用合适的Writable类型&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#java&quot; id=&quot;markdown-toc-java&quot;&gt;1.5.尽可能的少创建新的Java对象&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#linux&quot; id=&quot;markdown-toc-linux&quot;&gt;2. Linux系统层面上的配置调优&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#section-2&quot; id=&quot;markdown-toc-section-2&quot;&gt;2.1. 文件系统的配置&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#linux-1&quot; id=&quot;markdown-toc-linux-1&quot;&gt;2.2. Linux文件系统预读缓冲区大小&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#raidlvm&quot; id=&quot;markdown-toc-raidlvm&quot;&gt;2.3. 去除RAID和LVM&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-3&quot; id=&quot;markdown-toc-section-3&quot;&gt;2.4. 增大同时打开的文件数和网络连接数&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#swap&quot; id=&quot;markdown-toc-swap&quot;&gt;2.5. 关闭swap分区&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#io&quot; id=&quot;markdown-toc-io&quot;&gt;2.6. I/O调度器选择&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#hadoop&quot; id=&quot;markdown-toc-hadoop&quot;&gt;3. Hadoop平台内参数调优&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#section-4&quot; id=&quot;markdown-toc-section-4&quot;&gt;3.1. 计算资源优化&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-5&quot; id=&quot;markdown-toc-section-5&quot;&gt;3.2. 节点间的通信优化&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-6&quot; id=&quot;markdown-toc-section-6&quot;&gt;3.3. 磁盘块的配置优化&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#rpc-handlerhttp&quot; id=&quot;markdown-toc-rpc-handlerhttp&quot;&gt;3.4. RPC Handler个数和Http线程数优化&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-7&quot; id=&quot;markdown-toc-section-7&quot;&gt;3.5. 选择合适的压缩算法&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-8&quot; id=&quot;markdown-toc-section-8&quot;&gt;3.6. 启用批量任务调度(现在新版本都默认支持了)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#apache&quot; id=&quot;markdown-toc-apache&quot;&gt;3.7. 启用预读机制(Apache暂时没有)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#hdfs&quot; id=&quot;markdown-toc-hdfs&quot;&gt;3.8.HDFS相关参数优化&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-9&quot; id=&quot;markdown-toc-section-9&quot;&gt;4.系统实现角度调优&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#section-10&quot; id=&quot;markdown-toc-section-10&quot;&gt;4.1. 调度延迟&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-11&quot; id=&quot;markdown-toc-section-11&quot;&gt;4.2. 可移植性&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#prefetchingpreshuffling&quot; id=&quot;markdown-toc-prefetchingpreshuffling&quot;&gt;4.3. 优化策略：Prefetching与preshuffling&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这篇文章是我开始涉及做集群相关优化时的第一篇笔记，内容比较浅显易懂，适合想了解集群优化的朋友阅读。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;1.应用程序角度进行优化&lt;/h2&gt;

&lt;h3 id=&quot;reduce&quot;&gt;1.1.减少不必要的reduce任务&lt;/h3&gt;
&lt;p&gt;若对于同一份数据需要多次处理，可以尝试先排序、分区，然后自定义InputSplit将某一个分区作为一个Map的输入，在Map中处理数据，将Reduce的个数设置为空。&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;1.2.外部文件引用&lt;/h3&gt;
&lt;p&gt;如字典、配置文件等需要在Task之间共享的数据，可使用分布式缓存DistributedCache或者使用-files&lt;/p&gt;

&lt;h3 id=&quot;combiner&quot;&gt;1.3.使用Combiner&lt;/h3&gt;
&lt;p&gt;combiner是发生在map端的，作用是归并Map端输出的文件，这样Map端输出的数据量就小了，减少了Map端和reduce端间的数据传输。需要注意的是，Combiner不能影响作业的结果;不是每个MR都可以使用Combiner的，需要根据具体业务来定;Combiner是发生在Map端的，不能垮Map来执行（只有Reduce可以接收多个Map任务的输出数据）&lt;/p&gt;

&lt;h3 id=&quot;writable&quot;&gt;1.4.使用合适的Writable类型&lt;/h3&gt;
&lt;p&gt;尽可能使用二进制的Writable类型，例如：IntWritable， FloatWritable等，而不是Text。因为在一个批处理系统中将数值转换为文本时低效率的。使用二进制的Writable类型可以降低cpu资源的消耗，也可以减少Map端中间数据、结果数据占用的空间。&lt;/p&gt;

&lt;h3 id=&quot;java&quot;&gt;1.5.尽可能的少创建新的Java对象&lt;/h3&gt;
&lt;p&gt;a)需要注意的Writable对象，例如下面的写法：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;public void map(...) {
…
for (String word : words) {
    output.collect(new Text(word), new IntWritable(1));
} }
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;这样会冲去创建对象new Text(word)和new IntWritable(1))，这样可能会产生海量的短周期对象。更高效的写法见下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class MyMapper … {
Text wordText = new Text();
IntWritable one = new IntWritable(1);
public void map(...) {
    for (String word: words) {
    wordText.set(word);
        output.collect(wordText, one);
    }
} }
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;b)对于可变字符串，使用StringBuffer而不是String&lt;/p&gt;

&lt;p&gt;String类是经过final修饰的，那么每次对它的修改都会产生临时对象，而SB则不会。&lt;/p&gt;

&lt;h2 id=&quot;linux&quot;&gt;2. Linux系统层面上的配置调优&lt;/h2&gt;

&lt;h3 id=&quot;section-2&quot;&gt;2.1. 文件系统的配置&lt;/h3&gt;
&lt;p&gt;a) 关闭文件在被操作时会记下时间戳:noatime和nodiratime &lt;br /&gt;
b) 选择I/O性能较好的文件系统（Hadoop比较依赖本地的文件系统）&lt;/p&gt;

&lt;h3 id=&quot;linux-1&quot;&gt;2.2. Linux文件系统预读缓冲区大小&lt;/h3&gt;
&lt;p&gt;命令:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;blockdev
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;raidlvm&quot;&gt;2.3. 去除RAID和LVM&lt;/h3&gt;

&lt;h3 id=&quot;section-3&quot;&gt;2.4. 增大同时打开的文件数和网络连接数&lt;/h3&gt;
&lt;p&gt;ulimit net.core.somaxconn&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ulimit net.core.somaxconn
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;swap&quot;&gt;2.5. 关闭swap分区&lt;/h3&gt;
&lt;p&gt;在Hadoop中，对于每个作业处理的数据量和每个Task中用到的各种缓冲，用户都是完全可控的。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/etc/sysctl.conf
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;io&quot;&gt;2.6. I/O调度器选择&lt;/h3&gt;
&lt;p&gt;详情见AMD的白皮书&lt;/p&gt;

&lt;h2 id=&quot;hadoop&quot;&gt;3. Hadoop平台内参数调优&lt;/h2&gt;
&lt;p&gt;Hadoop相关可配置参数共有几百个，但是其中只有三十个左右会对其性能产生显著影响。&lt;/p&gt;

&lt;h3 id=&quot;section-4&quot;&gt;3.1. 计算资源优化&lt;/h3&gt;
&lt;p&gt;a) 设置合理的slot（资源槽位）&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mapred.tasktracker.map.tasks.maximum / mapred.tasktracker.reduce.tasks.maximum
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;参数说明：每个TaskTracker上可并发执行的Map Task和Reduce Task数目 &lt;br /&gt;
默认值：都是2 &lt;br /&gt;
推荐值：根据具体的节点资源来看，推荐值是(core_per_node)/2~2*(cores_per_node) &lt;br /&gt;
单位：无&lt;/p&gt;

&lt;h3 id=&quot;section-5&quot;&gt;3.2. 节点间的通信优化&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;a) TaskTracker和JobTracker之间的心跳间隔&lt;/strong&gt; &lt;br /&gt;
这个值太小的话，在一个大集群中会造成JobTracker需要处理高并发心跳，可能会有很大的压力。 &lt;br /&gt;
建议集群规模小于300时，使用默认值3秒，在此基础上，集群规模每增加100台，会加1秒。 &lt;br /&gt;
&lt;strong&gt;b) 启用带外心跳(out-of-band heartbeat)&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mapreduce.tasktracker.outofband.heartbeat
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;参数说明：主要是为了减少任务分配延迟。它与常规心跳不同，一般的心跳是一定时间间隔发送的，而带外心跳是在任务运行结束或是失败时发送，这样就能在TaskTracker节点出现空闲资源的时候能第一时间通知JobTracker。&lt;/p&gt;

&lt;h3 id=&quot;section-6&quot;&gt;3.3. 磁盘块的配置优化&lt;/h3&gt;
&lt;p&gt;a) 作业相关的磁盘配置&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mapred.local.dir
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;参数说明：map本地计算时所用到的目录，建议配置在多块硬盘上 &lt;br /&gt;
b) 存储相关的磁盘配置（HDFS数据存储） &lt;br /&gt;
dfs.data.dir &lt;br /&gt;
参数说明：HDFS的数据存储目录，建议配置在多块硬盘上，可提高整体IO性能 &lt;br /&gt;
例如：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;property&amp;gt;
 &amp;lt;name&amp;gt;dfs.name.dir&amp;lt;/name&amp;gt;
 &amp;lt;value&amp;gt;/data1/hadoopdata/mapred/jt/,/data2/hadoopdata/mapred/jt/&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;c) 存储相关的磁盘配置（HDFS元数据存储）&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dfs.name.dir
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;参数说明：HDFS的元数据存储目录，建议设置多目录，每个多目录都可保存元数据的一个备份 &lt;br /&gt;
注：要想提升hadoop整体IO性能，对于hadoop中用到的所有文件目录，都需要评估它磁盘IO的负载，对于IO负载可能会高的目录，最好都配置到多个磁盘上，以提示IO性能&lt;/p&gt;

&lt;h3 id=&quot;rpc-handlerhttp&quot;&gt;3.4. RPC Handler个数和Http线程数优化&lt;/h3&gt;
&lt;p&gt;a) RPC Handler个数&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mapred.job.tracker.handler.count
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;参数说明：JobTracker需要并发的处理来自各个TaskTracker的RPC请求，可根据集群规模和并发数来调整RPC Handler的个数。 &lt;br /&gt;
默认值：10 &lt;br /&gt;
推荐值：60-70，最少要是TaskTracker个数的4% &lt;br /&gt;
单位：无 &lt;br /&gt;
b) Http线程数
   &lt;br /&gt;
tasktracker.http.threads&lt;/p&gt;

&lt;p&gt;在Shuffle阶段，Reduce Task会通过Http请求从各个TaskTracker上读取Map Task的结果，TaskTracker是使用Jetty Server来提供服务的，这里可适量调整Jetty Server的工作线程以提高它的并发处理能力。 &lt;br /&gt;
默认值：40 &lt;br /&gt;
推荐值：50-80+&lt;/p&gt;

&lt;h3 id=&quot;section-7&quot;&gt;3.5. 选择合适的压缩算法&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mapred.compress.map.output / Mapred.output.compress
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;map输出的中间结果时需要进行压缩的，指定压缩方式&lt;strong&gt;（Mapred.compress.map.output.codec/ Mapred.output.compress.codec）&lt;/strong&gt;。推荐使用LZO压缩。&lt;/p&gt;

&lt;h3 id=&quot;section-8&quot;&gt;3.6. 启用批量任务调度(现在新版本都默认支持了)&lt;/h3&gt;
&lt;p&gt;a) Fair Scheduler&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mapred.fairscheduler.assignmultiple
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;b) Capacity Scheduler&lt;/p&gt;

&lt;h3 id=&quot;apache&quot;&gt;3.7. 启用预读机制(Apache暂时没有)&lt;/h3&gt;
&lt;p&gt;Hadoop是顺序读，所以预读机制可以很明显的提高HDFS的读性能。
HDFS预读：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dfs.datanode.readahead ：true
dfs.datanode.readahead.bytes ：4MB
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;shuffle预读&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mapred.tasktracker.shuffle.fadvise : true
mapred.tasktracker.shuffle.readahead.bytes : 4MB
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;hdfs&quot;&gt;3.8.HDFS相关参数优化&lt;/h3&gt;
&lt;p&gt;1) dfs.replication &lt;br /&gt;
参数说明：hdfs文件副本数 &lt;br /&gt;
默认值：3 &lt;br /&gt;
推荐值：3-5（对于IO较为密集的场景可适量增大） &lt;br /&gt;
单位：无 &lt;br /&gt;
2) dfs.blocksize &lt;br /&gt;
参数说明： &lt;br /&gt;
默认值：67108864(64MB) &lt;br /&gt;
推荐值：稍大型集群建议设为128MB(134217728)或256MB(268435456) &lt;br /&gt;
单位：无 &lt;br /&gt;
3) dfs.datanode.handler.count &lt;br /&gt;
参数说明：DateNode上的服务线程数 &lt;br /&gt;
默认值：10 &lt;br /&gt;
推荐值： &lt;br /&gt;
单位：无 &lt;br /&gt;
4) fs.trash.interval &lt;br /&gt;
参数说明：HDFS文件删除后会移动到垃圾箱，该参数时清理垃圾箱的时间 &lt;br /&gt;
默认值：0 &lt;br /&gt;
推荐值：1440(1day) &lt;br /&gt;
单位：无 &lt;br /&gt;
5) io.sort.factor &lt;br /&gt;
参数说明：当一个map task执行完之后，本地磁盘上(mapred.local.dir)有若干个spill文件，map task最后做的一件事就是执行merge sort，把这些spill文件合成一个文件（partition）。执行merge sort的时候，每次同时打开多少个spill文件由该参数决定。打开的文件越多，不一定merge sort就越快，所以要根据数据情况适当的调整。 &lt;br /&gt;
默认值：10 &lt;br /&gt;
推荐值： &lt;br /&gt;
单位：无 &lt;br /&gt;
6) mapred.child.java.opts &lt;br /&gt;
参数说明：JVM堆的最大可用内存 &lt;br /&gt;
默认值：-Xmx200m &lt;br /&gt;
推荐值：-Xmx1G | -Xmx4G | -Xmx8G &lt;br /&gt;
单位：-Xmx8589934592也行，单位不固定 &lt;br /&gt;
7) io.sort.mb &lt;br /&gt;
参数说明：Map Task的输出结果和元数据在内存中占的buffer总大小，当buffer达到一定阀值时，会启动一个后台进程来对buffer里的内容进行排序，然后写入本地磁盘，形成一个split小文件 &lt;br /&gt;
默认值：100 &lt;br /&gt;
推荐值：200 | 800 &lt;br /&gt;
单位：兆 &lt;br /&gt;
8) io.sort.spill.percent &lt;br /&gt;
参数说明：即io.sort.mb中所说的阀值 &lt;br /&gt;
默认值：0.8 &lt;br /&gt;
推荐值：0.8 &lt;br /&gt;
单位：无 &lt;br /&gt;
9) io.sort.record &lt;br /&gt;
参数说明：io.sort.mb中分类给元数据的空间占比 &lt;br /&gt;
默认值：0.05 &lt;br /&gt;
推荐值：0.05 &lt;br /&gt;
单位：无 &lt;br /&gt;
10) Mapred.reduce.parallel &lt;br /&gt;
参数说明：Reduce shuffle阶段copier线程数。默认是5，对于较大集群，可调整为16~25 &lt;br /&gt;
默认值：5 &lt;br /&gt;
推荐值：16~25 &lt;br /&gt;
单位：无&lt;/p&gt;

&lt;h2 id=&quot;section-9&quot;&gt;4.系统实现角度调优&lt;/h2&gt;
&lt;p&gt;https://www.xiaohui.org/archives/944.html&lt;/p&gt;

&lt;p&gt;主要针对HDFS进行优化，HDFS性能低下的两个原因：调度延迟和可移植性&lt;/p&gt;

&lt;h3 id=&quot;section-10&quot;&gt;4.1. 调度延迟&lt;/h3&gt;
&lt;p&gt;关于调度延迟主要是发生在两个阶段： &lt;br /&gt;
a) tasktracker上出现空余的slot到该tasktracker接收到新的task； &lt;br /&gt;
b) tasktracker获取到了新的Task后，到连接上了datanode，并且可以读写数据。 &lt;br /&gt;
之所以说这两个阶段不够高效，因为一个分布式计算系统需要解决的是计算问题，如果把过多的时间花费在其它上，就显得很不合适，例如线程等待、高负荷的数据传输。 &lt;br /&gt;
下面解释下会经历上边两个阶段发生的过程： &lt;br /&gt;
a) 当tasktracker上出现slot时，他会调用heartbeat方法向jobtracker发送心跳包（默认时间间隔是3秒，集群很大时可适量调整）来告知它，假设此时有准备需要执行的task，那么jobtracker会采用某种调度机制（调度机制很重要，是一个可以深度研究的东东）选择一个Task，然后通过调用heartbeat方法发送心跳包告知tasktracker。在该过程中，HDFS一直处于等待状态，这就使得资源利用率不高。 &lt;br /&gt;
b) 这个过程中所发生的操作都是串行化的 &lt;br /&gt;
tasktracker会连接到namenode上获取到自己需要的数据在datanode上的存储情况，然后再从datanode上读数据，在该过程中，HDFS一直处于等待状态，这就使得资源利用率不高。 &lt;br /&gt;
若能减短hdfs的等待时间;在执行task之前就开始把数据读到将要执行该task的tasktracker上，减少数据传输时间，那么将会显得高效很多。未解决此类问题，有这样几种解决方案：重叠I/O和CPU阶段（pipelining），task预取（task prefetching），数据预取（data prefetching）等。&lt;/p&gt;

&lt;h3 id=&quot;section-11&quot;&gt;4.2. 可移植性&lt;/h3&gt;
&lt;p&gt;Hadoop是Java写的，所以可移植性相对较高。由于它屏蔽了底层文件系统，所以无法使用底层api来优化数据的读写。在活跃度较高的集群里（例如共享集群），大量并发读写会增加磁盘的随机寻道时间，这会降低读写效率;在大并发写的场景下，还会增加大量的磁盘碎片，这样将会大大的增加了读数据的成本，hdfs更适合文件顺序读取。
对于上述问题，可以尝试使用下面的解决方案：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;tasktracker现在的线程模型是：one thread per client，即每个client连接都是由一个线程处理的（包括接受请求、处理请求，返回结果）。那么这一块一个拆分成两个部分来做，一组线程来处理和client的通信（Client Threads），一组用于数据的读写（Disk Threads）。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;想要解决上述两个问题，暂时没有十全十美的办法，只能尽可能的权衡保证调度延迟相对较低+可移植性相对较高。&lt;/p&gt;

&lt;h3 id=&quot;prefetchingpreshuffling&quot;&gt;4.3. 优化策略：Prefetching与preshuffling&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;a) Prefetching包括Block-intra prefetching和Block-inter prefetching &lt;br /&gt;
&lt;strong&gt;Block-intra prefetching：&lt;/strong&gt;对block内部数据处理方式进行了优化，即一边进行计算，一边预读将要用到的数据。这种方式需要解决两个难题：一个是计算和预取同步，另一个是确定合适的预取率。前者可以使用进度条（processing bar）的概念，进度条主要是记录计算数据和预读数据的进度，当同步被打破时发出同步失效的通知。后者是要根据实际情况来设定，可采用重复试验的方法来确定。 &lt;br /&gt;
&lt;strong&gt;Block-inter prefetching：&lt;/strong&gt;在block层面上预读数据，在某个Task正在处理数据块A1的时候，预测器能预测接下来将要读取的数据块A2、A3、A4，然后把数据块A2、A3、A4预读到Task所在的rack上。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;b) preshuffling&lt;br /&gt;
数据被map task处理之前，由预测器判断每条记录将要被哪个reduce task处理，将这些数据交给靠近reduce task的map task来处理。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;参考资料：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;cloudera官方文档 &lt;br /&gt;
http://blog.cloudera.com/blog/2009/12/7-tips-for-improving-mapreduce-performance/&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;AMD白皮书(较为实用) &lt;br /&gt;
http://www.admin-magazine.com/HPC/content/download/9408/73372/file/Hadoop_Tuning_Guide-Version5.pdf&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;国内博客（大部分内容都是AMD白皮书上的翻译）： &lt;br /&gt;
http://dongxicheng.org/mapreduce/hadoop-optimization-0/
http://dongxicheng.org/mapreduce/hadoop-optimization-1/&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 23 Feb 2015 00:00:00 +0800</pubDate>
        <link>http://leocook.github.io/2015/02/23/hadoop%E4%BC%98%E5%8C%96-%E6%A6%82%E8%BF%B0/</link>
        <guid isPermaLink="true">http://leocook.github.io/2015/02/23/hadoop%E4%BC%98%E5%8C%96-%E6%A6%82%E8%BF%B0/</guid>
        
        <category>集群优化</category>
        
        
        <category>hadoop</category>
        
      </item>
    
  </channel>
</rss>
