{"meta":{"title":"leocook","subtitle":"Big-Data Dev Engineer","description":"spark hadoop bigdata java","author":"leocook","url":"http://www.leocook.org","root":"/"},"pages":[{"title":"404 Not Found","date":"2019-06-09T15:23:42.703Z","updated":"2019-06-08T12:13:46.429Z","comments":true,"path":"404.html","permalink":"http://www.leocook.org/404.html","excerpt":"","text":"**404 Not Found** **很抱歉，您访问的页面不存在** 可能是输入地址有误或该地址已被删除"},{"title":"About","date":"2019-06-09T15:23:41.967Z","updated":"2019-06-09T07:57:17.631Z","comments":true,"path":"about/index.html","permalink":"http://www.leocook.org/about/index.html","excerpt":"","text":"关于本站本站的构建框架是hexo，本着不抄袭、不转载的原则去写一些自己的原创，如有表达不适的提法还望广大网友能提出宝贵的建议！ 关于我一名热爱生活的互联网从业者，热爱健身、阅读以及摄影，偶尔也玩一玩lol。从业以来主要做大数据方向的相关工作，个人对数据、算法一直都有着浓厚的兴趣。学习与进步是一件美好的事情,希望能多认识一些志同道合的朋友们，一起享受成长的快乐！ 联系我 个人微信：leocook，请备注：博客 GitHub：leocook 微信公众号: 收藏的blog过往记忆 | 粉丝日记 | UML.ORG | 大圆那些事 | whoami | 镜缘浮影"},{"title":"我的朋友们","date":"2019-06-09T15:53:28.053Z","updated":"2019-06-09T15:53:28.037Z","comments":true,"path":"friends/index.html","permalink":"http://www.leocook.org/friends/index.html","excerpt":"","text":"这里可以写友链页面下方的文字备注，例如自己的友链规范、示例等。"},{"title":"开源项目","date":"2019-06-09T15:24:02.547Z","updated":"2019-06-09T08:02:41.086Z","comments":true,"path":"projects/index.html","permalink":"http://www.leocook.org/projects/index.html","excerpt":"","text":"晚点加上"},{"title":"","date":"2019-06-09T15:23:44.615Z","updated":"2019-06-08T12:12:38.284Z","comments":true,"path":"mylist/index.html","permalink":"http://www.leocook.org/mylist/index.html","excerpt":"","text":""},{"title":"所有分类","date":"2019-06-09T07:54:24.486Z","updated":"2019-06-08T12:09:51.882Z","comments":true,"path":"blog/categories/index.html","permalink":"http://www.leocook.org/blog/categories/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2019-06-09T07:54:32.314Z","updated":"2019-06-08T12:12:02.676Z","comments":true,"path":"blog/tags/index.html","permalink":"http://www.leocook.org/blog/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"均值，中位数，众数，方差，标准差，与常见的统计图表","slug":"math-2019-05-23-均值，中位数，众数，方差，标准差，与常见的统计图表","date":"2019-05-22T16:00:00.000Z","updated":"2019-06-12T13:54:14.171Z","comments":true,"path":"2019/05/23/math-2019-05-23-均值，中位数，众数，方差，标准差，与常见的统计图表/","link":"","permalink":"http://www.leocook.org/2019/05/23/math-2019-05-23-均值，中位数，众数，方差，标准差，与常见的统计图表/","excerpt":"1.概述统计学是应用数学的一个分支，主要通过利用概率论建立数学模型，收集所观察系统的数据，进行量化的分析、总结，并进而进行推断和预测，为相关决策提供依据和参考。它被广泛的应用在各门学科之上，从物理和社会科学到人文科学，甚至被用来工商业及政府的情报决策之上。当前应用比较热门的领域有：经济学、医学，以及心理学等等。","text":"1.概述统计学是应用数学的一个分支，主要通过利用概率论建立数学模型，收集所观察系统的数据，进行量化的分析、总结，并进而进行推断和预测，为相关决策提供依据和参考。它被广泛的应用在各门学科之上，从物理和社会科学到人文科学，甚至被用来工商业及政府的情报决策之上。当前应用比较热门的领域有：经济学、医学，以及心理学等等。 概率论是统计学的基础，统计学冲锋在应用第一线，概率论提供武器。说到概率论就不得不提柯尔莫戈洛夫，他是20世纪全球最具有影响力的数学家之一，同时也是他创建了现代概率论。 统计学主要分为描述统计学和推断统计学： 描述统计学使用特定的数字或图表来体现数据的集中程度和离散程度。例如：一次考试的平均分，最高分，各个分段的人数分布等。 推断统计学根据样本数据推断总体数据特征。例如：产品质量检验，一般是抽检方式，使用抽样样本的质量合格率作为总体的质量合格率的一个估计。 2.集中趋势描述样本集中趋势的统计量一般有均值、中位数、众数来描述： 均值：算数平均数，描述平均水平使用； 中位数：将数据按大小排序后，位于正中间的数，描述中等水平； 众数：数据中出现最多的数，描述一般水平。 2.1.均值算术平均数，没啥好解释的。 2.2.中位数一批数据排序后中间位置的数。若这批数据的个数为奇数，中位数就是最中间的数；若这批数据个数为偶数，最中间的数据有2个，那么中位数也就是这2个数据的算术平均数。 2.3.众数数据中出现次数最多，且出现次数大于或等于2的数。一组数据中可能存在多个众数，也可能不存在众数。例如：{1 2 2 3 3}中的众数是2和3；{1 2 3 4 5}中没有众数。 众数不仅适用于数值型数据，对非数值型数据也同样使用。例如：{苹果，苹果，香蕉，橙，橙，橙，桃}这一组数据，不存在均值和中位数，但是存在着众数”橙”。 2.4.均值、中位数、众数对比 优点 缺点 均值 重复利用所有数据，适用性强 容易受到极端值影响 中位数 不受极端值影响 缺乏敏感性 众数 当数据具有明显的集中势时，代表性好；不受极端值影响 缺乏唯一性：可能有一个，可能有两个，可能一个都没有 3.离散程度描述例如有两组数据，A : {1,2,5,8,9}, B : {3,4,5,6,7}。两组数据的均值都是5，但是可以看出B组的数据更加集中。除了描述集中趋势的统计量，也需要描述数据离散程度的统计量，一般有极差、方差、标准差。 3.1.极差$极差 = 最大值-最小值$。例如上面描述的集合A和B，A的极差：$9-1=8$，B的极差：$7-3=4$。A和B集合的均值相等，但是A的极差要大于B的极差，所以A比B要更分散。 同样的，只用极差来衡量数据的离散程度也存在不足，例如集合C:{1 2 5 8 9}，和集合D:{1 4 5 6 9}。均值和极差都相等，但明显的C集合要比D集合更离散。 3.2.方差统计学上，一般比较常用方差来描述数据的离散程度。方差的现实意义是：数据离中心越远越离散。使用$X_i$表示集合中的第$i$个数值，$u$表示数据集的均值，则方差$\\sigma^2$的表达式如下： $\\sigma^2 = \\frac{1}{N} \\sum^{N}_{i}{(X_i - u)^2}$ 对于上述的集合A : {1,2,5,8,9}, B : {3,4,5,6,7}，他们的方差分别是： $\\sigma^2_A = \\frac{1}{5}[(1-5)^2+(2-5)^2+(5-5)^2+(8-5)^2+(9-5)^2]=10$ $\\sigma^2_B = \\frac{1}{5}[(3-5)^2+(4-5)^2+(5-5)^2+(6-5)^2+(9-5)^2]=2$ 对于上述的集合C:{1 2 5 8 9}，和集合D:{1 4 5 6 9}，他们的方差分别是： $\\sigma^2_C = 10$ $\\sigma^2_D = \\frac{1}{5}[(1-5)^2+(4-5)^2+(5-5)^2+(6-5)^2+(9-5)^2]=6.8$ 我们对方差进行推导转换得到：$\\sigma^2 = \\frac{1}{N}\\sum^{N}_{i=1}{(X_i - u)^2}$$ = \\frac{1}{N} [(X_1 - u)^2 + (X_2 - u)^2 + … + (X_N - u)^2]$$ = \\frac{1}{N} [(X_1^2 - 2X_1u + u^2) + (X_2^2 - 2X_2u + u^2) + … + (X_N^2 - 2X_Nu + u^2)]$$ = \\frac{1}{N} [X_1^2+X_2^2+…+X_N^2 - 2u(X_1+X_2+…+X_N)+Nu^2]$$ = \\frac{1}{N} (X_1^2+X_2^2+…+X_N^2) - 2u·\\frac{1}{N}(X_1+X_2+…+X_N)+u^2$$ = \\frac{1}{N}\\sum^{N}_{i=1}X_i^2 - 2u^2 + u^2$$ = \\frac{1}{N}\\sum^{N}_{i=1}X_i^2-u^2 $ 这么来看，方差的计算就变得简单很多了。 3.3.标准差标准差：$\\sigma = \\sqrt[2]{\\sigma^2}$ 和方差一样，标准差的值越大，表示数据越分散。 4.常见的数学图形4.1.直方图（Histogram）直方图又称为质量分布图，主要查看样本分段分布使用。 如果要画出直方图，需要下面这几个步骤： a. 找出最大值不最小值，确定数据的范围 例如有下面这一组成绩数据: {53 53 61 61 63 65 67 67 69 69 69 70 70 71 74 75 75 76 77 78 79 80 81 81 81 8182 84 85 86 87 87 87 88 89 90 91 91 94 95}，排序后很容易得到最大值是95，最小值是53。 b. 画出频数直方表 整理数据，将数据分为几组，按照50~60、 60~70、 70~80、80~90、 90~100这几个分段来划分（一般都分为5~10组），如下： ![频数分布表](http://leocook-blog.test.upcdn.net/频数分布表.png \"频数分布表\") 可以看到在80~90这个分段的人数最多。 - c. 画出频数直方图 根据频数分布表，可以画出频数直方图。频数作为纵坐标，成绩作为横坐标。通过 直方图可以对成绩的分布有了一个直观的印象。 ![频数直方图](http://leocook-blog.test.upcdn.net/频数直方图.png \"频数直方图\") d. 频率直方图 除了频数直方图，还有另一种直方图——频率直方图。不频数直方图相比，频率直方图的纵坐标有所改变，使用了频率/组距。 频率=频数/总数；组距就是分组的极差，这里组距是10（可以是100-90=10，也可以使90-80=10等）， ![频率直方图](http://leocook-blog.test.upcdn.net/频率直方图.png \"频率直方图\") 4.2.箱线图箱线图又称为箱形图、盒须图或盒式图，用于显示一组数据分散情况。 箱线图也可以大致地看出数据的分布。下图是40个成绩所画出的箱线图，可以看出数据分布稍微地偏重于高分段： ![箱线图](http://leocook-blog.test.upcdn.net/箱线图1.png \"箱线图\") 4.2.1.箱线图一些概念 下四分位数 Q1，将所有数据按照从小到大的顺序排序排在第25%位置的数字 上四分位数 Q3，将所有数据按照从小到大的顺序排序排在第75%位置的数字 四分位距 IQR，等于Q3-Q1，衡量数据离散程度的一个统计量 异常点 小于Q1－1.5IQR或大于Q3+1.5IQR的值 上边缘 除异常点以外的数据中的最大值 下边缘 除异常点以外的数据中的最小值 下面有一组数据： 53 53 61 61 63 65 67 67 69 69 69 70 70 71 74 75 75 76 77 78 79 80 81 81 81 8182 84 85 86 87 87 87 88 89 90 91 91 94 95 对于上述数据，Q1=69; Q3=86.5; IQR=86.5-69=17.5; 因为Q1－1.5IQR=42.75，Q3+1.5IQR=112.75，所以没有异常点。上边缘就是95，下边缘就是52。 4.2.1.箱线图的画法例如有一组数据 8 2 3 7 4 9 6 9 4 3 排序：2 3 3 4 4 6 7 8 9 9 找出中位数：（4+6）/2=5 分别找出前半部分不后半部分的中位数——下四分位数不上四分位数：3不8 判断异常点：3-1.5*(8-3)=-4.5；8+1.5*(8-3)=15.5；没有异常点 找出最大值不最小值：2不9 在3到8之间间画一个箱子，分别用箭头指向2,9 ![箱线图](http://leocook-blog.test.upcdn.net/箱线图2.png \"箱线图\") 4.3.茎叶图茎叶图又称为枝叶图，和直方图比较相似，用于查看样本分段分布。茎叶图的优点是没有原始数据信息的损失，但缺点也很显然，样本个位往前相差不能太大。 茎叶图可以在保留全部数据信息的情冴下，直观地显示出数据的分布情况4.2.1中40个成绩的茎叶图，左边是茎，右边是叶。 ![茎叶图](http://leocook-blog.test.upcdn.net/茎叶图1.png \"茎叶图\") 若将茎叶图旋转90度，则可以得到一个类似于直方图的图。跟直方图一样，也可以直观地知道数据的分布情冴。 ![茎叶图](http://leocook-blog.test.upcdn.net/茎叶图2.png \"茎叶图\") 茎叶图的画法： 假设有下面这一组数据 53 53 59 61 61 63 65 67 67 69 69 69 70 70 71 74 75 75 76 77 78 79 80 81 81 8181 82 84 85 86 87 87 87 88 89 90 91 91 94 95 将数据分为茎和叶两部分，这里的茎是指十位上的数字，叶是指个位上的数字 将茎部分（十位）从小到大，从上到下写出来 相对于各自的茎，将同一茎（十位）的叶子（个位）从小到大，从左往右写出来 ![茎叶图](http://leocook-blog.test.upcdn.net/茎叶图3.png \"茎叶图\") 4.4.线图线图的表现形式比较多，主要用作描述变化趋势。如下图所示，表示广州房价变化趋势： ![茎叶图](http://leocook-blog.test.upcdn.net/线图.png \"茎叶图\") 4.5.柱形图（bar chart）柱形图，又称长条图、柱状图、条状图、棒形图。通常用来比较两个或以上的度量，只有一个变量，可以是不同时间不同条件。 和直方图相比，直方图有两个变量，分别是统计样本，和该样本对应的某个属性的度量。 ![柱形图](http://leocook-blog.test.upcdn.net/柱形图.png \"柱形图\") 4.6.饼图饼图主要用作描述样本发布占比情况，见得比较多，没什么好说的，如下图： ![饼图](http://leocook-blog.test.upcdn.net/饼图.png \"饼图\") 5.延伸图表是一种工具，方便我们来管理数据、展示数据，以至于可以更快速的做决策判断。感觉兴趣的可以看看品质管理七大手法。","categories":[{"name":"统计学","slug":"统计学","permalink":"http://www.leocook.org/blog/categories/统计学/"}],"tags":[{"name":"统计学","slug":"统计学","permalink":"http://www.leocook.org/blog/tags/统计学/"}]},{"title":"Servlet的那些二三事儿","slug":"java-2017-08-31-Servlet的那些二三事儿","date":"2017-08-30T16:00:00.000Z","updated":"2019-06-09T15:36:42.987Z","comments":true,"path":"2017/08/31/java-2017-08-31-Servlet的那些二三事儿/","link":"","permalink":"http://www.leocook.org/2017/08/31/java-2017-08-31-Servlet的那些二三事儿/","excerpt":"Servlet（Server Applet），全称Java Servlet。主要是使用Java来开发一些web应用，例如网站、web service等等。下面就从几个方面来总结一下servlet的关键点，当然也会提及到jsp、http等。","text":"Servlet（Server Applet），全称Java Servlet。主要是使用Java来开发一些web应用，例如网站、web service等等。下面就从几个方面来总结一下servlet的关键点，当然也会提及到jsp、http等。 servlet的生命周期 初始阶段Web容器加载servlet，调用init()方法。 运行阶段当请求到达时，运行其service()方法。service()识别请求类型，并调用相对应的doGet、doPost方法。 销毁阶段服务结束，web容器会调用servlet的distroy()方法销毁servlet。 如何去实现一个Servlet先了解一下Servlet API的两个包：javax.servlet.*和javax.servlet.http.*。javax.servlet.*包下放的主要是一些接口，javax.servlet.http.*包下方的主要是上述接口在http协议中的实现。目前网络数据交互式传输中，http协议占据这主导的地位，说不准哪天出现一个新的协议，那时servlet可能还会对新协议做支持。实现一个Servlet目前有两种方式，一种是直接实现最底层的Servlet接口；第二种则是实现继承HttpServlet类。 javax.servlet.Servlet接口有下面这几个方法 init在servlet初始化时，会调用它。 service在servlet处理请求的时候，会调用它。 destroy在服务结束时，销毁servlet的时候，会调用它。 getServletConfig 123456789101112&lt;servlet&gt; &lt;servlet-name&gt;ServletConfigTest&lt;/servlet-name&gt; &lt;servlet-class&gt;com.vae.servlet.ServletConfigTest&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;name1&lt;/param-name&gt; &lt;param-value&gt;value1&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;encode&lt;/param-name&gt; &lt;param-value&gt;utf-8&lt;/param-value&gt; &lt;/init-param&gt;&lt;/servlet&gt; 获取servlet的配置信息,init-param标签下的内容。 getServletInfo获取servlet信息， javax.servlet.http.HttpServlet类 doGet处理get请求时，会执行该方法。 doPost处理post请求时，会执行该方法。 init在servlet初始化时，会调用它。相当于Servlet接口中的init方法。 destroy在servlet初始化时，会调用它。相当于Servlet接口中的init方法。 JSP和Servlet的关系这里说明一下，JSP在被编译后生成的就是Servlet，所以访问jsp，其实访问的也就是其对于的Servlet。 四种会话作用域由于在web开发中，数据传输的范围是不同的，所以产生了四种会话作用域，其实就是提供了4种生命周期的对象，用于存储数据使用。 page单个页面级别 request单个请求级别 session单个session级别 application服务器从启动到停止 Jsp的九个内置对象 page指JSP也变本身，代表的是java.lang.Object类的对象。 request包含了客户端请求的信息，代表的是javax.servlet.HttpServletRequest类的对象 session包含了当前会话的信息，代表的是javax.servlet.http.HttpSession类的对象。 application包含了当前服务器全局的信息，重启后失效。代表的是javax.servlet.ServletContext类的对象。 response包含了服务端返回客户端的响应信息，代表的是javax.servlet.HttpServletResponse类的对象。 out向客户端输出数据。 configServletConfig类的一个实例， pageContext包含了当前页面的信息 exception处理异常使用 内置对象和作用域的对照 内置对象名 对应的Java类 作用域 page java.lang.Object Page request javax.servlet.http.HttpServletRequest Request session javax.servlet.http.HttpSession Session application javax.servlet.ServletContext Application response javax.servlet.http.HttpServletResponse Page out javax.servlet.jsp.JspWriter Page config javax.servlet.ServletConfig Page pageContext javax.servlet.jsp.PageContext Page exception java.lang.Throwable Page 转发和重定向什么是转发和重定向从体验上来说，转发后浏览器的URL不会变，而重定向时URL会变。从执行的原理上来说，转发是服务端的行为，期间request域共享；重定向是客户端的行为，客户端发现返回的http状态码是302时，将会向新的URL发送 Servlet中如何实现转发和重定向 转发request.getRequestDispatcher(“目标文件”).forward(request,response) 重定向response.sendRedirect(“目标文件”) JSP中如何实现转发和重定向 转发 &lt;jsp:forward page=”目标文件” /&gt; 重定向&lt;%response.sendRedirect(“目标文件”);//重定向到new.jsp%&gt; 如何选择转发和重定向 何时选择转发 当需要隐藏地址的时候，选用转发； 当需要共享request域数据时，选用转发。 何时选择重定向当不需要隐藏地址的时候，且不需要共享request域数据时，选用重定向。重定向只是单纯的重新访问一个新的地址。 Servlet中的过滤器为什么需要过滤器在web开发中，我们可能需要对所有的请求都做一层过滤，例如权限验证这些操作等等。此时我们就需要一种技术，所有或者一类请求在处理前都会经过这个技术来进行一次预处理。 什么是过滤器过滤器就是在请求到达Servlet之前会经过的一层，在过滤器这一层我们可以检查权限、可以对所有或者部分请求做统一的处理。 如何使用过滤器过滤器类一般都会实现javax.servlet.Filter这个接口。然后在web.xml中进行配置，下面是一个例子： 123456789101112&lt;filter&gt; &lt;filter-name&gt;filter01&lt;/filter-name&gt; &lt;!-- filter的名字，自定义 --&gt; &lt;filter-class&gt;filter.class&lt;/filter-class&gt; &lt;!-- filter类，可以是自己实现的，也可以使用默认的 --&gt; &lt;init-param&gt; &lt;param-name&gt;key&lt;/param-name&gt;&lt;!-- 初始化参数，可选配置 --&gt; &lt;param-value&gt;value&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;!-- 配置filter生效的url，当访问这些url时，都会执行该filter --&gt; &lt;filter-name&gt;filter01&lt;/filter-name&gt; &lt;!-- filter的名字，自定义 --&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;!-- 这里可以精确匹配，也可以模糊匹配，例如匹配所有的url就是“/*”，匹配所有的jsp，就是“*.jsp”.注意，路径和扩展名匹配不可以同时使用 --&gt;&lt;/filter-mapping&gt; 过滤器链Servlet中的监听器Servlet的监听器是实现了javax.servlet.ServletContextListener接口的服务端程序，它随web的启动而启动，在启动期间可以监听web应用内部发生的一系列事件。可以对不同域的不同事件设置监听，例如域内部属性的变化，request、session和application的创建和销毁等等。 官方提供的几个监听器Servlet监听器对特定的事件进行监听，当产生这些事件的时候，会执行监听器的代码。可以对应用的加载、卸载，对session的初始化、销毁，对session中值变化等事件进行监听。官方提供了一些对ServletContext、session、request的监听。 Session级别的监听HttpSessionListener全局监听session的创建和销毁，在XML中配置监听，全局的。 sessionCreated有新的session创建时，会出发执行这个方法 sessionDestroyed执行session.invalidate()方法时，或者session超时（默认30分钟）会执行该方法。 HttpSessionBindingListener实例化后设置为session的属性，绑定到单个session。只能监听单个session，一个session对应一个HttpSessionBindingListener对象的实例。 valueBound当对象被绑定到session上时，例如：application.setAttribute(“onlineUserList”, onlineUserList); valueUnbound执行方法session.invalidate()；或者session超时；或者修改了session域的属性，例如：session.setAttribute(“onlineUserListener”, “其他对象”);或者删除了某一个属性，例如：session.removeAttribute(“onlineUserListener”)。都会触发valueUnbound方法的执行。 HttpSessionAttributeListener监听session的属性变化 attributeAdded当有新的属性添加时，将会将会触发 attributeRemoved当有属性移除时，将会触发 attributeReplaced当有属性被覆盖时，将会触发 HttpSessionActivationListener和HttpSessionBindingListener一样，需要绑定到session上，是对单个session的监听。有的时候需要对session做持久化，即将session持久化写到磁盘上，当服务器挂了重启后，还能恢复session中的信息。 sessionWillPassivate当执行这个方法的时候，可以把session保存到硬盘上 sessionDidActivate当执行这个方法的时候，就是把session从硬盘读取出来，并使用 ServletContext级别的监听ServletContextAttributeListenerServletContext上存在属性变化时，将会出发此类监听。 attributeAddedServletContext上添加属性时触发。 attributeRemovedServletContext上删除属性时触发。 attributeReplacedServletContext上添加属性值被修改时触发。 ServletContextListener监听ServletContext的创建和销毁 contextInitialized初始化ServletContext对象 contextDestroyed销毁ServletContext对象 request级别的监听ServletRequestAttributeListener监听request属性变化事件 attributeAdded当request域有新的属性添加时，触发事件 attributeRemoved当request域有属性移除时，触发事件 attributeReplaced当request域有属性被覆盖时，触发事件 ServletRequestListener监听request生命周期的变化 requestDestroyed当request销毁时，触发该事件 requestInitialized当request初始化时，触发该事件 servlet的单例与多线程问题servlet是单例，多线程的。servlet类在web容器中只会被创建一个实例。但是多线程环境下，多个线程可以同时调用同一个Servlet的service方法。所以在Servlet对象中使用共享变量的时候要小心，很容易出现线程安全问题。 servlet的单线程模式默认Jsp的Servlet是单例、多线程的。但也可以在Jsp页面里声明 &lt;%@page isThreadSafe=”false”%&gt; ，表示Jsp的Servlet类将以Singleton模式来运行。即：Jsp对应的Servlet实例将不会同时被多个线程调用，一个请求创建一个Servlet实例。 Jsp的Servlet里一般不会有很复杂的业务逻辑，更多的是页面展示，以及一些简单的数据处理，所以一般情况下不用配置isThreadSafe=false的，推荐配置为true.","categories":[{"name":"java","slug":"java","permalink":"http://www.leocook.org/blog/categories/java/"}],"tags":[{"name":"Servlet","slug":"Servlet","permalink":"http://www.leocook.org/blog/tags/Servlet/"}]},{"title":"Java并发(七)：JCF线程安全问题","slug":"java-2017-07-22-Java并发-七-：JCF线程安全问题","date":"2017-07-21T16:00:00.000Z","updated":"2019-06-09T15:36:25.264Z","comments":true,"path":"2017/07/22/java-2017-07-22-Java并发-七-：JCF线程安全问题/","link":"","permalink":"http://www.leocook.org/2017/07/22/java-2017-07-22-Java并发-七-：JCF线程安全问题/","excerpt":"Jdk本身给我实现了很多数据结构，例如List、Set、Queue以及Map等。但是这些类都不是线程安全的，在Jdk1.0的时候提供了Hashtable、Vector以及Stack这些线程安全集合，但由于它们只是在原来集合的基础上通过关键字synchronized来保证功能的原子性和可见性，这样做的缺点就是效率地下，当有一个线程执行get的时候，其它所有线程都被阻塞。","text":"Jdk本身给我实现了很多数据结构，例如List、Set、Queue以及Map等。但是这些类都不是线程安全的，在Jdk1.0的时候提供了Hashtable、Vector以及Stack这些线程安全集合，但由于它们只是在原来集合的基础上通过关键字synchronized来保证功能的原子性和可见性，这样做的缺点就是效率地下，当有一个线程执行get的时候，其它所有线程都被阻塞。 什么是线程安全线程安全是一个很难一言表达清楚的，通俗的说就是：一个程序在单线程的环境下可以正常运行，在多线程环境下也能正确的运行，并且可以返回确定的结果，那么这个程序就是线程安全的。一个线程安全的类，在多线程环境下不会因为执行的顺序导致对象成为无效状态，同样也不会违反任何先验条件和后验条件。 关于无效状态、先验条件、后验条件，下面是一些说明： 无效状态 在做选择排序的过程中，前边已经排序好的那段数据顺序被其它线程给打乱了，这时候就可以说这个被排序的类处于无效状态。 先验条件 在做二分查找的时候，被查找的集合必须是已经排好序的，这就是执行二份查找的条件，也就是先验条件。 后验条件 指的代码的业务目标是否正确。 fail-fast机制当迭代器实现了fail-fast机制，创建了该集合的迭代器后，使用该集合迭代器遍历或者修改集合元素的之前，若该集合被直接调用方法并产生了修改（size的变化），那么将会抛出java.util.ConcurrentModificationException异常。 例如ArrayList对象，线程1在用迭代器遍历里边元素的同时，线程2put了一个新的元素，那么就会抛出异常java.util.ConcurrentModificationException。 fail-fast机制是使用了modCount变量（集合类持有的字段）和expectedModCount（迭代器持有的字段）来实现。在迭代器创建的时候将exceptedModCount被初始化为modCount的值，任何对集合的修改都会改变modCount的值.每当调用next等方法的时候，会判断expectedModCount和当前的modCount是否相等，不相等的话则抛出CME异常. modCountmodCount变量是集合类中的变量，在集合被修改时，modCount变量将会加1；当调用迭代器的方法改变了集合size的方法时，modCount变量也会加1。 expectedModCountexpectedModCount变量是迭代器类中的变量，在创建的迭代器对象时，会使用modCount的值来对其进行初始化。只有在调用迭代器改变了集合size的时候，才会加1。 ArrayList、LikedList它们都是AbstractList的子类，它们的modCount都是从AbstractList类中继承来的，当执行会使集合size发生变化的方法时，例如：add、remove等，modCount会加1。执行set、get操作不会使modCount发生变化，因为他们没有使集合的size发生变化。当调用迭代器的next、add、remove、previous、set等访问集合元素的方法时，都会校验exceptedModCount和modCount是否相等，如果不相等，则抛出CME异常。 HashMap、LinkedHashMap当调用Map的方法或者Map的迭代器使得Map集合被修改（值被修改，或者顺序发生变化）时，modCount加1。在调用Map的迭代器访问、修改Map集合元素的时候，都会判断expectedModCount和当前的modCount是否相等，不相等的话则抛出CME异常. 很显然，只使用modCount和expectedModCount并不能保证集合的线程安全。 modCount是集合类的成员变量，expectedModCount是迭代器的成员变量 在使用迭代器遍历集合元素时，会判断modCount和expectedModCount是否相等来校验集合是否被修改过，很显然这个办法是存在bug的。 对于Map结构，数据只要变化modCount就会加1；对于List结构，当集合size变化时，modCount才会加1。 这些规则表述的可能不是很准确，具体可以查看相关源码。Set结构是什么样的呢？留给读者自己查看源码吧！ Collections.synchronizedXXX发展到Jdk1.2时，Jdk提供了Collections.synchronizedXXX系列的方法： Collections.synchronizedCollection包装Collection接口的实现类，使其具备线程安全 Collections.synchronizedList包装List接口的实现类，使其具备线程安全 Collections.synchronizedMap包装Map接口的实现类，使其具备线程安全 Collections.synchronizedSet包装Set接口的实现类，使其具备线程安全 这里我们查看java.util.Collections.SynchronizedCollection类的源码，可以看出，这也是在原来集合的基础上包装了一层synchronized关键字，只是提供了并发集合使用的便利性，并没有从本质上提高并发集合的效率！ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566static class SynchronizedCollection&lt;E&gt; implements Collection&lt;E&gt;, Serializable &#123; private static final long serialVersionUID = 3053995032091335093L; final Collection&lt;E&gt; c; // Backing Collection final Object mutex; // Object on which to synchronize SynchronizedCollection(Collection&lt;E&gt; c) &#123; if (c==null) throw new NullPointerException(); this.c = c; mutex = this; &#125; SynchronizedCollection(Collection&lt;E&gt; c, Object mutex) &#123; this.c = c; this.mutex = mutex; &#125; public int size() &#123; synchronized (mutex) &#123;return c.size();&#125; &#125; public boolean isEmpty() &#123; synchronized (mutex) &#123;return c.isEmpty();&#125; &#125; public boolean contains(Object o) &#123; synchronized (mutex) &#123;return c.contains(o);&#125; &#125; public Object[] toArray() &#123; synchronized (mutex) &#123;return c.toArray();&#125; &#125; public &lt;T&gt; T[] toArray(T[] a) &#123; synchronized (mutex) &#123;return c.toArray(a);&#125; &#125; public Iterator&lt;E&gt; iterator() &#123; return c.iterator(); // Must be manually synched by user! &#125; public boolean add(E e) &#123; synchronized (mutex) &#123;return c.add(e);&#125; &#125; public boolean remove(Object o) &#123; synchronized (mutex) &#123;return c.remove(o);&#125; &#125; public boolean containsAll(Collection&lt;?&gt; coll) &#123; synchronized (mutex) &#123;return c.containsAll(coll);&#125; &#125; public boolean addAll(Collection&lt;? extends E&gt; coll) &#123; synchronized (mutex) &#123;return c.addAll(coll);&#125; &#125; public boolean removeAll(Collection&lt;?&gt; coll) &#123; synchronized (mutex) &#123;return c.removeAll(coll);&#125; &#125; public boolean retainAll(Collection&lt;?&gt; coll) &#123; synchronized (mutex) &#123;return c.retainAll(coll);&#125; &#125; public void clear() &#123; synchronized (mutex) &#123;c.clear();&#125; &#125; public String toString() &#123; synchronized (mutex) &#123;return c.toString();&#125; &#125; private void writeObject(ObjectOutputStream s) throws IOException &#123; synchronized (mutex) &#123;s.defaultWriteObject();&#125; &#125;&#125; java.util.concurrent概述Jdk1.5开始，Java的并发模型发生了翻天覆地的变化,具体可以查看java.util.concurrent包下的类。 CAS的实现目前大多数CPU硬件级别都只是CAS了，jdk1.5后封装了CAS的实现，并以Unsafe类供jdk中其它类调用。这个设计的初衷是给Jdk的类库使用，都不是给开发人员。 锁和原子类在CAS的基础上，实现了AQS，并实现了ReentrantLock锁，一般很少创建ReentrantLock对象，而是继承ReentrantLock类并在此基础上构建。java.util.concurrent.atomic包下提供了一系列的原子变量，例如在并发环境下计算自增值，就可以使用该包下面的类。 高级的工具类例如下面三个： Executor框架：线程池 TimerTask类：定时器 Semaphore类：资源计数器 CountdownLatch类：倒计时计数器 上面简单的介绍了java.util.concurrent包，那么该包下关于集合线程安全的实现体现在哪些方面呢？ 阻塞的线程安全集合集合被其它线程访问时，例如使用迭代器迭代、get或者put的时候，当其它线程在访问该资源时，会进入阻塞状态。这种集合在并发执行时，可以保证数据的强一致性。 非阻塞的线程安全集合集合被其它线程访问时，例如使用迭代器迭代、get或者put的时候，当其它线程在访问该资源时，不会进入阻塞状态。这种集合在并发执行时，可以保证数据的弱强一致性，更新的数据在经过某一段时间之后才会被访问到。 ConcurrentXXX以Concurrent开头的集合类，都是非阻塞的线程安全类。非阻塞的线程安全集合在使用时只能保证数据的弱一致性，无法保证数据的强一致性。如果必须要保证数据的强一致性，可以使用阻塞的线程安全类。前边提到的使用synchronized来保证线程安全的集合类，就是阻塞的线程安全类。 这里号称是非阻塞的线程安全类，只是理想状态下非阻塞，使用时阻塞的概率较低，效率较高，但还是会可能线程被阻塞的。 ConcurrentHashMap非阻塞线程安全的HashMap ConcurrentLinkedDeque非阻塞线程安全的双端队列 ConcurrentLinkedQueue非阻塞线程安全的队列 ConcurrentSkipListMap非阻塞线程安全的TreeMap（有序） ConcurrentSkipListSet非阻塞线程安全的TreeSet（有序） CopyOnWriteArrayXXX这一类集合在数据发生变化时，会通过内存拷贝来创建一个新的集合。若在迭代器遍历过程中集合的元素发生变化，迭代器会继续遍历“老”的集合，其它操作会访问“新”的集合。这类集合适合“数据量小”、“读多写少”的场景。 CopyOnWriteArrayList线程安全的ArrayList，基于内存拷贝实现的。 CopyOnWriteArraySet线程安全的ArrayList，基于内存拷贝实现的。其实就是对CopyOnWriteArrayList的包装，调用add方法的时候会调用CopyOnWriteArrayList类的addIfAbsent方法，当元素不存在的时候才会被加入到集合中。 BlockingXXX阻塞线程安全类，其中BlockingDeque接口继承了BlockingQueue接口。这一类集合在并发过程中，可以保证数据的强一致性。 ArrayBlockingQueue阻塞的线程安全队列，它是基于数组实现的，创建的时候就需要指定队列的长度。 LinkedBlockingQueue阻塞的线程安全队列，是基于链表实现的。 LinkedBlockingDeque阻塞的线程安全双端队列，是基于链表实现的。 PriorityBlockingQueue阻塞的线程安全优先队列，队列中的元素必须实现Comparable接口，在插入元素时，会调用接口的compareTo方法来判断元素的大小，从而确定元素的位置。 ConcurrentHashMap这里把ConcurrentHashMap单独拿出来说一下，因为ConcurrentXXX系列集合实现的核心思想和它基本上一样。CopyOnWriteArray系列和BlockingXXX系列实现手法比较简单，自己浏览一下源码就可以了。 ConcurrentHashMap的类部结构大概如下图这样： 默认情况下Segment数组的大小是16(DEFAULT_INITIAL_CAPACITY),Segment下的HashEntry数组的默认大小是2(MIN_SEGMENT_TABLE_CAPACITY)。 原理相关 锁的范围ConcurrentHashMap中读数据是没有锁的，写数据时会有锁。在向同一个Segment元素下写数据时，会使用同一把锁。 非阻塞锁这里号称是非阻塞的线程安全类，只是理想状态下非阻塞，使用时阻塞的概率较低，效率较高，但还是会可能线程被阻塞的。 弱一致性原理ConcurrentHashMap的get是没有锁的，但是put是有锁的。可能会出现一种情况是：线程1先执行了put，但是在数据没还没完全写入时，线程2读取了集合里的数据，此时是读取不到线程1刚刚执行写操作写入的数据，但是在很短暂的一个时间段后，线程2就能读到线程1写入的数了。这就是读写分离产生的弱一致性。 内部类 HashEntry该类实现的就是单纯的单向链表结构，和HashMap中的Entry类似。 关键字段 1234567891011/** * hash值 */final int hash;final K key;volatile V value;/** * 下一个节点 */volatile HashEntry&lt;K,V&gt; next; 关键方法 1234567891011121314151617181920/** * 使用UNSAFE来设置next节点 */final void setNext(HashEntry&lt;K,V&gt; n) &#123; UNSAFE.putOrderedObject(this, nextOffset, n);&#125;// Unsafe相关的初始化static final sun.misc.Unsafe UNSAFE;static final long nextOffset;static &#123; try &#123; UNSAFE = sun.misc.Unsafe.getUnsafe(); Class k = HashEntry.class; nextOffset = UNSAFE.objectFieldOffset (k.getDeclaredField(&quot;next&quot;)); &#125; catch (Exception e) &#123; throw new Error(e); &#125;&#125; Segment该类就是ConcurrentHashMap中分桶的实现。 关键字段 123456789101112131415/** * 处理器个数 */static final int MAX_SCAN_RETRIES = Runtime.getRuntime().availableProcessors() &gt; 1 ? 64 : 1; /** * 基于数组实现的Hash表 */transient volatile HashEntry&lt;K,V&gt;[] table;/** * 分桶内的元素个数。 */transient int count; 关键方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138final V put(K key, int hash, V value, boolean onlyIfAbsent) &#123; /** * 1. 写数据前,先拿到当前的节点元素,并执行park */ HashEntry&lt;K,V&gt; node = tryLock() ? null : scanAndLockForPut(key, hash, value); V oldValue; try &#123; //拿到分桶数组 HashEntry&lt;K,V&gt;[] tab = table; //拿到该节点应该在的哪个 int index = (tab.length - 1) &amp; hash; //拿到分桶中HashEntry的起始位置HashEntry链表下 HashEntry&lt;K,V&gt; first = entryAt(tab, index); //往下迭代 for (HashEntry&lt;K,V&gt; e = first;;) &#123; // 当分桶不为空时,持续遍历,找到节点,并修改它的value if (e != null) &#123; K k; if ((k = e.key) == key/*基本类型,或同一对象*/ || (e.hash == hash &amp;&amp; key.equals(k))/*+自定义对象*/) &#123; oldValue = e.value; if (!onlyIfAbsent) &#123; e.value = value; ++modCount; &#125; break; &#125; e = e.next; &#125; //当分桶非空的时候 else &#123; //若node已经获取到了,设置node为first元素 if (node != null) node.setNext(first); else node = new HashEntry&lt;K,V&gt;(hash, key, value, first); int c = count + 1; //集合元素超过阈值后,扩充 if (c &gt; threshold &amp;&amp; tab.length &lt; MAXIMUM_CAPACITY) rehash(node); else //否则直接把节点的值放在分桶的第一个位置 setEntryAt(tab, index, node); ++modCount; count = c; oldValue = null; break; &#125; &#125; &#125; finally &#123; unlock(); &#125; return oldValue;&#125;/** * 获取锁,并返回对应的 HashEntry 节点。 * * 1.若对应的分桶数组不存在,表示该hash值还未出现过,则直接创建出一个node,并在尝试次数达到MAX_SCAN_RETRIES时,阻塞 * 2.若对应的分桶数组存在,则阻塞,接触阻塞后返回节点 * * @return a new node if key not found, else null */private HashEntry&lt;K,V&gt; scanAndLockForPut(K key, int hash, V value) &#123; //拿到分桶数组的首元素 HashEntry&lt;K,V&gt; first = entryForHash(this, hash); HashEntry&lt;K,V&gt; e = first; HashEntry&lt;K,V&gt; node = null; int retries = -1; // negative while locating node //尝试获取锁,若获取不到锁,就一直轮询下去 while (!tryLock()) &#123; HashEntry&lt;K,V&gt; f; // to recheck first below if (retries &lt; 0) &#123; if (e == null) &#123; //首元素不存在,意思是hash还没有碰撞过 if (node == null) // speculatively create node node = new HashEntry&lt;K,V&gt;(hash, key, value, null); retries = 0; &#125; else if (key.equals(e.key)) //查看key和首元素的key是否相等 retries = 0; else //如果key的位置不在首元素出 e = e.next; &#125; else if (++retries &gt; MAX_SCAN_RETRIES) &#123; lock(); //独占 break; &#125; else if ((retries &amp; 1) == 0/*retries为偶数时*/ &amp;&amp; (f = entryForHash(this, hash)) != first) &#123; /** * 如果在次过程中HashEntry数组的first节点发生了变化,则重新遍历 */ e = first = f; // re-traverse if entry changed retries = -1; &#125; &#125; return node;&#125;/** * 获取数据，key不能为null */public V get(Object key) &#123; Segment&lt;K,V&gt; s; // manually integrate access methods to reduce overhead HashEntry&lt;K,V&gt;[] tab; int h = hash(key); //确定数据是在哪个分桶中的 long u = (((h &gt;&gt;&gt; segmentShift) &amp; segmentMask) &lt;&lt; SSHIFT) + SBASE; //确定数据是在哪个分桶中的 if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(segments, u)) != null &amp;&amp; (tab = s.table) != null) &#123; /** * 1.先确定数据是在哪个链表中 * 2.遍历下去,直到读到对应的key的值 */ for (HashEntry&lt;K,V&gt; e = (HashEntry&lt;K,V&gt;) UNSAFE.getObjectVolatile (tab, ((long)(((tab.length - 1) &amp; h)) &lt;&lt; TSHIFT) + TBASE); e != null; e = e.next) &#123; K k; if ((k = e.key) == key || (e.hash == h &amp;&amp; key.equals(k))) return e.value; &#125; &#125; return null;&#125; 总结 老的线程安全集合是基于synchronized关键字实现的，效率低下，但是具备强一致性的特性； 基于AQS实现的新版线程安全集合效率较高，强一致性和弱一致性的集合都有部分的实现； 个人觉得ConcurrentHashMap这个类源码研究透了，整个Java集合并发这部分知识就很好理解了； 原理级别的东西看下源码就知道了，不用每一步都刻意去记忆，理解并记住其中的设计思想就可以了。","categories":[{"name":"java","slug":"java","permalink":"http://www.leocook.org/blog/categories/java/"}],"tags":[{"name":"ConcurrentHashMap","slug":"ConcurrentHashMap","permalink":"http://www.leocook.org/blog/tags/ConcurrentHashMap/"}]},{"title":"Java并发(六)：ReentrantLock、synchronized","slug":"java-2017-07-16-Java并发-六-：ReentrantLock、synchronized","date":"2017-07-15T16:00:00.000Z","updated":"2019-06-09T15:36:47.657Z","comments":true,"path":"2017/07/16/java-2017-07-16-Java并发-六-：ReentrantLock、synchronized/","link":"","permalink":"http://www.leocook.org/2017/07/16/java-2017-07-16-Java并发-六-：ReentrantLock、synchronized/","excerpt":"ReentrantLock是基于AQS设计的可重入锁，synchronized是基于对象监视器实现的可重入锁。使用了它们后，代码都会具有原子性（atomicity）和 可见性（visibility）。 可重入锁也被称为递归锁，指同一个线程内，外层代码锁未被释放时，内层代码也可以获取到锁，递归就是一种很常见的场景。下面就是可重入锁的一种使用场景： 12345678910111213public class Demo&#123; Lock lock = new Lock(); public void outer()&#123; lock.lock(); inner(); lock.unlock(); &#125; public void inner()&#123; lock.lock(); //do something lock.unlock(); &#125;&#125;","text":"ReentrantLock是基于AQS设计的可重入锁，synchronized是基于对象监视器实现的可重入锁。使用了它们后，代码都会具有原子性（atomicity）和 可见性（visibility）。 可重入锁也被称为递归锁，指同一个线程内，外层代码锁未被释放时，内层代码也可以获取到锁，递归就是一种很常见的场景。下面就是可重入锁的一种使用场景： 12345678910111213public class Demo&#123; Lock lock = new Lock(); public void outer()&#123; lock.lock(); inner(); lock.unlock(); &#125; public void inner()&#123; lock.lock(); //do something lock.unlock(); &#125;&#125; ReentrantLock和synchronized在并发编程中，有着相同的语义，但是它们实现的原理存在着较大的差异，在设计的思想上更是有着很多不同之处。 实现原理上的区别ReentrantLockReentrantLock是基于AQS实现的锁，我们知道AQS是调用了LockSupport类的park和unpark方法来实现阻塞和唤醒的。如果之前看过Java并发(四)：locksupport-LockSupport/)这篇文章的话，应该很好理解：ReentrantLock是通过一个_counter变量来标记阻塞状态的。阅读ReentrantLock源码可以发现，AQS中的state字段在ReentrantLock中也被用作记录锁的重入次数（也就是同一个线程同时获得锁的次数），当state的值为0时，则表示资源没有被其它锁占用。 synchronized在JVM中，对象（this）或者类（SomeClass.class）都会被分配一个监视器(Monitor)。Monitor可以理解为一种同步工具，也可以理解为伴随着对象实例的一种JVM内部对象。synchronized关键字是使用了对象监视器(Monitor)来标识资源是否被锁占用，我们将下边代码反进行反编译： 123456789101112131415/** * 同步代码块 */public void method1()&#123; synchronized (this)&#123; System.out.println(&quot;method1 start&quot;); &#125;&#125;/** * 同步方法 */public synchronized void method2()&#123; System.out.println(&quot;method2 start&quot;);&#125; 反编译后，我们可以看到字节码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public void method1(); flags: ACC_PUBLIC Code: stack=2, locals=3, args_size=1 0: aload_0 1: dup 2: astore_1 3: monitorenter 4: getstatic #2 // Field java/lang/System.out:Ljava/io/PrintStream; 7: ldc #3 // String method1 start 9: invokevirtual #4 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 12: aload_1 13: monitorexit 14: goto 22 17: astore_2 18: aload_1 19: monitorexit 20: aload_2 21: athrow 22: return Exception table: from to target type 4 14 17 any 17 20 17 any LineNumberTable: line 12: 0 line 13: 4 line 14: 12 line 15: 22 LocalVariableTable: Start Length Slot Name Signature 0 23 0 this Lcom/xxx/bi/ThreadDemo; StackMapTable: number_of_entries = 2 frame_type = 255 /* full_frame */ offset_delta = 17 locals = [ class com/xxx/bi/ThreadDemo, class java/lang/Object ] stack = [ class java/lang/Throwable ] frame_type = 250 /* chop */ offset_delta = 4 public synchronized void method2(); flags: ACC_PUBLIC, ACC_SYNCHRONIZED Code: stack=2, locals=1, args_size=1 0: getstatic #2 // Field java/lang/System.out:Ljava/io/PrintStream; 3: ldc #5 // String method2 start 5: invokevirtual #4 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 8: return LineNumberTable: line 19: 0 line 20: 8 LocalVariableTable: Start Length Slot Name Signature 0 9 0 this Lcom/xxx/bi/ThreadDemo; 阅读上面字节码，我们可以从两个方面来看： synchronized同步代码块(method1)我们能看到在进入同步代码块时，会执行monitorenter(占用监视器)，退出代码块时会执行monitorexit(退出监视器)。 关于monitorenter 若monitor的entry count为0，则该线程进入monitor，然后将entry count设置为1，该线程成为了monitor的所有者； 若monitor的entry count不为0，且该进程已经占用了monitor，则线程只是重新进入该代码块，且entry count加1； 若monitor的entry count不为0，且被其它进程已经占用了monitor，则该线程进入阻塞状态，直到monitor的entry count为0时，再重新尝试成为monitor的所有者； 关于monitorexit 指令执行的时候，monitor的entry count减1，当减1后为0的时候，释放monitor不在占有。其它被这个monitor阻塞的线程开始尝试获取该monitor的所有权。 object的wait/notify方法是依赖monitor的，所以只能在同步代码块或者方法中才能调用wait/notify等方法，否则会抛出异常。执行monitorexit必须是某个已经占用了monitor的线程的对象实例。 synchronized同步方法(method2)我们可以看出该方法的access_flags中存在ACC_SYNCHRONIZED标签，该method加上了该标签之后，可以理解进入方法的时候，会做和monitorenter一样的事情，当退出方法的时候将会作出和monitorexit一样的事情。 关于synchronized，有下面几个总结： 1.使用同步代码块后，JVM会用monitorenter和monitorexit指令完成同步。2.使用同步方法后，JVM会使用方法的访问修饰符ACC_SYNCHRONIZED来完成同步。3.synchronized只能持有一个对象监视器。4.synchronized强制所有锁的获取和释放都在一个代码块中。5.synchronized对锁的释放是隐式的。运行超出代码块时，自动释放。6.synchronized在释放了monitor之后，随机选取新的线程获取monitor。当线程数多的时候，可能会导致部分线程一直获取不到锁 API使用上的区别何时获取锁、何时释放锁，ReentrantLock相对来说更自由，可以由开发者自己来决定，且支持多个条件变量。但synchronized却不行，只能被动的在synchronized代码范围结束时释放锁。 ReentrantLock还支持公平锁和非公平锁，在一些需要保证线程FIFO获取锁的场景下，可以使用ReentrantLock的公平锁，synchronized是没有这个特性的。但是为了避免出现死锁，ReentrantLock的锁必须在finally中释放，例如下面代码： 12345678Lock lock = new ReentrantLock();lock.lock();try &#123; // do something&#125;finally &#123; lock.unlock(); &#125; 所以在使用ReentrantLock时需要慎重！ 锁的范围关于锁的范围我是这么理解的，如果某个类的对象中存在一把锁可以被该类的所有对象访问，那么这个锁就是类级别的锁；如果某个类的对象中存在一把锁只能被该对象访问，那么这个锁就是对象级别的锁。其实这样解释不是很严谨，下面具体看例子。 ReentrantLock当ReentrantLock对象是静态的时候就是类级别的锁，否则锁就是对象级别的锁。 12345//类级别的锁static Lock lock1 = new ReentrantLock();//对象级别的锁Lock lock2 = new ReentrantLock(); synchronized当synchronized同步的变量或者方法是静态的时候，锁就是类级别的锁，否则就是对象级别的锁 类级别的锁这里使用的是类监视器 12345678910111213141516171819//变量static int count = 0;synchronized(count)&#123; //...&#125;//方法public synchronized static void method()&#123; //...&#125;//类加载器synchronized(Demo2.class)&#123; //...&#125;//或者synchronized(this.getClass())&#123; //...&#125; 对象级别的锁 1234567891011121314//变量int count = 0;synchronized(count)&#123; //...&#125;//方法public synchronized void method()&#123; //...&#125;synchronized(this)&#123; ...&#125; ReentrantLock源码解析如果阅读过看AQS的相关源码，查看ReentrantLock类源码将会很轻松的。阅读ReentrantLock源码时主要就是查看它三个静态内部类的实现，以及公平锁和非公平锁的实现差异。 在查看源码的时候需要注意，在实现ReentrantLock的时候，AbstractQueuedSynchronizer类中的state字段的作用是记录重入锁的重入次数，每次获取锁的时候state字段值加一，释放锁的时候state值减一。 内部类主要有这三个静态内部类java.util.concurrent.locks.ReentrantLock.Sync、java.util.concurrent.locks.ReentrantLock.NonfairSync以及java.util.concurrent.locks.ReentrantLock.FairSync。Sync类是另外两个的父类，NonfairSync类实现的是非公平锁，FairSync类实现的是公平锁。 SyncSync类是一个抽象类，它主要声明了lock抽象方法,实现了获取非公平锁的方法nonfairTryAcquire，以及释放锁的方法tryRelease。 final boolean nonfairTryAcquire(int acquires)尝试获取非公平锁。 123456789101112131415161718192021final boolean nonfairTryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; //重入次数为0, 锁未被占用时,直接占用就可以了 if (compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123;//锁已经被当前线程占用了 int nextc = c + acquires; //重入次数加1 if (nextc &lt; 0) // overflow 重入次数超过int范围的时候,报错 throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); //更新重入次数 return true; &#125; //锁已经被其它线程占用了 return false;&#125; protected final boolean tryRelease(int releases)尝试释放锁，当重入计数器state值变为0后，表示以及没有锁的占用了。 12345678910111213141516protected final boolean tryRelease(int releases) &#123; //重入次数减一 int c = getState() - releases; //当前的线程不可以释放其它线程的锁 if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) &#123; //当state为0时,说明锁已经完全释放了 free = true; setExclusiveOwnerThread(null); &#125; setState(c); return free;&#125; NonfairSync非公平锁的实现，主要是实现了lock方法。 12345678910111213141516//获取锁final void lock() &#123; //判断一下锁是否已经被获取: //若锁已经被获取,则进入阻塞;否则直接获取 if (compareAndSetState(0, 1)) //CAS判断锁是否已经被获取 //符合条件 setExclusiveOwnerThread(Thread.currentThread()); //直接锁定 else //锁已经被获取,调用AQS尝试获取锁以及进入阻塞 acquire(1);&#125;//尝试获取锁，被AQS调用protected final boolean tryAcquire(int acquires) &#123; return nonfairTryAcquire(acquires);&#125; FairSync公平锁的实现 123456789101112131415161718192021222324252627282930/** * 由于需要判断是否公平,所以和NonfairSync#lock()的实现稍有不同,并没有在AQS的state值为0时,立马获取到锁。 */final void lock() &#123; acquire(1);&#125;/** * 除了多调用了hasQueuedPredecessors方法,其它和和nonfairTryAcquire几乎一样 */protected final boolean tryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); //获取到重入次数 if (c == 0) &#123; //未重入过 if (!hasQueuedPredecessors()/*查看是否有比当前线程等待更久的线程,有就返回true(不通过),没有就返回false(通过),和nonfairTryAcquire相比,只多出了这一块*/ &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; &#125; return false;&#125; 对比ReentrantLock.Sync#nonfairTryAcquire方法的实现，我可以看出公平锁中多调用了方法java.util.concurrent.locks.AbstractQueuedSynchronizer#hasQueuedPredecessors。 12345678910111213141516/** * 查看是否有比当前线程等待更久的线程,有就返回true,没有就返回false.&lt;br /&gt; * * 若存在比当前线程等待还要久的线程需要满足下面2个条件中的任意一个即可: * 1. 线程等待队列中存在等待的线程,且线程不是队列中的第一个线程; * 2. 线程等待队列中存在等待的线程,其它线程正在初始化线程队列,已经修改好了tail指针，但head的next指针还没有修改好，导致head.next为空，可以查看方法java.util.concurrent.locks.AbstractQueuedSynchronizer#enq * */public final boolean hasQueuedPredecessors() &#123; // thread is first in queue. Node t = tail; // Read fields in reverse initialization order Node h = head; Node s; return h != t &amp;&amp; ((s = h.next) == null || s.thread != Thread.currentThread());&#125; 这段代码有点饶人，其实简单理解，就是判断线程队列中，当前线程前边有没有其它线程，是的话返回true，没有的话就返回false。 关键方法平时用的比较多的方法就是lock和unlock。截止到jdk1.7时ReentrantLock类的很多方法都是对Sync类的二次封装。 123456//创建非公平锁Lock lock1 = new ReentrantLock();Lock lock2 = new ReentrantLock(false);//创建公平锁Lock lock3 = new ReentrantLock(true); 总结ReentrantLock是一个基于AQS实现的高性能的可重入锁，相比synchronized来说使用时更灵活、且效率更高。如果对线程调用的顺序不是很关系，可以使用非公平锁；否则就使用公平锁。非公平锁的性能是优于公平锁的。相比synchronized来说，ReentrantLock太过灵活，新手使用很容易出现问题，unlock的代码务必写到finally代码块中。","categories":[{"name":"java","slug":"java","permalink":"http://www.leocook.org/blog/categories/java/"}],"tags":[{"name":"ReentrantLock synchronized","slug":"ReentrantLock-synchronized","permalink":"http://www.leocook.org/blog/tags/ReentrantLock-synchronized/"}]},{"title":"Java并发(五)：AQS框架","slug":"java-2017-07-15-Java并发-五-：AQS框架","date":"2017-07-14T16:00:00.000Z","updated":"2019-06-09T15:36:12.497Z","comments":true,"path":"2017/07/15/java-2017-07-15-Java并发-五-：AQS框架/","link":"","permalink":"http://www.leocook.org/2017/07/15/java-2017-07-15-Java并发-五-：AQS框架/","excerpt":"关键词：AQS CLH AQS指的是java.util.concurrent.locks.AbstractQueuedSynchronizer这个类，在阅读Jdk源码时，你会发现这个类是java.util.concurrent包的核心。例如在ReentrantLock、ReentrantReadWriteLock、CountDownLatch等类中存在内部类Sync，都是用了AQS。如果想清楚整个Java的并发体系，这个类必读不可。","text":"关键词：AQS CLH AQS指的是java.util.concurrent.locks.AbstractQueuedSynchronizer这个类，在阅读Jdk源码时，你会发现这个类是java.util.concurrent包的核心。例如在ReentrantLock、ReentrantReadWriteLock、CountDownLatch等类中存在内部类Sync，都是用了AQS。如果想清楚整个Java的并发体系，这个类必读不可。 AQS概括先说说几个概念 独占锁&amp;共享共享锁 独占锁资源最多同时只能被一个线程占用 共享共享锁资源同时可以被多个线程占用 公平锁&amp;非公平锁 公平锁线程按照提交的顺序依次去获取资源，按照一定的优先级来，FIFO 非公平锁线程获取资源的顺序是无序的 AQS中的线程阻塞队列是基于CLH lock queue来实现的，后边会重点说明一下。 AQS的关键点有三个 提供变量state来维护状态 维护线程阻塞队列 阻塞和唤醒线程 AQS中的关键字段 private volatile int state;同步状态，通过设置state来达到同步的效果.该字段在不同的子类中，代表的意义是不同的 private transient volatile Node tail;线程等待队列的尾部，只有enq方法在向队列添加新线程时，才会修改该值 private transient volatile Node head;线程等待队列的头部 下面是offset相关的变量，主要是为了提供CAS操作而设立的变量： private static final long stateOffset;同步状态的offset private static final long headOffset;等待队列头的offset private static final long tailOffset;等待队列尾的offset private static final long waitStatusOffset;当前节点的等待状态的offset private static final long nextOffset;当前节点的下一个节点offset AQS中的关键方法已经实现的方法如下 public final void acquire(int arg)获取锁，一般会调用acquire(1)来获取。 public final boolean release(int arg)释放锁，一般会调用release(1)来释放。 需要被子类实现的方法 protected boolean tryAcquire(int arg)尝试获取独占锁,在获取前会检查同步状态是否允许获取独占锁。获取成功则返回true，返回false则把线程加入到等待队列。 protected boolean tryRelease(int arg)尝试通过设置同步状态，来释放独占锁。 protected int tryAcquireShared(int arg)尝试获取共享锁,在获取前会检查同步状态是否允许获取共享锁。获取成功则返回true，返回false则把线程加入到等待队列。 protected boolean tryReleaseShared(int arg)尝试通过设置同步状态，来释放共享锁。 CLH lock queue和自旋锁CLH lock queueCLH lock queue是一个存放线程的FIFO队列，队列中的每个线程都在等待它前一个线程释放锁，前面的线程释放了锁之后，该线程将会开始回解除锁并开始执行线程。 Spin Lock（自旋锁）是线程通过循环来等待而不是睡眠。 java.util.concurrent.locks.AbstractQueuedSynchronizer.Node类该类实现了CLH lock queue的一个变种，它的数据结构大致如下：123 +------+ prev +-----+ +-----+head | | &lt;---- | | &lt;---- | | tail +------+ +-----+ +-----+ 图中没有把next指向描述出来。 关键字段 SHARED共享模式的节点常量，供其方式使用 EXCLUSIVE独占模式的节点常量，供其方式使用 waitStatus当前节点/线程的等待状态,该变量会有下面几个值：CANCELLED/SIGNAL/CONDITION/PROPAGATE CANCELLED由于超时或者中断线程被中断，节点/线程的状态变为该值时，状态将不会发生变化，可以理解线程被中断之后，就停止了，状态自然不会发生什么变化了。 SIGNAL表示当前节点以及成功执行,等待unpark CONDITION标识线程在condition queue中处于等待状态,等待某一个条件 PROPAGATE后续结点会传播唤醒的操作，共享锁可执行,独占锁不可 prev等待队列中，该节点的上一个节点 next等待队列中，该节点的下一个节点 thread当前的线程 关键方法其实没有什么重要的方法需要在这里提，Node这个类就是个用来实现双向队列的数据结构，它是双向队列中的一个节点。 AQS详解在“AQS概括”中已经简单的描述了一些关键的字段和方法，相信在了解了CLH lock queue、自旋锁，以及LockSupport类的设计和实现之后，再回过头来查看AQS内部的实现细节将会很轻松。AQS重要实现了这几个重要的功能，CLH lock queue的维护、独占模式的获取/释放、共享模式的获取/释放。 CLH lock queue的维护我们知道AQS维护了一个FIFO的队列，那肯定就有入队和出队的操作。 CLH lock queue入队 1234567891011121314151617181920212223242526272829303132333435363738private Node addWaiter(Node mode) &#123; Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; if (pred != null) &#123; node.prev = pred; if (compareAndSetTail(pred, node)) &#123; pred.next = node; return node; &#125; &#125; enq(node); //把节点放入队列 return node;&#125;private Node enq(final Node node) &#123; for (;;) &#123; Node t = tail; if (t == null) &#123; // Must initialize if (compareAndSetHead(new Node())) tail = head; &#125; else &#123; node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125;&#125;private final boolean compareAndSetTail(Node expect, Node update) &#123; return unsafe.compareAndSwapObject(this, tailOffset, expect, update);&#125;private final boolean compareAndSetHead(Node update) &#123; return unsafe.compareAndSwapObject(this, headOffset, null, update);&#125; 当队列不为空时，节点添加到队列前，队列状态如下图所示： 节点加入队列的过程如下图所示： 当队列为空时，首次加入节点会进行一些初始化，具体的操作过程可以看下图： CLH lock queue出队 12345private void setHead(Node node) &#123; head = node; node.thread = null; node.prev = null;&#125; 独占模式的实现独占模式的获取12345public final void acquire(int arg) &#123; if (!tryAcquire(arg)/*尝试获取锁*/ &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE/*设置为独占锁状态,并把当前线程放入了队列中*/), arg)/*放入队列后,阻塞该线程*/) selfInterrupt(); //若park的原因是线程被interrupt掉了,则中断线程&#125; 这块的原理大概如下： 先通过tryAcquire来尝试获取锁，如果能获取到锁的话就返回；否则加入到队列，并阻塞线程； 线程退出阻塞的时候，通过Thread.interrupted()来判断线程是否是因为被中断才退出park的，若是的话，则中断当前线程。 具体我们可以查阅下面这几个方法的源码： acquireQueued线程节点之前以及加入到队列中了，该方法将要阻塞该线程，在退出阻塞的时候，返回该线程是否是被中断退出的。 1234567891011121314151617181920212223242526272829303132final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; //自旋锁,double check //等待前面的节点释放锁 for (;;) &#123; final Node p = node.predecessor(); //获取该节点的上一个节点 //如果前继节点就是head,现在就可以直接去尝试获取锁,如果没有其它线程的干扰,肯定是能够获取到的 if (p == head/*前继节点就是head*/ &amp;&amp; tryAcquire(arg)/*尝试获取锁*/) &#123; //前继节点出队,当前的node设置为head setHead(node); p.next = null; // help GC failed = false; return interrupted; &#125; //去除前面被取消的(CANCELLED)的线程 //若前继节点没有没被取消,则表示当前线程可以被park //park当前线程,park结束时检查是否被interrupt,若是则设置interrupted为true,跳出循环后中断线程 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; //若线程失败,则取消获取锁 if (failed) cancelAcquire(node); &#125;&#125; shouldParkAfterFailedAcquire去除前面被取消的(CANCELLED)的线程,若前继节点没有没被取消,则表示当前线程可以被park 123456789101112131415161718192021222324private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; int ws = pred.waitStatus; if (ws == Node.SIGNAL) /* * 前继节点已经成功执行,等待释放锁,所以当前的线程可以安全的park。 */ return true; if (ws &gt; 0) &#123; /* * 前继节点的线程已经被取消,所以跳过已经被取消的节点,并一直往前跳过所有连续的CANCELLED节点 * */ do &#123; node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0); pred.next = node; &#125; else &#123; /* * 当前继节点的waitStatus非SIGNAL &amp; 非CANCELLED,需要设置waitStatus的值为SIGNAL,并返回false,继续自旋 */ compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; return false;&#125; parkAndCheckInterruptpark当前线程,并返回线程是否被中断 123456private final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); //阻塞结束时,调用Thread.interrupted()来检查结束阻塞的原因是什么 return Thread.interrupted();&#125; 独占模式的释放释放独占模式，解除线程阻塞；该过程中会唤醒队列中后继节点的线程。 release该方法一般会被子类调用，例如java.util.concurrent.locks.ReentrantLock#unlock这个方法，release的实现如下： 123456789public final boolean release(int arg) &#123; if (tryRelease(arg)/*该方法被子类实现*/) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h)/*唤醒后继节点的线程*/; return true; &#125; return false;&#125; unparkSuccessor唤醒后继节点的线程 123456789101112131415161718192021private void unparkSuccessor(Node node) &#123; /* * node节点以及执行完成,不用加锁了,所以可以把等待状态设置为0。 */ int ws = node.waitStatus; if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); /* * 唤醒node节点的后继节点线程。 */ Node s = node.next; if (s == null || s.waitStatus &gt; 0) &#123; s = null; for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; &#125; if (s != null) LockSupport.unpark(s.thread);&#125; 共享模式的实现共享模式的获取 acquireShared 1234public final void acquireShared(int arg) &#123; if (tryAcquireShared(arg)/*尝试获取共享模式,若无许可则进入等待*/ &lt; 0) doAcquireShared(arg);&#125; doAcquireShared该方法和独占模式的acquireQueued方法很像，主要的区别就是在setHeadAndPropagate中，如果当前节点获取到了许可，且还有多余的许可，则继续让后继节点获取许可并唤醒他们，并一直往后传递下去。 12345678910111213141516171819202122232425262728293031private void doAcquireShared(int arg) &#123; final Node node = addWaiter(Node.SHARED)/*添加节点到队列*/; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; //获取到前继节点 final Node p = node.predecessor(); if (p == head) &#123; //如果前继节点就是head节点的话 int r = tryAcquireShared(arg);//尝试获取共享模式 if (r &gt;= 0) &#123; setHeadAndPropagate(node, r); //设置当前节点为head节点 p.next = null; // help GC if (interrupted) //判断退出xxxx的时候,是否是被interrupt掉的 selfInterrupt(); failed = false; return; &#125; &#125; //去除前面被取消的(CANCELLED)的线程 //若前继节点没有没被取消,则表示当前线程可以被park //park当前线程,park结束时检查是否被interrupt,若是则设置interrupted为true,跳出循环后中断线程 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; setHeadAndPropagate 12345678910111213141516171819202122232425private void doReleaseShared() &#123; for (;;) &#123; Node h = head; if (h != null &amp;&amp; h != tail) &#123; //若队列里有线程节点 int ws = h.waitStatus; //线程状态为SIGNAL的时候,才可以唤醒后继节点的线程 if (ws == Node.SIGNAL) &#123; //重置Node的状态为0,unpark该节点 if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases unparkSuccessor(h); //unpark &#125; else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) //如果线程状态为0则更新状态为PROPAGATE，让后继节点传播唤醒操作,失败重试 continue; // loop on failed CAS &#125; //head变化了,说明该节点被唤醒了,则继续唤醒后边的节点 if (h == head) // loop if head changed break; &#125;&#125; 共享模式的释放当tryReleaseShared返回true的时候,会把一个或多个线程从共享模式中唤醒。 1234567public final boolean releaseShared(int arg) &#123; if (tryReleaseShared(arg)/*该方法由子类实现*/) &#123; doReleaseShared(); return true; &#125; return false;&#125; 总结AQS是java.util.concurrent中的核心，它的内部类java.util.concurrent.locks.AbstractQueuedSynchronizer.Node通过实现CLH lock queue的一个变种来轮询执行CAS操作来调用Unsafe.park()获取锁。AQS理解了，下一篇会聊聊它的一个具体的实现java.util.concurrent.locks.ReentrantLock类，这样会更直观的展现AQS的精妙。","categories":[{"name":"java","slug":"java","permalink":"http://www.leocook.org/blog/categories/java/"}],"tags":[{"name":"AQS","slug":"AQS","permalink":"http://www.leocook.org/blog/tags/AQS/"}]},{"title":"Java并发(四)：LockSupport","slug":"java-2017-07-08-Java并发-四-：LockSupport","date":"2017-07-07T16:00:00.000Z","updated":"2019-06-09T15:36:36.640Z","comments":true,"path":"2017/07/08/java-2017-07-08-Java并发-四-：LockSupport/","link":"","permalink":"http://www.leocook.org/2017/07/08/java-2017-07-08-Java并发-四-：LockSupport/","excerpt":"关键词：LockSupport 前面我们讨论了sun.misc.Unsafe类，该类提供了面向操作系统直接操作内存和CPU的方法，例如分配内心和阻塞线程等等。但是该类在使用时是不安全的，所以jdk在不同的场景下对它做了不同的包装。 java.util.concurrent.locks.LockSupport类就是对sun.misc.Unsafe类进行了一些封装，主要提供一些锁的基础操作。","text":"关键词：LockSupport 前面我们讨论了sun.misc.Unsafe类，该类提供了面向操作系统直接操作内存和CPU的方法，例如分配内心和阻塞线程等等。但是该类在使用时是不安全的，所以jdk在不同的场景下对它做了不同的包装。 java.util.concurrent.locks.LockSupport类就是对sun.misc.Unsafe类进行了一些封装，主要提供一些锁的基础操作。 LockSupport阻塞线程的机制与Object的wait和notify是不一样的： 调用API层面的区别LockSupport的park和unpark可以用“线程”作为该方法的参数，语义更合乎逻辑。Object的wait和notify是由监视器对象来调用的，对线程来说，它的阻塞和唤醒是被动的，不能准确的控制某个指定的线程，要么随机唤醒（notify）、要么唤醒全部（notifyAll）。 实现原理的区别Object的wait和notify以及synchronized都是通过占用和释放该对象的监视器来实现锁的获取和释放。LockSupport不使用对象的监视器，每次执行park时会消耗1个许可，每次执行unpark时会获得1个许可。 如果查看Unsafe的C++源码会发现，这个许可，其实就是一个_counter变量。当执行park的时候，若_counter值大于0则立马返回并把_counter的值设置为0，线程不会阻塞；若_counter值等于0，则阻塞当前线程。可以理解这个过程将会消耗一个许可，若没有许可被消耗，则阻塞。当执行unpark的时候，将会把_counter的值设置为1。可以理解这个过程是是给线程添加一个许可，且多次调用也只会添加一个许可。_counter为1的时候表示许可可用，为0的时候表示许可不可用。如果能够很好的理解这个“许可”的设计，在查看LockSupport类源码的时候，将会轻松很多。 LockSupport类的核心字段12345//unsafe，用于CAS操作private static final Unsafe unsafe = Unsafe.getUnsafe();//Thread中parkBlocker的内存偏移量private static final long parkBlockerOffset; 在java.lang.Thread类中有个字段parkBlocker用来存放该线程阻塞时是被哪个对象阻塞的。 LockSupport类的核心方法 public static void park()如果许可可用，则使用该许可，并立刻返回；否则阻塞当前线程。 public static void park(Object blocker)如果许可可用，则使用该许可，并立刻返回；否则阻塞当前线程，并告诉线程是谁阻塞了它。 public static void parkNanos(long nanos)如果许可可用，则使用该许可，并立刻返回；否则阻塞当前线程nanos纳秒。 public static void parkNanos(Object blocker, long nanos)如果许可可用，则使用该许可，并立刻返回；否则阻塞当前线程nanos纳秒，并告诉线程是谁阻塞了它。 public static void parkUntil(long deadline)如果许可可用，则使用该许可，并立刻返回；否则阻塞当前线程直到时间deadline。与parkNanos相比，这里的时间是绝对时间戳。 public static void parkUntil(Object blocker, long deadline)如果许可可用，则使用该许可，并立刻返回；否则阻塞当前线程直到时间deadline，并告诉线程是谁阻塞了它。 public static void unpark(Thread thread)若许可不可用，则使许可可用。若线程因为调用了park而阻塞，则它将解除阻塞状态。否则保证下一次调用park不会受阻。 private static void setBlocker(Thread t, Object arg)给线程t设置阻塞对象，告诉t是谁阻塞了它。 public static Object getBlocker(Thread t)获取是谁阻塞了线程t Thread.interrupt()方法Thread.interrupt()方法不会中断一个正在运行的线程。当线程被Object.wait、Thread.join和Thread.sleep三种方法阻塞时，若调用了Thread.interrupt()方法，线程将退出阻塞状态，并会抛出一个InterruptedException中断异常。 LockSupport.park()也能够响应中断信号，但是它不会抛出InterruptedException中断异常。 Thread.interrupted() &amp; Thread.isInterrupted()方法测试线程是否已经中断。前者是静态方法，后者不是。当我们需要停止一个线程的时候，一般有两种方式： 通过共享变量； 通过调用线程的interrupt方法。 前者在线程阻塞的时候不能够被中断，只有当线程执行的时候才可以;而后者只能在线程处于阻塞的时候才能够被中断，线程执行时不可以中断。 interrupted statusinterrupted status是线程的中断状态，被JVM的C++代码维护着。 在调用Thread.join和Thread.sleep之后，将会设置interrupted status； 当退出阻塞、抛出InterruptedException异常的时候，interrupted status将会被清除； 调用public static boolean interrupted()方法后，会清除interrupted status 总结java.util.concurrent.locks.LockSupport类是对sun.misc.Unsafe类的二次封装，主要提供了一些线程阻塞的工具。该类在AQS中被大量的运用，在阅读AQS的源码时，需要对该类有所了解。","categories":[{"name":"java","slug":"java","permalink":"http://www.leocook.org/blog/categories/java/"}],"tags":[{"name":"LockSupport","slug":"LockSupport","permalink":"http://www.leocook.org/blog/tags/LockSupport/"}]},{"title":"Java并发(三)：Unsafe和CAS","slug":"java-2017-06-24-Java并发-三-：Unsafe和CAS","date":"2017-06-23T16:00:00.000Z","updated":"2019-06-09T15:36:19.912Z","comments":true,"path":"2017/06/24/java-2017-06-24-Java并发-三-：Unsafe和CAS/","link":"","permalink":"http://www.leocook.org/2017/06/24/java-2017-06-24-Java并发-三-：Unsafe和CAS/","excerpt":"关键词：CAS、Unsafe CAS是JSR-166和核心思想，Java中的CAS思想被C/C++实现，并被sun.misc.Unsafe类包装，供Java调用。查看其对应的C/C++源码可以点击这里。Java中的非阻塞锁是基于AQS实现的，而AQS的设计就是建立在CAS和Unsafe类上的，所以学习本文还是很有必要性的。 在学习CAS、sun.misc.Unsafe类之前，我们需要知道一些基础的概念： Java并发：原子性、可见性、有序性 Java并发：volatile关键字","text":"关键词：CAS、Unsafe CAS是JSR-166和核心思想，Java中的CAS思想被C/C++实现，并被sun.misc.Unsafe类包装，供Java调用。查看其对应的C/C++源码可以点击这里。Java中的非阻塞锁是基于AQS实现的，而AQS的设计就是建立在CAS和Unsafe类上的，所以学习本文还是很有必要性的。 在学习CAS、sun.misc.Unsafe类之前，我们需要知道一些基础的概念： Java并发：原子性、可见性、有序性 Java并发：volatile关键字 CASCAS是Compare and Swap的缩写，即比较并转换。在设计并发算法的时候会用到的技术，JSR-166特性是完全建立在CAS的基础上的，可见其重要性。维基百科对CAS的解释可以看这里：CAS比较并交换 Java就是通过Unsafe类的compareAndSwap系列方法实现的CAS，当前的绝大多是CPU都是支持CAS的，不同厂商的CPU的CAS指令可能是不同的。 CAS原理CAS有三个操作数：内存位置V，预期值A和新值B（将要被修改成的值）。在修改值的时候，若内存位置V存的值和预期值A相等，那么就把内存位置的V的值修改为B，返回true；否则，什么都不做，并返回false。在Java的实现中，V可以是一个存储A地址的long整数，A是一个使用了volatile修饰的基础数据类型或者对象，B的类型和A的类型一致。 AtomicInteger示例java.util.concurrent.atomicAtomicInteger是Jdk提供的一个类，如果你需要一个读写有原子性的整数类型，使用它就对了！我们可以趴一下AtomicInteger的源码，可以观察到下面这个方法: 123456789101112131415/** * 在当前值上原子性的自增1 */public final int getAndIncrement() &#123; for (;;) &#123; int current = get(); int next = current + 1; if (compareAndSet(current, next)) return current; &#125;&#125;public final boolean compareAndSet(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, valueOffset, expect, update);&#125; 我们能够看到，为了保证操作的原子性，调用了compareAndSet方法，而compareAndSet方法又调用了Unsafe的compareAndSwapInt方法。 CAS的ABA问题维基百科上的说明是如下： 1.进程P1读取了一个数值A 2.P1被挂起(时间片耗尽、中断等)，进程P2开始执行 3.P2修改数值A为数值B，然后又修改回A 4.P1被唤醒，比较后发现数值A没有变化，程序继续执行。对于线程P1来说，数值一直是A未变化过，但实际上数值发生过变化的。关于这个维基百科里说的很清楚。 sun.misc.Unsafe我们在日常开发的时候，Java是无法直接做操作系统级别的访问，如果想访问操作系统，我们可以使用C/C++来开发，然后使用JNI或者JNA来调用C/C++的库。JVM中存在sun.misc.Unsafe这样的一个类，该类中提供了一系列的底层方法，这些方法可以直接操作操作系统的内存等。该类在设计的时候，默认是不让一般的开发人员不可以使用，只有授信代码才可以使用。 Unsafe类是单例的从下面的代码中，我们可以看出Unsafe是单例的。且只能通过类加载器来获取，不能直接使用new来创建。 1234567891011121314private static final Unsafe theUnsafe;private Unsafe() &#123;&#125;@CallerSensitivepublic static Unsafe getUnsafe() &#123; Class var0 = Reflection.getCallerClass(); if(var0.getClassLoader() != null) &#123; throw new SecurityException(&quot;Unsafe&quot;); &#125; else &#123; return theUnsafe; &#125;&#125; 创建Unsafe对象如果你直接调用getUnsafe方法来创建Unsafe对象，那么在编译的时候你将会得到警告提示，例如对于下面这段代码： 1234567import sun.misc.Unsafe;public class ThreadDemo&#123; public static void main(String[] args)&#123; Unsafe unsafe = Unsafe.getUnsafe(); &#125;&#125; 我们编译时将会看到如下的警告信息： 12345678910ThreadDemo.java:1: 警告: Unsafe是内部专用 API, 可能会在未来发行版中删除import sun.misc.Unsafe; ^ThreadDemo.java:6: 警告: Unsafe是内部专用 API, 可能会在未来发行版中删除Unsafe unsafe = Unsafe.getUnsafe();^ThreadDemo.java:6: 警告: Unsafe是内部专用 API, 可能会在未来发行版中删除Unsafe unsafe = Unsafe.getUnsafe(); ^3 个警告 但是我可以对代码进行授信处理，Java是通过内加载器是否为根类加载器判断是否授信的，我们可以使用JVM参数bootclasspath： 1java -Xbootclasspath:/usr/java/jdk1.7.0/jre/lib/rt.jar:. com.mishadoff.magic.UnsafeClient Jdk不建议开发者直接创建Unsafe对象，如果我们必须要创建，那么我们可以使用反射的方式来创建： 12345678import java.lang.reflect.Field;import sun.misc.Unsafe;import sun.reflect.Reflection;Field field = Unsafe.class.getDeclaredField(&quot;theUnsafe&quot;);field.setAccessible(true);Unsafe unsafe = (Unsafe) field.get(null);System.out.println(unsafe); Unsafe类的方法介绍Unsafe类中的方法有很多，其实我们需要关注只有这么几个。 内存分配allocateMemory：分配内存（非堆内存）reallocateMemory：重新分配内存freeMemory：释放内存 线程操作park：锁定当前的线程unpark：解锁指定的线程 CAS操作public final native boolean compareAndSwapXXX(Object o,long offset,K expected,K x)例如：compareAndSwapInt、compareAndSwapLong、compareAndSwapObject等等。该方法也就是Java中的CAS的实现，这个方法会比较expected的值和内存地址在offset位置的值是否一样，如果一样则会更新expected的值为x，并返回true，否则返回false。 还有其它的相关方法可以看下面： public native int addressSize()本地指针所占用的存储大小，值为4或8. public native int pageSize()返回内存页信息 public native Object allocateInstance(Class cls)分配一个指定的对象，但是不执行任何构造方法。 public native int arrayBaseOffset(Class arrayClass)返回数组的内存起始位置 copyMemory把某段内存块中的数据copy到另外一段内存块中。 defineAnonymousClass定义一个匿名类。 defineClass让JVM定义一个类，不进行安全性检查。 ensureClassInitialized确定类已经被初始化了。这个经常在访问类的静态成员变量时会结合访问。 fieldOffset返回字段在对象中的内存offset freeMemory释放内存 getXXX(Object o,long offset)获取对象o中内存偏移量为offset的值。 getUnsafe单例，获取Unsafe对象 monitorEnter(Object o)锁定对象，直到调用了monitorExit(Object o)后才会被解锁 monitorExit(Object o)解锁对象，该对象必须是之前就已经被锁定了(被执行过monitorExit) 详情可以查看：http://www.docjar.com/docs/api/sun/misc/Unsafe.html Examples for Unsafe sun.misc.Unsafe#allocateInstance()不使用类的构造方法，来生成一个类的对象。 12345678910111213import java.lang.reflect.Field;import sun.misc.Unsafe;import sun.reflect.Reflection;Field field = Unsafe.class.getDeclaredField(&quot;theUnsafe&quot;);field.setAccessible(true);Unsafe unsafe = (Unsafe) field.get(null);Person person1 = (Person)unsafe.allocateInstance(Person.class);System.out.println(person1); //null: 0Person person2 = new Person();System.out.println(person2);//test: 22 我们对比person1和person2，可以发现person1是没有使用构造方法的，而person2是使用了构造方法的。 objectFieldOffset、putXXX操作对象成员的值：objectFieldOffset(Field var1)是获取成员变量的相对于对象内存的偏移量。putLong，putInt，putDouble，putChar，putObject等，可以直接修改对象内存中的数据，可以突破访问修饰符(private/protected)的限制。可以查看下面的例子： 12345678910111213141516171819import java.lang.reflect.Field;import sun.misc.Unsafe;import sun.reflect.Reflection;Field field = Unsafe.class.getDeclaredField(&quot;theUnsafe&quot;);field.setAccessible(true);Unsafe unsafe = (Unsafe) field.get(null);Person person1 = (Person)unsafe.allocateInstance(Person.class);System.out.println(person1); //null: 0Class clazz = person1.getClass();Field name = clazz.getDeclaredField(&quot;name&quot;);Field age = clazz.getDeclaredField(&quot;age&quot;);unsafe.putObject(person1, unsafe.objectFieldOffset(name),&quot;张三&quot;);unsafe.putInt(person1, unsafe.objectFieldOffset(age),18);System.out.println(person1); //张三: 18 关于Unsafe类里的方法，感兴趣的网上查找阅读相关的C/C++实现。 大数组操作Java最大只能创建长度为Integer.MAX_VALUE的数组，我们知道当JVM消耗内存过高的时候发生GC，性能上会大打折扣。那么当我们需要创建一个大数组，且不想因为GC而引起大的性能损耗时，我们能想到的就是使用堆外内存。例如下边的代码就实现了一个大的字节数组： 1234567891011121314151617181920212223class BigByteArray &#123; private final static int BYTE = 1; private long size; private long address; public BigArray(long size) &#123; this.size = size; address = getUnsafe().allocateMemory(size * BYTE); &#125; public void set(long i, byte value) &#123; getUnsafe().putByte(address + i * BYTE, value); &#125; public int get(long idx) &#123; return getUnsafe().getByte(address + idx * BYTE); &#125; public long size() &#123; return size; &#125;&#125; 总结学习完Unsafe和CAS之后，我们再来学习Java中的AQS的实现，就好理解了。","categories":[{"name":"java","slug":"java","permalink":"http://www.leocook.org/blog/categories/java/"}],"tags":[{"name":"CAS Unsafe","slug":"CAS-Unsafe","permalink":"http://www.leocook.org/blog/tags/CAS-Unsafe/"}]},{"title":"CAP理论","slug":"java-2017-07-01-CAP理论","date":"2017-06-23T16:00:00.000Z","updated":"2019-06-09T15:36:40.203Z","comments":true,"path":"2017/06/24/java-2017-07-01-CAP理论/","link":"","permalink":"http://www.leocook.org/2017/06/24/java-2017-07-01-CAP理论/","excerpt":"关键词：CAP、可用性、分区容错性、强一致性、弱一致性、最终一致性 CAP理论在互联网的知名度挺高，有些开发在设计分布式系统的时候甚至会把它作为衡量系统设计的标准。CAP理论指出任何分布式系统在可用性、一致性、分区容错性这3个点上最多只能同时满足2点。但事实上，CAP理论在被人称之为定理的同时，也伴随着很多争议。其实我个人并不觉得它是一个定理。","text":"关键词：CAP、可用性、分区容错性、强一致性、弱一致性、最终一致性 CAP理论在互联网的知名度挺高，有些开发在设计分布式系统的时候甚至会把它作为衡量系统设计的标准。CAP理论指出任何分布式系统在可用性、一致性、分区容错性这3个点上最多只能同时满足2点。但事实上，CAP理论在被人称之为定理的同时，也伴随着很多争议。其实我个人并不觉得它是一个定理。 CAP理论的起源CAP理论起源于伯克莱加州大学的计算机科学家Eric Brewer在2000年的分布式计算原则研讨会（PODC）上提出的猜想：一致性（Consistency）、可用性（Availability）、分区容错性（Partition tolerance）三者无法在分布式系统中同时满足。其实Eric Brewer教授当时的猜想是建立在webservice集群场景上的，并对CAP理论进行定义： C(一致性)系统在执行过某项操作后，仍处于状态一致。 A(可用性)分布式系统中的每个节点都能响应客户端的请求，数据可以不是最新的。 P(分区容错性)当分布式系统遇到节点故障、网络分区故障的时候，仍然能够提供一致性和可用性保障。 单看上面的理论，很抽象。下面简单聊聊我对它的理解： C(一致性)一致性可以从客户端和服务端两个角度来看。从服务端来说，一致性就是指在分布式系统中修改了某个值之后，该值在其它节点上的副本也能立马更新到最新的值，并且客户端写、各分区数据更新到最新这一系列的操作组合是原子的。从客户端来说，当修改了某条记录之后，后续的所有请求都能够返回到更新后的新数据。为此又衍生出了三个概念： 强一致性分布式系统中某个值发生了变化后，必须要保证节点的副本值同时更新到最新值。保证该值在更新后，后续所有访问都能访问到返回最新的值。 弱一致性分布式系统中某个值发生了变化后，不能保证部分和所有的数据的副本能立马被更新到最新状态。后续的访问可能部分请求不能返回最新的值，可能所有请求都不能返回最新的值，但经过“不一致时间窗口”这段时间后，所有数据的副本都更新到了最新状态，客户端后续的访问也都会返回最新的数值。 最终一致性最终一致性是弱一致性的特殊形式，系统保证数据在后续没有再次被更新的前提下，该数据的所有副本最终都将会变为最新的数据，且客户端对它后续的所有访问返回的都是最新的数据。 A(可用性)这里的可用性指的不是分布式环境的可用性，这里的可以性是指分布式中单个节点可响应客户端的请求，响应返回的数据可以是最新状态，也可以不是最新状态，只要客户端的请求能被响应，就算该节点是可用的。 P(分区容错性)当分布式系统遇到节点故障、网络分区故障的时候，分布式系统仍然能够给客户端提供服务。其实，就是客户端在请求到故障节点时，会跳过该节点，并继续请求其它正常的节点。 CAP三选二 CA without P（一致性、可用性）如果同时满足了一致性和可用性，那么存在两种情况：第一种是单机环境，这样就可以保证C和A了；第二种是分布式环境，每次数据修改时都对分区中对这条数据的副本加锁，不给其它操作来修改它，当所有副本数据都是最新之后，再释放锁，但是这些设计出来的系统效率极为低下，甚至还不如单机环境。所以无法满足P。 CP without A（一致性、分区容错性）如果要保证在多个节点环境下，每个节点之间都是强一致的，那么肯定会导致分区之间存在同步时间，当并发高的时候，同步时间可能会延长很多，这样就不能保证每个节点的数据都是最新的了。但是可以通过P保证用户请求指向更新后的那些节点，以此来保障整个系统可以正常工作。 AP without C（可用性、分区容错性）如果保证了可用性、以及分区容错性，那就必须要放弃一致性，例如某个节点故障了，这时候该节点将会被划分到“故障分区”进行隔离，隔离后节点将接收不到请求，自然就失去了一致性。 CAP理论的确定2002年，麻省理工学院的Lynch和Gilbert证明了Eric Brewer对CAP的猜想，并发表了论文。从此CAP理论成为了定理。 对CAP理论的质疑其实初看CAP理论的人都会对此表示一头雾水，很难用一两句简洁的语言来把它描述清楚。业界对它的质疑还是比较多的，下面简单的列两个： 质疑1不适合用于数据库事务架构。 质疑2使用不可变模型来降低CAP的复杂性，传统的CURD变为CR，例如Hadoop中HDFS的设计，以及Hbase的WAL Log设计。 还有其他质疑就不列举了。 作者的回应作者的回应主要是明确、缩小CAP理论的使用场景，例如CAP理论只局限在原子读写的场景，并申明不支持数据库事务等的场景。 总结个人觉得学习CAP理论，当做了解分布式中几个重要场景就可以了，例如：如何保障数据多副本同步问题；故障转移的实现等。除此以外，在查看一些并发资料的时候，会经常遇到强一致性、弱一致性、最终一致性这些词，这些名词的意义，以及这几种一致性场景在不同技术中的设计思想，是需要我们去留意的。最后，相信技术但不迷信技术！","categories":[{"name":"编程思想","slug":"编程思想","permalink":"http://www.leocook.org/blog/categories/编程思想/"}],"tags":[{"name":"CAP 可用性 分区容错性 强一致性 弱一致性 最终一致性","slug":"CAP-可用性-分区容错性-强一致性-弱一致性-最终一致性","permalink":"http://www.leocook.org/blog/tags/CAP-可用性-分区容错性-强一致性-弱一致性-最终一致性/"}]},{"title":"Java并发(二)：volatile关键字","slug":"java-2017-06-17-Java并发-二-：volatile关键字","date":"2017-06-16T16:00:00.000Z","updated":"2019-06-09T15:36:54.097Z","comments":true,"path":"2017/06/17/java-2017-06-17-Java并发-二-：volatile关键字/","link":"","permalink":"http://www.leocook.org/2017/06/17/java-2017-06-17-Java并发-二-：volatile关键字/","excerpt":"关键词：volatile、happen-before、内存屏障 在阅读Java并发编程相关的jdk源码中，有个关键字volatile会经常看到，在并发编程中偶尔也会用到该关键字，但是在使用它的过程中又很容易引起混淆。volatile关键字一方面通过内存屏障禁止了指令重排，从而保证了有序性；另一方面通过内存屏障实现了可见性。 阅读本文前，你需要理解原子性、可见性、有序性这三个基本的并发特性。可以查看这里：Java并发：原子性、可见性、有序性 本文会先聊聊CPU从内存中读取值的硬件层面的过程，然后说一说volatile关键字是如何保证可见性、有序性的，以及相关的原理。最后还会列出几个示例来说明volatile不能解决的一些场景。","text":"关键词：volatile、happen-before、内存屏障 在阅读Java并发编程相关的jdk源码中，有个关键字volatile会经常看到，在并发编程中偶尔也会用到该关键字，但是在使用它的过程中又很容易引起混淆。volatile关键字一方面通过内存屏障禁止了指令重排，从而保证了有序性；另一方面通过内存屏障实现了可见性。 阅读本文前，你需要理解原子性、可见性、有序性这三个基本的并发特性。可以查看这里：Java并发：原子性、可见性、有序性 本文会先聊聊CPU从内存中读取值的硬件层面的过程，然后说一说volatile关键字是如何保证可见性、有序性的，以及相关的原理。最后还会列出几个示例来说明volatile不能解决的一些场景。 CPU读取值的过程CPU在读取数据进行计算的时候，cpu并不是直接读取内存中的值，而是先把内存中的值读到cpu的高速Cache中，然后cpu直接操作Cache中的值。cpu在完成计算后，把结果写到Cache中，然后再把Cache写回到内存中。具体的过程可以查看下图： 如果修改某个变量的值，大概会有下面几步操作： 把值从内存中读到CPU Cache中 CPU读取Cache中的值执行操作，并把修改后的值写入到CPU Cache中 数据从CPU Cache中刷到内存中 所以，只有上面三个步骤在执行的过程中不被其它操作干扰时，这个修改操作才会正常完成。 volatile变量具有可见性简言之，被volatile关键字修饰的变量在修改后，将会强制被刷到内存中，且该变量在其它CPU中的Cache将会失效，从而保证线程在修改变量值后，其它线程能立马读到。 下面详细说说使用与不使用volatile关键字的差异： 未使用volatile关键字 1234567int i=0;//共享变量//线程1的操作i=i+1//线程2的操作j=i 我们假设先执行的线程1，然后执行线程2，一般会想到j=1，但事实上却不一定。 在创建线程的时候，内存结构如下： 当执行一次线程1之后，cpu缓存中i变为了1，并把cache中的1刷到了内存中，内存结构如下： 由于在线程启动的时候已经把i的值读到了cpu的cache中，所以在执行j=i的时候，给j赋的值是0，而不是1。很显然，在线程1修改了i的值之后，线程2并没有读到修改后的i值，可以理解i的读取操作不具备可见性。 使用volatile关键字 在Java中，我们可以使用volatile关键字来保证变量的可见性，被volatile修饰后的变量在被修改后，会直接把缓存刷入内存，从而保证下次读取能够读到最新的值。如下分析： 第一步：线程1直接读取内存的i值 第二步：执行i=i+1，并把结果刷回内存，CPU2中的缓存失效，然后CPU2更新缓存，把i值赋值给j，并写回内存 这样就保证了i的操作是具备可见性的了，所以线程1修改了i之后，线程2能立刻读到修改后的值。 volatile变量不具有原子性volatile可以理解为一个轻量级的synchronized，但是volatile变量不具备原子性。synchronized对比volatile实现的是锁，锁提供了两个重要的特性：互斥（mutual exclusion） 和可见性（visibility）。正是互斥保证了操作的原子性。 那么什么时候使用volatile，什么时候使用synchronized呢？ 当变量只需要具备可见性的时候使用volatile，例如：对变量的写操作不依赖于该变量当前值； 当变量需要同时具备原子性和可见性的时候，就使用synchronized。 在使用volatile和synchronized都可以的时候，优先使用volatile，因为volatile的同步机制性能要高于锁的性能。 volatile变量一定程度上具有有序性在使用了volatile关键字之后，将会禁用指令重排，从而保证有序性。当程序执行到volatile变量的读取或写操作时，将会保证该操作前面的语句都已经执行完成且结果对后边代码具有可见性；该操作后边的代码都没有执行。 我们可以查看下面这个代码块：12345x = 2; //语句1y = 0; //语句2flag = true; //语句3x = 4; //语句4y = -1; //语句5 flag变量没有被volatile关键字修饰时由于指令重排的原因，我们可以得到下面依赖关系：我们可以看出：语句1执行完之后才可以执行语句4；语句2执行完之后才可以执行语句5；其它执行的顺序不一定。 flag变量被volatile关键字修饰后由于volatile禁用了指令重排，我们可以得到下面依赖关系：我们可以看出：语句1和语句2的顺序不一定；语句1和语句2都执行完之后执行语句3；语句3执行完之后才执行语句4和语句5；语句4和语句5谁先执行不一定。 volatile关键字的实现原理happen-before如果A happen-before B，那么A的所有操作完成后并产生结果才会执行B操作，可以说A所做的任何操作对B都是可见的。happen-before大概有下面几种场景： 1.程序次序规则：在一个单独的线程中，按照程序代码的执行流顺序，（时间上）先执行的操作happen—before（时间上）后执行的操作； 2.管理锁定规则：一个unlock操作happen—before后面（时间上的先后顺序，下同）对同一个锁的lock操作； 3.volatile变量规则：对一个volatile变量的写操作happen—before后面对该变量的读操作。 4.线程启动规则：Thread对象的start()方法happen—before此线程的每一个动作； 5.线程终止规则：线程的所有操作都happen—before对此线程的终止检测，可以通过调用Thread.join()方法、获取Thread.isAlive()的返回值等手段检测到线程已经终止执行； 6.线程中断规则：对线程interrupt()方法的调用happen—before发生于被中断线程的代码检测到中断时事件的发生； 7.对象终结规则：一个对象的初始化完成（构造函数执行结束）happen—before它的finalize（）方法的开始； 8.传递性：如果操作A happen—before操作B，操作B happen—before操作C，那么可以得出A happen—before操作C。 其实我们这里主要查看的是第3条，即volatile变量保证的有序性。在代码重排中，主要分为编译器重排和指令重排，为了实现volatile变量的内存语义，JMM会限制这两类重排，下面是JMM针对volatile变量所规定的重排规则表： 1st operation 2st operation 2st operation 2st operation Normal LoadNormal Store Volatile Load Volatile Store Normal Load No Normal Store No Volatile Load No No No Volatile Store No No 观察上述表格，可以得知Volation变量的happen—before原则： 所有的Volatile变量读都happen—before改变量的其它操作 Volation变量的所有操作都happen—before该变量的写操作 Volation变量的写操作happen—before该变量的读操作 内存屏障内存屏障也称之为内存栅栏，是一组处理器指令，用于实现对内存操作顺序限制。 1st operation 2st operation 2st operation 2st operation 2st operation Normal Load Normal Store Volatile Load Volatile Store Normal Load LoadStore Normal Store StoreStore Volatile Load LoadLoad LoadStore LoadLoad LoadStore Volatile Store StoreLoad StoreStore JMM中共有这4种屏障： LoadLoad屏障执行顺序：Load1—&gt;Loadload—&gt;Load2Load2以及后序的Load指令在加载数据之前，都能访问到Load1加载的数据。 LoadStore屏障执行顺序： Load1—&gt;LoadStore—&gt;Store2Store2以及后序的Store指令在存储数据之前，都能访问到Load1加载的数据。 StoreStore屏障执行顺序：Store1—&gt;StoreStore—&gt;Store2Store2以及后序的Store指令在存储数据之前，都能访问到Store1操作所存储的数据。 StoreLoad屏障执行顺序: Store1—&gt; StoreLoad—&gt;Load2Load2以及后序的Load指令在加载数据之前，都可以访问到Store1操作所存储的数据。 volatile关键字原理volatile实际上就是使用内存屏障的来实现可见性和有序性的： 有序性：它会确保指令重排时，使用内存屏障保证volatile变量操作前的操作都已经完成了，且在volatile变量操作完成后，才会执行后边的代码 可见性：CPU每次在修改volatile变量值之后，它会强制把数据从缓存刷到内存中去，才算本次操作完成 可见性：volatile变量值修改后，它会导致其他CPU中的缓存失效 volatile的几个使用场景 状态标记量 12345678910//使用volatile来保证标记变量的可见性volatile boolean flag = false; while(!flag)&#123; doSomething();&#125; public void setFlag() &#123; flag = true;&#125; 1234567891011//使用volatile来保证标有序性volatile boolean inited = false;//线程1:context = loadContext(); inited = true; //线程2:while(!inited )&#123;sleep()&#125;doSomethingwithconfig(context); 双重检查锁 12345678910111213141516171819//除了饿汉模式，这种写法创建单例效率最高class Singleton&#123; private volatile static Singleton instance = null; private Singleton() &#123; &#125; public static Singleton getInstance() &#123; if(instance==null) &#123; synchronized (Singleton.class) &#123; if(instance==null) //一定会把数据刷到内存中 instance = new Singleton(); &#125; &#125; return instance; &#125;&#125; volatile关键字陷阱Java是一门支持多线程的语言，为了解决线程的并发问题，使用了同步块和volatile关键字机制。synchronized关键字是同时具备原子性、可见性和有序性；而volatile关键字只具备可见性和有序性，并不具备原子性，问题就出在这里，下面我们举例代码来看看。 代码一 1234567891011121314151617181920212223242526272829303132333435363738public class Counter &#123; public static Integer count = 0; static CountDownLatch countDownLatch = null; public static void inc() &#123; //这里延迟1毫秒，使得结果明显 try &#123; Thread.sleep(1); &#125; catch (InterruptedException e) &#123; &#125; count++; countDownLatch.countDown(); &#125; public static void main(String[] args) throws InterruptedException &#123; int threadCount = 1000; countDownLatch = new CountDownLatch(threadCount); //同时启动1000个线程，去进行i++计算，看看实际结果 for (int i = 0; i &lt; threadCount; i++) &#123; Thread t = new Thread(new Runnable() &#123; @Override public void run() &#123; Counter.inc(); &#125;&#125;); t.start(); &#125; countDownLatch.await(); //这里每次运行的值都有可能不同,可能为1000 System.out.println(&quot;运行结果:Counter.count=&quot; + Counter.count); &#125;&#125; 我这边计算的结果是978，每次运行的结果应该都是不一样的。这段代码应该很好理解，上面代码是很常见的线程不安全实例。 代码二在上面代码的基础上，使用volatile来修饰count：1public volatile static int count = 0; 当执行完之后，我们发现结果仍然不是1000，原因是volatile无法保证变量的操作是原子的，只能保证变量的操作是具备可见性的。 代码三在代码一的基础上，修改了inc方法,给count++;添加同步代码：123456789101112public synchronized static void inc() &#123; //这里延迟1毫秒，使得结果明显 try &#123; Thread.sleep(1); &#125; catch (InterruptedException e) &#123; &#125; count++; countDownLatch.countDown();&#125; 查看上面的代码，我们可以发现volatile关键字是不具备原子性的。我们再使用的时候，需要避免这个坑。 当我们需要使用具备原子操作的基础类型时，我们除了使用同步代码块，还可以使用java.util.concurrent.atomic包下的原子类型,例如代码一可以这样修改：1234567891011121314151617181920212223242526272829303132333435363738public class Counter &#123; public static AtomicInteger count = new AtomicInteger(0); static CountDownLatch countDownLatch = null; public static void inc() &#123; //这里延迟1毫秒，使得结果明显 try &#123; Thread.sleep(1); &#125; catch (InterruptedException e) &#123; &#125; count.incrementAndGet(); countDownLatch.countDown(); &#125; public static void main(String[] args) throws InterruptedException &#123; int threadCount = 1000; countDownLatch = new CountDownLatch(threadCount); //同时启动1000个线程，去进行i++计算，看看实际结果 for (int i = 0; i &lt; threadCount; i++) &#123; Thread t = new Thread(new Runnable() &#123; @Override public void run() &#123; Counter.inc(); &#125;&#125;); t.start(); &#125; countDownLatch.await(); //这里每次运行的值都有可能不同,可能为1000 System.out.println(&quot;运行结果:Counter.count=&quot; + Counter.count.get()); &#125;&#125; 总结 一句话总结下，volatile关键字通过内存屏障来保证了变量的可见性和有序性。","categories":[{"name":"java","slug":"java","permalink":"http://www.leocook.org/blog/categories/java/"}],"tags":[{"name":"volatile happen-before 内存屏障","slug":"volatile-happen-before-内存屏障","permalink":"http://www.leocook.org/blog/tags/volatile-happen-before-内存屏障/"}]},{"title":"Java并发(一)：原子性、可见性、有序性","slug":"java-2017-06-12-Java并发-一-：原子性、可见性、有序性","date":"2017-06-11T16:00:00.000Z","updated":"2019-06-09T15:36:03.296Z","comments":true,"path":"2017/06/12/java-2017-06-12-Java并发-一-：原子性、可见性、有序性/","link":"","permalink":"http://www.leocook.org/2017/06/12/java-2017-06-12-Java并发-一-：原子性、可见性、有序性/","excerpt":"关键词：原子性、可见性、有序性、volatile、happen-before 并发编程中，有三个特性需要我们时刻关注的：原子性、可见性、有序性，本文主要是对这三个特性做解释，其中涉及到的volatile关键字，如果不是很理解的可以查看这里：Java并发：volatile关键字","text":"关键词：原子性、可见性、有序性、volatile、happen-before 并发编程中，有三个特性需要我们时刻关注的：原子性、可见性、有序性，本文主要是对这三个特性做解释，其中涉及到的volatile关键字，如果不是很理解的可以查看这里：Java并发：volatile关键字 原子性如果某个操作是原子的，那么该操作要么不执行，若执行的话就一定会成功。可以理解该操作在执行期间不会被其它因素中断，若是被一些不可控的元素破坏，该操作也不会产生副作用。 银行转账的操作就具备原子性，例如A向B转账1000元，其中包括了2个动作：A账户减去1000元，B账户加上1000元。很好理解，这个转账操作要么不执行，若执行必须保证这两个动作都成功才行。 编程时，假设Int类型是4字节(32位)，我们把一个Int类型的内存结构用下图展示：当执行赋值语句a = 128的时候，如果能保证要么不执行，要执行的话1~8B、9~16B、17~24B、26~32B这四个字节都成功写入，且不会被其它线程操作干扰，那么这个赋值操作就具备原子性。 在Java中，基础数据类型的变量的赋值和读取是原子性的操作。例如下面代码： 1234x = 10; //原子的y = x; //非原子的x++; //非原子的x = x + 1; //非原子的 可见性可见性就是指当多个线程访问同一个变量时，若某个线程修改了变量的值，其他线程能够立即看得到修改的值。 例如下面的代码块：1234567i=0;//共享变量//线程1的操作i=i+1//线程2的操作j=i 我们假设先执行的线程1，然后执行线程2，且线程1和线程2不同时执行一般会想到j=1，但事实上却不一定。 在创建线程的时候，内存结构如下： 当执行一次线程1之后，cpu缓存中i变为了1，并把cache中的1刷到了内存中，内存结构如下：由于在线程启动的时候已经把i的值读到了cpu的cache中，所以在执行j=i的时候，给j赋的值是0，而不是1。 所以上述的并发设计是不具备可见性的，因为在线程1修改了i的时候，线程2并没有立刻就能读到。面对这个问题，在Java中，我们可以使用volatile关键字声明变量不使用CPU缓存，这样就能保证了并发的可见性。如下分析： 第一步：线程1直接读取内存的i值 第二步：执行i=i+1，并把结果刷回内存，CPU2中的缓存失效，然后CPU2更新缓存，把i值赋值给j，并写回内存 这样就保证了i的操作是具备可见性的了，所以线程1修改了i之后，线程2能立刻读到修改后的值。 由于线程1修改了变量i的值之后，会立马把值刷到内存中，并使其它CPU中i的缓存失效，这样就能保证变量值修改后，其它线程能立刻读到最新的值。 有序性有序性指的是程序执行时按照代码的先后顺序执行。 happen-before如果A happen-before B，那么A的所有操作完成后并产生结果才会执行B操作，可以说A所做的任何操作对B都是可见的。 1.程序次序规则：在一个单独的线程中，按照程序代码的执行流顺序，（时间上）先执行的操作happen—before（时间上）后执行的操作； 2.管理锁定规则：一个unlock操作happen—before后面（时间上的先后顺序，下同）对同一个锁的lock操作； 3.volatile变量规则：对一个volatile变量的写操作happen—before后面对该变量的读操作。 4.线程启动规则：Thread对象的start（）方法happen—before此线程的每一个动作； 5.线程终止规则：线程的所有操作都happen—before对此线程的终止检测，可以通过Thread.join（）方法结束、Thread.isAlive（）的返回值等手段检测到线程已经终止执行； 6.线程中断规则：对线程interrupt（）方法的调用happen—before发生于被中断线程的代码检测到中断时事件的发生； 7.对象终结规则：一个对象的初始化完成（构造函数执行结束）happen—before它的finalize（）方法的开始； 8.传递性：如果操作A happen—before操作B，操作B happen—before操作C，那么可以得出A happen—before操作C。 指令重排在Java中，为了提高执行效率、CPU的利用率，在执行字节码文件的时候会发生指令重排操作。我们观察下面这段代码：1234int a = 1; //语句1int b = 2; //语句2a += 1; //语句3b += 1; //语句4 假设每行代码执行需要耗时1ms(实际上执行实际会比1ms短很多很多)，那么这四条语句会执行4ms时间，他们的执行顺序是：1234第1步：int a = 1; 第2步：int b = 2; 第3步：a += 1; 第4步：b += 1; 我们可以观察出这四行语句有如下的依赖关系：语句3依赖语句1、语句4依赖语句2。 其实在Java中，JVM在执行时会发生指令重排，语句1和语句2谁先执行是说不定的，语句3和语句4谁先执行也是说不定的。但一定是语句1执行完之后才执行语句3，语句2执行完之后才执行语句4，这样就可以把语句1和语句3放在一个cpu中，语句2和语句4放在一个cpu，两个cpu并行执行。这样的话，执行的顺序可能就是：12第1步：int a = 1; a += 1; 第2步：int b = 2; b += 1; 这样操作完只需要花费2ms，相比于之前的4ms，时间节省了一倍！ 虽然有序性在一定程度上提高了效率，但是在实际开发中也会带来一些灾难，例如下面的这段代码： 123456789//线程1:context = loadContext(); //语句1inited = true; //语句2 //线程2:while(!inited )&#123; sleep()&#125;doSomethingwithconfig(context); 由于语句2可能会在语句1之前运行，所以可能会在context还没有被初始化的时候，就开始执行doSomethingwithconfig(context)了。在Java中可以使用volatile关键字来对inited进行修饰，从而保证语句1 happen-before 语句2。 总结原子性保证了某（多）个操作在执行的时候不会被打断；可见性保证了某个值被修改后在其它线程中可以立马看到，不会读到久值；有序性保证了某些操作happen-before另外一些操作。","categories":[{"name":"java","slug":"java","permalink":"http://www.leocook.org/blog/categories/java/"}],"tags":[{"name":"原子性 可见性 有序性 volatile happen-before","slug":"原子性-可见性-有序性-volatile-happen-before","permalink":"http://www.leocook.org/blog/tags/原子性-可见性-有序性-volatile-happen-before/"}]},{"title":"JCF（Java集合框架）概括","slug":"java-2017-06-05-JCF（Java集合框架）概括","date":"2017-06-04T16:00:00.000Z","updated":"2019-06-09T15:36:09.199Z","comments":true,"path":"2017/06/05/java-2017-06-05-JCF（Java集合框架）概括/","link":"","permalink":"http://www.leocook.org/2017/06/05/java-2017-06-05-JCF（Java集合框架）概括/","excerpt":"在Java中，集合也就是可以装载多个Java对象的某种对象，所以Java的集合只能装载对象，在装载基础数据类型的时候，事实上装载的是基础类型锁对应的包装类。 在学习JCF之前，我们先回顾一下，在计算机编程中我们期望集合能有哪些能力？ 可以遍历集合全部的元素 关于集合类部元素的顺序 保留写入集合的顺序 可根据元素的大小自动排序 能快速查找出集合中的某个元素 可以根据位置来快速的修改元素 能高效的增加、删除集合中元素 集合内的元素可以快速去重 聊到集合，那肯定少不了下面几种基础的数据结构： 线性存储 （可变）数组 链表 Hash散列表 平衡树","text":"在Java中，集合也就是可以装载多个Java对象的某种对象，所以Java的集合只能装载对象，在装载基础数据类型的时候，事实上装载的是基础类型锁对应的包装类。 在学习JCF之前，我们先回顾一下，在计算机编程中我们期望集合能有哪些能力？ 可以遍历集合全部的元素 关于集合类部元素的顺序 保留写入集合的顺序 可根据元素的大小自动排序 能快速查找出集合中的某个元素 可以根据位置来快速的修改元素 能高效的增加、删除集合中元素 集合内的元素可以快速去重 聊到集合，那肯定少不了下面几种基础的数据结构： 线性存储 （可变）数组 链表 Hash散列表 平衡树 1.概述在Java2.0之前，只有一些简单的集合，例如Vector/Stack/Hashtable，由于这些集合直接使用synchronized关键字来实现线程安全，使得这些集合在使用的时候效率极低。从Java2.0开始之后，Java提供了一系列的Java Collections Framework（JCF）。 下面开始讲讲Java的集合框架（后面简称JCF）。JCF主要包括了两种类型的集合：Collection和Map。Collection集合中的每个节点存放的是一个元素。Map集合中的每个元素存放的是型的键值对。 2.Iterator接口这是迭代器接口，不同集合的Iterator实现会不一样。例如ArrayList的Iterator实现是内部类Itr。集合实现了Iterator之后，就可以使用迭代器顺序遍历了。Iterator接口的定义如下： 12345Object next()：返回迭代器刚越过的元素的引用，返回值是Object，需要强制转换成自己需要的类型boolean hasNext()：判断容器内是否还有可供访问的元素void remove()：删除迭代器刚越过的元素 查看源码的话，会看到下面这一段代码： 123456......* @see Collection* @see ListIterator* @see Iterablepublic interface Iterator&lt;E&gt; &#123;...... Iterable 所有实现了Iterable接口的集合，都可以使用增强的for循环，因为这个集合将会实现一个自己的Iterator。 ListIterator ListIterator是针对List实现的迭代器。由于List是顺序存储结构，所以除了next()、hasNext()、remove()方法，ListIterator还提供了List下标处理的相关方法，例如获取上/下一个元素以及它们的下标情况。 3.CollectionCollection是Java集合层次结构中的根节点，实现Collection的集合中，有的是允许存放重复的元素，有的不允许有重复的元素（set），有的集合是有序的，有的则是无序的。Collection下面有三种子接口，分别是List、Set、Queue，下面逐一介绍。 3.1.List实现List接口的集合有着两大特性：允许元素重复、元素是有序的。实现了List接口的集合，可以通过位置来操作集合的元素。下图展示了List接口中的全部方法： 下面是几个常用的List类型集合。 ArrayList 基于数组实现的线性存储集合。数组的大小可以变化，当元素个数超过了数组的大小，将会重新创建一个更大长度的数组，并把当前数组中的内容复制进去，每次长度增长为原来的1.5倍左右，可以查看源码： 1234567891011private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125; LinkedList 基于链表实现的线性存储集合。下面对比ArrayList和LinkedList： 操作 ArrayList LinkedList 内部数据结构 数组 链表 是否顺序结构 是 是 位置检索 快 慢 增、删 慢 快 3.2.Set实现Set接口的集合内部元素不重复，Set有三个具体的实现类：HashSet（散列集）、LinkedHashSet（顺序集）、TreeSet（平衡树）。 HashSet 基于hash的无序set。其实HashSet内部是用HashMap实现的，HashMap后边会说到。在散列集中，有两个名词需要关注，初始容量和客座率。客座率是确定在增加规则集之前，该规则集的饱满程度，当元素个数超过了容量与客座率的乘积时，容量就会自动翻倍。 LinkedHashSet 基于链表实现的HashSet，LinkedHashSet中的元素是有序的，且顺序和写入的顺序一致。 TreeSet TreeSet是一个有序的Set，排序的比较器可以通过传入Comparator来自定义。 3.3.Queue队列是一种FIFO（First in first out）数据结构，元素在Queue的末尾添加，在头部删除。Queue接口分别定义了上面6个方法，分别有插入、移除和检查的功能，有的方法在某些特殊情况下回报错，有的则不会，具体见下表： Operation 抛出异常 返回特殊值 操作失败的条件 Insert（插入） add(e) offer(e),return false 当队列空间有限制，且没有多余的空间时 Remove（移除） remove() poll(),return null 队列为空 Examine（检查） element() peek(),return null 队列为空 Queue平时用的不是很多，优先队列PriorityQueue有的时候会用到。 4.MapMap是存储键值对映射（key,value）的容器类，可以存储任意类型的对象。但是key不能重复，且一个key只能对应一个值。如果使用对象作为key,那么必须要考虑一下该对象类的hashCode方法和equals方法是否需要重写，因为map是用着两个方法来判断key是否相等的，其中hashCode是用来加速判断的，如果hashCode相等，还会用equals方法来判断。 常用到的Map接口实现类，有三个：HashMap、LinkedHashMap、TreeMap。Map接口定义的方法列表可见下图： 4.1.HashMapHashMap是基于数组+链表实现的Hash散列Map结构，数组里存放着索引，链表里存放的是元素数据。(key,value)键值对中，key的hash值就是数组的下标。 在Jdk1.8中，对HashMap做了优化，当链表的长度超过8时，链表结构将会变为了平衡树，这样做主要是为了在HashMap里的元素较多时，能够加快查找的速度。 关于HashMap，其实有很多可以细聊的，它与很多集合都有着关系。后边会再来一篇，详细说一下Java中HashMap的设计，以及Java中散列存储的设计思想。 4.2.LinkedHashMapLinkedHashMap类继承了HashMap类，HashMap中的元素是没有顺序的，但是LinkedHashMap中的元素是有顺序的。LinkedHashMap里的元素顺序有两种排序方式：第一种是根据元素key被插入的顺序;第二种是根据元素被访问的顺序来排序（最近最少被访问的元素优先）。 4.3.TreeMapTreeMap是基于红黑树结构来实现的，可以使用Comparable或Comparator接口来实现排序的比较算法。 在日常开发中，如果不用考虑键值对元素的顺序，就使用HashMap；如果需要考虑元素插入顺序，就使用LinkedHashMap；如果需要根据Key自定义排序规则，那么就使用TreeMap。 5.其它集合下面简单说几个不是很常用的集合。 5.1.VectorVector的使用和ArrayList的使用基本一样，它是线程安全的线程安全，但是Vector的线程安全是使用关键字synchronized修饰实现的，所以Vector的效率很低。相对来说，ArrayList更加高效！ 5.2.StackStack是Java2之前设计的栈结构，内部是使用数组实现的。我们知道栈结构的操作中会频繁的出现入栈和出栈，那么使用数组结构的话，在栈结构变长时，自然会带来性能上的折扣。LinkedList也具备栈的功能，而且是基于链表实现的，所以在开发需要用到栈结构时，推荐使用LinkedList。 5.3.HashTableHashTable的功能和HashMap相似，它是Dictionary类的，并且使用了synchronized关键字实现了线程安全，所以性能会很差！除此之外，HashTable的Key不能为空指针null,但是HashMap的key可以为null。 关于集合的线程安全，后边会另写一篇详细说明。 参考地址：http://www.open-open.com/lib/view/open1474167415464.htmlhttp://www.cnblogs.com/CarpenterLee/p/5414253.html 6.总结 JCF接口架构 常用的集合分类情况 接口功能实现 下面表格描述的比较好，每个接口对应不同数据结构的实现。 参考文档：http://docs.oracle.com/javase/6/docs/technotes/guides/collections/overview.htmlhttp://www.jianshu.com/p/63e76826e852","categories":[{"name":"java","slug":"java","permalink":"http://www.leocook.org/blog/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://www.leocook.org/blog/tags/java/"}]},{"title":"SciPy生态系统初探","slug":"python-2017-05-10-SciPy生态系统初探","date":"2017-05-09T16:00:00.000Z","updated":"2019-06-09T15:37:14.049Z","comments":true,"path":"2017/05/10/python-2017-05-10-SciPy生态系统初探/","link":"","permalink":"http://www.leocook.org/2017/05/10/python-2017-05-10-SciPy生态系统初探/","excerpt":"SciPy是一个基于Python的开源生态系统，也称为SciPy技术栈，主要为数学计算、科学研究以及工程计算提供服务。 Anaconda则是SciPy技术栈的发行版，它解决了SciPy技术栈各个组件之间的兼容、以及管理工作。","text":"SciPy是一个基于Python的开源生态系统，也称为SciPy技术栈，主要为数学计算、科学研究以及工程计算提供服务。 Anaconda则是SciPy技术栈的发行版，它解决了SciPy技术栈各个组件之间的兼容、以及管理工作。 下面是SciPy生态圈中一些常用的包： NumPy NumPy是Python科学计算中的一个比较基础的包，它主要有下面几项功能： a.强大的矩阵计算能力 b.用于整合C/C++和Fortran代码的工具包 c.比较成熟的（广播）函数库 d.支持线性代数求解、傅里叶变换，以及随机数处理 SciPy library SciPy是组成SciPy技术栈的核心包之一，它的相关API使用起来都比较友好，且执行效率高。 Matplotlib Matplotlib是一个基于Python的2D绘图库，能绘制出出版社级别的高质量图。 IPython 一个功能强大的Python命令行，也是Jupyter的内核。关于Jupyter，被人们广为熟知的有Jupyter Notebook，之前的名字叫IPython Notebook，是一个交互式笔记本，可以运行几十种语言。在Jupyter Notebook中，代码可以实时的生成图像、视频等。 Sympy 基于Python的符号计算包。 pandas pandas提供一些高效、易用的数据结构，以及数据分析工具。 1.SciPy技术栈的安装直接使用pip安装1pip install --user numpy scipy matplotlib ipython jupyter pandas sympy nose 2.NumPy&amp;scipy测试1234567891011121314151617181920212223&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; a = np.arange(15).reshape(3, 5)&gt;&gt;&gt; aarray([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]])&gt;&gt;&gt; a.shape(3, 5)&gt;&gt;&gt; a.ndim2&gt;&gt;&gt; a.dtype.name&apos;int64&apos;&gt;&gt;&gt; a.itemsize8&gt;&gt;&gt; a.size15&gt;&gt;&gt; type(a)&lt;type &apos;numpy.ndarray&apos;&gt;&gt;&gt;&gt; b = np.array([6, 7, 8])&gt;&gt;&gt; barray([6, 7, 8])&gt;&gt;&gt; type(b)&lt;type &apos;numpy.ndarray&apos;&gt; 12345$ python&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; np.test(&apos;full&apos;)&gt;&gt;&gt; import scipy&gt;&gt;&gt; scipy.test() 2.Matplotlib测试关于matplotlib的例子，在http://matplotlib.org/examples/index.html这里有很多，下面随机选了两个运行一下作为例子让大家看下。 animate_decay 1python animate_decay.py dynamic_image 1python dynamic_image.py 3.IPython直接使用命令ipython进入 4.关于SciPy技术栈的发行版我们知道SciPy技术栈内有很多技术组件，那么不同的组件配合使用时，肯定会有兼容的问题。我们期望能有个工具，它可以帮助我们管理SciPy技术栈的各个组件，保证他们的兼容性，同事也方便我们安装管理。它就是Anaconda。 可以在这里选择自己的操作系统环境https://www.continuum.io/downloads.笔者使用的是macOS，在安装了Anaconda之后，系统的Python环境都变为了Anaconda安装的python了： 1234567891011leocookMacBook-Pro:~ leocook$ which python/Users/wulin/anaconda/bin/pythonleocookMacBook-Pro:~ leocook$ whereis python/usr/bin/pythonleocookMacBook-Pro:~ leocook$ pythonPython 2.7.13 |Anaconda 4.3.1 (x86_64)| (default, Dec 20 2016, 23:05:08)[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwinType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.Anaconda is brought to you by Continuum Analytics.Please check out: http://continuum.io/thanks and https://anaconda.org&gt;&gt;&gt; 可以使用Anaconda命令查看我们安装了SciPy技术栈中的哪些组件： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213leocookMacBook-Pro:~ leocook$ conda list# packages in environment at /Users/leocook/anaconda:#_license 1.1 py27_1alabaster 0.7.9 py27_0anaconda 4.3.1 np111py27_0anaconda-client 1.6.0 py27_0anaconda-navigator 1.5.0 py27_0anaconda-project 0.4.1 py27_0appdirs 1.4.3 &lt;pip&gt;appnope 0.1.0 py27_0appscript 1.0.1 py27_0argcomplete 1.0.0 py27_1astroid 1.4.9 py27_0astropy 1.3 np111py27_0babel 2.3.4 py27_0backports 1.0 py27_0backports_abc 0.5 py27_0beautifulsoup4 4.5.3 py27_0bitarray 0.8.1 py27_0blaze 0.10.1 py27_0bokeh 0.12.4 py27_0boto 2.45.0 py27_0bottleneck 1.2.0 np111py27_0cdecimal 2.3 py27_2cffi 1.9.1 py27_0chardet 2.3.0 py27_0chest 0.2.3 py27_0click 6.7 py27_0cloudpickle 0.2.2 py27_0clyent 1.2.2 py27_0colorama 0.3.7 py27_0conda 4.3.17 py27_0conda-env 2.6.0 0configobj 5.0.6 py27_0configparser 3.5.0 py27_0contextlib2 0.5.4 py27_0cryptography 1.7.1 py27_0curl 7.52.1 0cycler 0.10.0 py27_0cython 0.25.2 py27_0cytoolz 0.8.2 py27_0dask 0.13.0 py27_0datashape 0.5.4 py27_0decorator 4.0.11 py27_0dill 0.2.5 py27_0docutils 0.13.1 py27_0entrypoints 0.2.2 py27_0enum34 1.1.6 py27_0et_xmlfile 1.0.1 py27_0fastcache 1.0.2 py27_1flask 0.12 py27_0flask-cors 3.0.2 py27_0freetype 2.5.5 2funcsigs 1.0.2 &lt;pip&gt;funcsigs 1.0.2 py27_0functools32 3.2.3.2 py27_0futures 3.0.5 py27_0get_terminal_size 1.0.0 py27_0gevent 1.2.1 py27_0greenlet 0.4.11 py27_0grin 1.2.1 py27_3h5py 2.6.0 np111py27_2hdf5 1.8.17 1heapdict 1.0.0 py27_1icu 54.1 0idna 2.2 py27_0imagesize 0.7.1 py27_0ipaddress 1.0.18 py27_0ipykernel 4.5.2 py27_0ipython 5.1.0 py27_1ipython_genutils 0.1.0 py27_0ipywidgets 5.2.2 py27_1isort 4.2.5 py27_0itsdangerous 0.24 py27_0jbig 2.1 0jdcal 1.3 py27_0jedi 0.9.0 py27_1jinja2 2.9.4 py27_0jpeg 9b 0jsonschema 2.5.1 py27_0jupyter 1.0.0 py27_3jupyter_client 4.4.0 py27_0jupyter_console 5.0.0 py27_0jupyter_core 4.2.1 py27_0Keras 2.0.4 &lt;pip&gt;lazy-object-proxy 1.2.2 py27_0libgpuarray 0.6.4 0libiconv 1.14 0libpng 1.6.27 0libtiff 4.0.6 3libxml2 2.9.4 0libxslt 1.1.29 0llvmlite 0.15.0 py27_0locket 0.2.0 py27_1lxml 3.7.2 py27_0mako 1.0.6 py27_0markupsafe 0.23 py27_2matplotlib 2.0.0 np111py27_0mistune 0.7.3 py27_1mkl 2017.0.1 0mkl-service 1.1.2 py27_3mock 2.0.0 &lt;pip&gt;mpmath 0.19 py27_1multipledispatch 0.4.9 py27_0nbconvert 4.2.0 py27_0nbformat 4.2.0 py27_0networkx 1.11 py27_0nltk 3.2.2 py27_0nose 1.3.7 py27_1notebook 4.3.1 py27_0numba 0.30.1 np111py27_0numexpr 2.6.1 np111py27_2numpy 1.12.1 &lt;pip&gt;numpy 1.11.3 py27_0numpydoc 0.6.0 py27_0odo 0.5.0 py27_1openpyxl 2.4.1 py27_0openssl 1.0.2k 1packaging 16.8 &lt;pip&gt;pandas 0.19.2 np111py27_1partd 0.3.7 py27_0path.py 10.0 py27_0pathlib2 2.2.0 py27_0patsy 0.4.1 py27_0pbr 3.0.0 &lt;pip&gt;pep8 1.7.0 py27_0pexpect 4.2.1 py27_0pickleshare 0.7.4 py27_0pillow 4.0.0 py27_0pip 9.0.1 py27_1ply 3.9 py27_0prompt_toolkit 1.0.9 py27_0protobuf 3.3.0 &lt;pip&gt;psutil 5.0.1 py27_0ptyprocess 0.5.1 py27_0py 1.4.32 py27_0pyasn1 0.1.9 py27_0pyaudio 0.2.7 py27_0pycosat 0.6.1 py27_1pycparser 2.17 py27_0pycrypto 2.6.1 py27_4pycurl 7.43.0 py27_2pyflakes 1.5.0 py27_0pygments 2.1.3 py27_0pygpu 0.6.4 py27_0pylint 1.6.4 py27_1pyopenssl 16.2.0 py27_0pyparsing 2.2.0 &lt;pip&gt;pyparsing 2.1.4 py27_0pyqt 5.6.0 py27_1pytables 3.3.0 np111py27_0pytest 3.0.5 py27_0python 2.7.13 0python-dateutil 2.6.0 py27_0python.app 1.2 py27_4pytz 2016.10 py27_0pyyaml 3.12 py27_0pyzmq 16.0.2 py27_0qt 5.6.2 0qtawesome 0.4.3 py27_0qtconsole 4.2.1 py27_1qtpy 1.2.1 py27_0readline 6.2 2redis 3.2.0 0redis-py 2.10.5 py27_0requests 2.12.4 py27_0rope 0.9.4 py27_1ruamel_yaml 0.11.14 py27_1scandir 1.4 py27_0scikit-image 0.12.3 np111py27_1scikit-learn 0.18.1 np111py27_1scipy 0.18.1 np111py27_1seaborn 0.7.1 py27_0setuptools 27.2.0 py27_0setuptools 35.0.2 &lt;pip&gt;simplegeneric 0.8.1 py27_1singledispatch 3.4.0.3 py27_0sip 4.18 py27_0six 1.10.0 py27_0six 1.10.0 &lt;pip&gt;snowballstemmer 1.2.1 py27_0sockjs-tornado 1.0.3 py27_0sphinx 1.5.1 py27_0spyder 3.1.2 py27_0sqlalchemy 1.1.5 py27_0sqlite 3.13.0 0ssl_match_hostname 3.4.0.2 py27_1statsmodels 0.6.1 np111py27_1subprocess32 3.2.7 py27_0sympy 1.0 py27_0tensorflow 1.1.0 &lt;pip&gt;terminado 0.6 py27_0theano 0.9.0 py27_0tk 8.5.18 0toolz 0.8.2 py27_0tornado 4.4.2 py27_0traitlets 4.3.1 py27_0unicodecsv 0.14.1 py27_0wcwidth 0.1.7 py27_0Werkzeug 0.12.1 &lt;pip&gt;werkzeug 0.11.15 py27_0wheel 0.29.0 &lt;pip&gt;wheel 0.29.0 py27_0widgetsnbextension 1.2.6 py27_0wrapt 1.10.8 py27_0xlrd 1.0.0 py27_0xlsxwriter 0.9.6 py27_0xlwings 0.10.2 py27_0xlwt 1.2.0 py27_0xz 5.2.2 1yaml 0.1.6 0zlib 1.2.8 3 5.使用Anaconda管理软件包5.1. 安装TheanoTheano是一个基于Python的深度学习库。默认Theano是没有被安装的，那么我们下面使用Anaconda来安装它。首先我们使用命令来查看一下Theano包的情况： 1leocookMacBook-Pro:~ leocook$ conda search theano 然后得到：12345678Fetching package metadata .........theano 0.8.2 py35_0 defaults 0.8.2 py34_0 defaults 0.8.2 py27_0 defaults 0.9.0 py35_0 defaults 0.9.0 py36_0 defaults * 0.9.0 py27_0 defaults 我们可以看到有多个版面，然后我们直接运行下面的命令： 1conda install theano 它会选择和我们py版本兼容的最新版本0.9.0 py27_0. 5.2 安装TensorflowTensorflow是Google开源的一个人工智能学习系统。我们使用conda search tensorflow命令查看发现Anaconda库中没有Tensorflow的包。 其实，我们也可以用Anaconda来管理那些使用其他方式安装的包，例如使用pip安装的包。下面简单说明一下使用Anaconda管理使用pip安装的Tensorflow。 创建一个名称为tensorflow的conda环境 1conda create -n tensorflow 激活conda环境 1source activate tensorflow 激活环境后，在这里执行命令安装的程序，都会被Anaconda管理。 使用pip来安装TensorFlow 在这里https://www.tensorflow.org/install/install_mac#the_url_of_the_tensorflow_python_package找到对应版本的URL，也就是下面命令中将会使用到的$TF_PYTHON_URL. 1(tensorflow)$ pip install --ignore-installed --upgrade $TF_PYTHON_URL 例如笔者的是：1pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.1.0-py2-none-any.whl 安装完成后，退出tensorflow这个Anaconda环境： 1source deactivate tensorflow","categories":[{"name":"python SciPy","slug":"python-SciPy","permalink":"http://www.leocook.org/blog/categories/python-SciPy/"}],"tags":[{"name":"SciPy Anaconda  NumPy Matplotlib IPython Sympy pandas Tensorflow Theano","slug":"SciPy-Anaconda-NumPy-Matplotlib-IPython-Sympy-pandas-Tensorflow-Theano","permalink":"http://www.leocook.org/blog/tags/SciPy-Anaconda-NumPy-Matplotlib-IPython-Sympy-pandas-Tensorflow-Theano/"}]},{"title":"JVM内存模型基础","slug":"java-2017-04-05-JVM内存模型基础","date":"2017-04-04T16:00:00.000Z","updated":"2019-06-09T15:36:31.366Z","comments":true,"path":"2017/04/05/java-2017-04-05-JVM内存模型基础/","link":"","permalink":"http://www.leocook.org/2017/04/05/java-2017-04-05-JVM内存模型基础/","excerpt":"Java内存模型也被称为JMM，开发人员在使用时无需过多考虑因指针引起的如内存泄露和溢出问题，也不用为每个new出来的对象使用delete等方法来释放内存。JVM已经为开发人员处理好了这一切，但是由于各种原因，Java程序也会出现内存溢出等问题，如果不了解JVM的内存模型以及相关的管理策略，那么整个排查过程将会变得十分艰难。 JVM的运行时内存共有5块区域，其中有3个是线程间隔离的，有2个是线程间共享的。 线程间隔离（线程私有区） 程序计数器 JVM栈 本地方法栈 线程间共享 Java堆 方法区","text":"Java内存模型也被称为JMM，开发人员在使用时无需过多考虑因指针引起的如内存泄露和溢出问题，也不用为每个new出来的对象使用delete等方法来释放内存。JVM已经为开发人员处理好了这一切，但是由于各种原因，Java程序也会出现内存溢出等问题，如果不了解JVM的内存模型以及相关的管理策略，那么整个排查过程将会变得十分艰难。 JVM的运行时内存共有5块区域，其中有3个是线程间隔离的，有2个是线程间共享的。 线程间隔离（线程私有区） 程序计数器 JVM栈 本地方法栈 线程间共享 Java堆 方法区 1.程序计数器它是线程间隔离的。字节码的行号指示器，用来标记当前线程所执行的字节码的行号。 Q1. 为什么每个线程都需要一个独立的计数器呢？因为JVM的多线程是轮询在CPU上执行的，任何一个时间点，一个处理器上最多只有一条指定在执行。为保证线程切换后，能恢复从正确的执行位置执行指令，所以给每个线程都独立的使用一块空间作为程序计数器，使得多线程之间计数器互不影响。 2.JVM栈它是线程间隔离的，也就是常说到的“栈内存”。每个方法执行的时候都会创建一个栈桢，用来存放局部变量表用来存放该方法内使用到的局部变量表、操作数栈、动态链接、方法出口灯信息。 一个方法从调用到执行完成的过程也就对应着一个栈桢在JVM栈中入栈到出栈的过程。局部变量表中存放了编译器可确定的各种基本数据类型（例如：boolean、byte、char、short等等），以及对象的引用。 使用-Xss参数设置栈容量 例： -Xss128k JVM规范规定该区域会抛出两种异常： StackOverFlowError：当线程请求栈深度超出虚拟机栈所允许的深度时,抛出该异常 OutOfMemoryError：当Java虚拟机扩展到无法申请足够内存时,抛出该异常 3.本地方法栈它是线程间隔离的，和“JVM栈”的区别是：“JVM栈”是执行Java方法时所使用到的内存；“本地方法栈”是JVM执行Native方法时所使用到的内存存。这一块内存在JVM规范中没有强制指定，所以不同的虚拟机实现它的方法可能不一样。Sum HotSpot虚拟机是把“JVM栈”和“本地方法栈”合二为一的。 JVM规范规定该区域可抛出StackOverFlowError和OutOfMemoryError异常。 4.Java堆它是线程间共享的，存放JVM对象实例，也是GC发生的主要区域，所以也被称为GC堆。关于GC算法垃圾回收器，后边会介绍。 JVM规范规定该区域可抛出OutOfMemoryError异常。 -Xms参数设置最小值，-Xmx参数设置最大值，可以指定参数-XX:+HeapDumpOnOutOfMemoryError，使得JVM在出现OutOfMemoryError异常异常时能够dump出Java堆的快照。 5.方法区它是线程间共享的，存放已被JVM加载的类信息、常量、静态变量、即时编译器编译后的代码等。它还有个别名叫做Non-Heap（非堆）。 -XX:MaxPermSize可以设置方法区的最大值，-XX:PermSize设置方法区的最小值。 JVM规范规定该区域可抛出OutOfMemoryError异常。 5.1.运行时常量池它是方法区的一部分，存放编译器生成的各种编译期生成的字面量和符号引用。 HotSpot虚拟机HotSpot虚拟机中有两个比较特别的地方： 本地方法栈和虚拟机栈合二为一了； 在分代垃圾回收器中，把方法区称为永久代，但它们不是等价的。这里准确的说是使用永久代来实现了方法区。","categories":[{"name":"java","slug":"java","permalink":"http://www.leocook.org/blog/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://www.leocook.org/blog/tags/jvm/"}]},{"title":"2016书单","slug":"book-2016-12-31-2016书单","date":"2016-12-29T16:00:00.000Z","updated":"2019-06-09T15:35:45.688Z","comments":true,"path":"2016/12/30/book-2016-12-31-2016书单/","link":"","permalink":"http://www.leocook.org/2016/12/30/book-2016-12-31-2016书单/","excerpt":"2016年看的书主要还是以技术类的比价多，意料之外的是看完了《西游记》。","text":"2016年看的书主要还是以技术类的比价多，意料之外的是看完了《西游记》。 1.《西游记》和86版的电视剧差别挺大，在北京时，主要是坐地铁的时候看完的。 图书地址：https://www.amazon.cn/dp/B00C4PGGUW 2.《Spark最佳实践》同类目书籍中，比较偏向实战，推荐。 图书地址：https://item.jd.com/11923673.html 3.《Spark快速大数据分析》spark入门级别的书，写得比较细致，也可以作为手册查询，内容基本上都是官网翻译过来的。 图书地址：https://item.jd.com/11782888.html","categories":[{"name":"书单","slug":"书单","permalink":"http://www.leocook.org/blog/categories/书单/"}],"tags":[{"name":"书单","slug":"书单","permalink":"http://www.leocook.org/blog/tags/书单/"}]},{"title":"spark内存管理","slug":"spark-2016-10-13-spark内存管理","date":"2016-10-12T16:00:00.000Z","updated":"2019-06-09T15:37:21.570Z","comments":true,"path":"2016/10/13/spark-2016-10-13-spark内存管理/","link":"","permalink":"http://www.leocook.org/2016/10/13/spark-2016-10-13-spark内存管理/","excerpt":"Spark的最大卖点就是内存迭代运算，相对于传统MapReduce的磁盘迭代运算，spark的迭代运算要快得多。作为内存迭代运算的spark，掌握它的内存管理是很有必要的。 Spark的内存可以大体上分为三大块：Reserved Memory（预留内存）、User Memory（用户内存），以及Spark Memory（Spark内存）。Spark Memory又包含Storage Memory和Execution Memory这两大块，1.6版本之前他们是不能共享的，从1.6版本(如下图)开始它们就可以共享了。而本文介绍的就是1.6版本开始之后内存管理机制。","text":"Spark的最大卖点就是内存迭代运算，相对于传统MapReduce的磁盘迭代运算，spark的迭代运算要快得多。作为内存迭代运算的spark，掌握它的内存管理是很有必要的。 Spark的内存可以大体上分为三大块：Reserved Memory（预留内存）、User Memory（用户内存），以及Spark Memory（Spark内存）。Spark Memory又包含Storage Memory和Execution Memory这两大块，1.6版本之前他们是不能共享的，从1.6版本(如下图)开始它们就可以共享了。而本文介绍的就是1.6版本开始之后内存管理机制。 本文在描述spark各个部分内存的时候，大概从三个方面介绍：概念描述、该内存参数怎么配置，内存不够的时候会发生什么情况。 1.Reserved Memory系统预留内存，用于存储Spark内部对象。它的大小是300MB，不能通过参数修改，如果真的需要修改，需要重新编译（spark.testing.reservedMemory参数可以使用，但是不推荐在线上环境中使用）。 当executor分配的内存小于1.5*Reserved Memory的时候，将会报“please use larger heap size”错误。 2.User Memory用户内存，用于存储RDD转换操作所需要的数据，例如RDD依赖等信息。这个内存大小为(“Java Heap” – “Reserved Memory”) (1.0 – spark.memory.fraction)，默认是(“Java Heap” – 300MB) 0.25。如果用户使用的内存大于这个值，将会导致OOM。 3.Spark Memory这部分内存是归Spark自身管理的，大小为(“Java Heap” – “Reserved Memory”) spark.memory.fraction，默认为(“Java Heap” – 300MB) 0.75。Spark Memory又被分为Storage Memory和Execution Memory，下面详细说明。 3.1.Storage Memory用来存储spark的cache数据，例如RDD的缓存、unroll数据。当缓存数据的持久化level达到一定的时候，spark将会把它存到磁盘中，例如广播变量的数据持久化级别都是“MEMORY_AND_DISK”，所以所有的广播变量数据大小达到一定量的时候，都会存到磁盘中的。默认大小是Spark Memory的0.5，可用过参数spark.memory.storageFraction来配置（spark.memory.storageFraction=0.5）。 Unroll Memory “Unroll Memory”是”Storage Memory”的一部分，在Spark中数据可以以序列化和反序列化的形式存储，序列化后的数据是无法直接被访问的，只有反序列化后才能被使用，反序列化过程中用到的RAM就是Unroll Memory。Spark中的大部分数据都是以序列化的形式传输的。 3.2.Execution Memory用于存储spark的buffer部分，例如task运行过程中产生的一些对象，shuffle过程中map的输出，在内存不够的时候，支持写到磁盘上。Spark Memory中能被应用使用的内存中，除了Storage Memory剩余内存都是Execution Memory的了。 Shuffle Memory shuffle阶段使用的内存，主要是使用在sort上。如果这一块没有足够的内存来用作shuffle，将会内存溢出失败。当然，在内存不足的时候，也可以使用spark的外部排序（spark.shuffle.spill=true），但是性能将会有些折扣。 3.3.Storage Memory和Execution Memory共享规则 一方空闲，一方内存不足的时候，内存不足的一方可以借用另一方的内存。 Storage Memory占用了Execution Memory的内存，当Execution Memory内存不够用时 强制释放Storage Memory中属于Execution Memory的那部分内存，释放后的内存被Execution Memory使用。Storage Memory丢失的数据在下次使用的时候会被重新计算。 Execution Memory占用了Storage Memory的内存，当Storage Memory内存不够用时 不强制释放Execution Memory中属于Storage Memory的那部分内存，Storage Memory会一直等待，直到Execution Memory主动释放属于Storage Memory的那部分内存。因为强制释放Execution Memory会导致任务失败。 （PS：感觉Execution Memory在欺负Storage Memory，有木有~~~）","categories":[{"name":"spark","slug":"spark","permalink":"http://www.leocook.org/blog/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://www.leocook.org/blog/tags/spark/"}]},{"title":"spark streaming最佳实践-概述","slug":"spark-2016-10-12-spark-streaming最佳实践-概述","date":"2016-10-11T16:00:00.000Z","updated":"2019-06-09T15:37:29.058Z","comments":true,"path":"2016/10/12/spark-2016-10-12-spark-streaming最佳实践-概述/","link":"","permalink":"http://www.leocook.org/2016/10/12/spark-2016-10-12-spark-streaming最佳实践-概述/","excerpt":"Spark streaming是基于Spark core的，天然的具备易扩展、高吞吐量，以及自动容错等特性。支持的主流数据源有Kafka、Flume、HDFS、Twitter、TCP socket等等，Spark的数据输出在spark streaming中都支持，对于有spark基础的开发人员来说，开发spark streaming应用成本将会少很多。本文是一篇概述型的文章，相关详细的配置会在后边逐渐补上。 Spark streaming在企业实战中经常回到这么几类问题：内存溢出、外部系统连接数过多、分配的资源超过了程序所需要的资源，造成资源浪费。","text":"Spark streaming是基于Spark core的，天然的具备易扩展、高吞吐量，以及自动容错等特性。支持的主流数据源有Kafka、Flume、HDFS、Twitter、TCP socket等等，Spark的数据输出在spark streaming中都支持，对于有spark基础的开发人员来说，开发spark streaming应用成本将会少很多。本文是一篇概述型的文章，相关详细的配置会在后边逐渐补上。 Spark streaming在企业实战中经常回到这么几类问题：内存溢出、外部系统连接数过多、分配的资源超过了程序所需要的资源，造成资源浪费。 1.内存溢出Spark内存大概分为了两类：Execution Memory和Storage Memory，前者主要用来做buffer的，例如joins、shuffle、sort等等；后者主要用来做cache，例如RDD的数据存储、广播变量、task结果数据等等。从1.6版本之前这两部分内存是不能共享的，从1.6开始之后这两部分内存就可以共享了。 我遇到的内存溢出问题大概有三类，通常是单个分区处理的数据量过多： a).数据倾斜引； b).数据未倾斜，分区数过少； c).某个分区中产生了一个较大的内存集合，例如大的List、Set，或者Map。 这类问题在spark core中也是经常会出现的，关于数据倾斜的问题以后会详细讲解。入手一个新的业务时，应该对该业务的数据量有个大概的预估，这样给这个应用分配多少节点，每个节点会处理多大的数据量，这样就能很好的预估出每个节点分配多少资源比较合理了。 为了避免内存溢出，可以从下面几个方向来做： a).避免数据倾斜； b).评估每个分区的数据量，给每个分区分配合理的资源； c).控制好List、Set等集合的大小； d).控制Spark streaming读取源数据的最大速度（spark.streaming.kafka.maxRatePerPartition），实时数据流量有高峰和低谷，不同时间处理的数据量是不一样的，为防止数据高峰的时候内存溢出，这里有必要做配置； e).配置Spark streaming可动态控制读取数据源的速度（spark.streaming.backpressure.enabled）。 2.外部系统连接数过多在使用spark streaming解决问题的时候，经常会对外部数据源进行读写操作，切记每次读写完成之后需要关闭网络连接。在我们以往的编程经验中，这些思维都是一直持有的。在往hbase写入数据的时候，如果你使用了RDD.saveAsHadoopDataset方法，就需要注意了，org.apache.hadoop.hbase.mapred.TableOutputFormat类存在bug：不能释放zookeeper连接，导致在往hbase写数据的时候，zookeeper的连接数不停得增长。 推荐使用下面这种写法： 12345678910111213141516171819202122232425import org.apache.hadoop.mapreduce.Jobimport org.apache.hadoop.hbase.mapreduce.TableOutputFormatval conf = HBaseConfiguration.create()conf.set(&quot;hbase.zookeeper.quorum&quot;, zk_hosts)conf.set(&quot;hbase.zookeeper.property.clientPort&quot;, zk_port)conf.set(TableOutputFormat.OUTPUT_TABLE, &quot;TABLE_NAME&quot;)val job = Job.getInstance(conf)job.setOutputFormatClass(classOf[TableOutputFormat[String]])formatedLines.map&#123; case (a,b, c) =&gt; &#123; val row = Bytes.toBytes(a) val put = new Put(row) put.setDurability(Durability.SKIP_WAL) put.addColumn(Bytes.toBytes(&quot;cf&quot;), Bytes.toBytes(&quot;node&quot;), Bytes.toBytes(b)) put.addColumn(Bytes.toBytes(&quot;cf&quot;), Bytes.toBytes(&quot;topic&quot;), Bytes.toBytes(c)) (new ImmutableBytesWritable(row), put) &#125;&#125;.saveAsNewAPIHadoopDataset(job.getConfiguration) 3.资源分配的浪费如果实时数据在每天的某个时间点有着平时的几倍的数据量，如果给该作业分配过多的资源，那么在绝大多数，这些资源都是闲置浪费的。这里可以启用动态资源分配。 关于配置介绍可以查看官方文档：http://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation 如果启用该配置，需要做如下配置： 1.在spark应用中配置spark.dynamicAllocation.enabled=true2.每个节点启动外部shuffle服务，并在spark应用中配置spark.shuffle.service.enabled=true 关于外部shuffle服务，在standalone、Mesos，yarn中的配置是不一样的。 standalone 启动worker的时候指定spark.shuffle.service.enabled=true Mesos 在所有节点上配置spark.shuffle.service.enabled=true，然后执行$SPARK_HOME/sbin/start-mesos-shuffle-service.sh yarn a.添加yarn的配置文件，重新编译spark。如果使用官方编译好的安装包，可以忽略这一步。b.找到spark--yarn-shuffle.jar。如果自己编译spark的话，在目录$SPARK_HOME/common/network-yarn/target/scala-下；如果是使用官方编译好的spark，在lib目录下寻找。c.添加到spark--yarn-shuffle.jar到yarn所有NodeManager的classpath下。d.配置所有NodeManager的yarn-site.xml文件如下： 123456789&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;spark_shuffle&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.spark_shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.spark.network.yarn.YarnShuffleService&lt;/value&gt;&lt;/property&gt; e.重启所有的NodeManager spark on yarn配置了外部shuffle之后，—num-executors配置将不再生效。","categories":[{"name":"spark","slug":"spark","permalink":"http://www.leocook.org/blog/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://www.leocook.org/blog/tags/spark/"}]},{"title":"Lua_on_Nginx","slug":"nginx-2016-07-26-Lua-on-Nginx","date":"2016-07-25T16:00:00.000Z","updated":"2019-06-09T15:37:08.313Z","comments":true,"path":"2016/07/26/nginx-2016-07-26-Lua-on-Nginx/","link":"","permalink":"http://www.leocook.org/2016/07/26/nginx-2016-07-26-Lua-on-Nginx/","excerpt":"Nginx的高并发是它的一大显著优势，Lua则是一门较为轻便的脚本语言。把他们组合在一起，则极大的增强了Nginx的能力（灵活性，扩展性）。Nginx-Lua模块是由淘宝开发的第三方模块，使用它可以把Lua内嵌到Nginx中。 nginx 地址：http://www.nginx.org luajit 地址：http://luajit.org/download.html HttpLuaModule 地址：http://wiki.nginx.org/HttpLuaModule","text":"Nginx的高并发是它的一大显著优势，Lua则是一门较为轻便的脚本语言。把他们组合在一起，则极大的增强了Nginx的能力（灵活性，扩展性）。Nginx-Lua模块是由淘宝开发的第三方模块，使用它可以把Lua内嵌到Nginx中。 nginx 地址：http://www.nginx.org luajit 地址：http://luajit.org/download.html HttpLuaModule 地址：http://wiki.nginx.org/HttpLuaModule 1.系统环境必须的编译环境，需要提前准备好。我这里的环境是ubuntu14.04，用的apt source是官方的源，使用163 source的时候有问题。 1234apt-get install makeapt-get install gccapt-get install libpcre3 libpcre3-devapt-get install libssl-dev 2.Lua的运行时环境配置2.1.下载Lua的运行环境12345cd /opt/wget http://luajit.org/download/LuaJIT-2.0.4.tar.gztar zxvf LuaJIT-2.0.4.tar.gz 2.2.编译安装123cd /opt/LuaJIT-2.0.4makemake install 3.下载Nginx的lua模块12345cd /opt/wget https://github.com/openresty/lua-nginx-module/archive/v0.10.5.tar.gztar zxvf v0.10.5.tar.gz 4.Nginx配置4.1.下载nginx12345cd /opt/wget http://nginx.org/download/nginx-1.10.1.tar.gztar zxvf nginx-1.10.1.tar.gz 4.2.编译安装123456789# 导入环境变export LUAJIT_LIB=/usr/local/libexport LUAJIT_INC=/usr/local/include/luajit-2.0# 安装到/usr/local/nginx-1.10.1目录下./configure --prefix=/usr/local/nginx-1.10.1 --add-module=../lua-nginx-module-0.10.5make -j2make install 4.3.配置12cd /usr/local/nginx-1.10.1vi conf/nginx.conf 然后在http -&gt; server下加入配置： 1234location /lua_test &#123; default_type &apos;text/plain&apos;; content_by_lua &apos;ngx.say(&quot;hello, ttlsa lua&quot;)&apos;;&#125; 这个配置后，访问http://[hostname]:[port]/lua_test，就能访问你所定义的代码块了。 如果你还想修改nginx的http端口，修改一下http.server中的listen值，就可以了。 4.4.启动nginx1/usr/local/nginx-1.10.1/sbin/nginx 4.5.验证查看端口的运行情况： 12root@ubuntu:~# netstat -anp|grep 4002tcp 0 0 0.0.0.0:4002 0.0.0.0:* LISTEN 1736/nginx 或者直接请求4002端口。（我这里使用的是4002端口，你可以根据需要，设置为你需要的。） 12root@ubuntu:/usr/local/nginx-1.10.1/sbin# curl http://192.168.1.160:4002/lua_testhello, ttlsa lua 5.FAQ5.1.缺少pcre3 错误log 1234./configure: error: the HTTP rewrite module requires the PCRE library.You can either disable the module by using --without-http_rewrite_moduleoption, or install the PCRE library into the system, or build the PCRE librarystatically from the source with nginx by using --with-pcre=&lt;path&gt; option. 解决办法 1apt-get install libpcre3 libpcre3-dev 5.2.缺少libssl 错误log 1234./configure: error: the HTTP gzip module requires the zlib library.You can either disable the module by using --without-http_gzip_moduleoption, or install the zlib library into the system, or build the zlib librarystatically from the source with nginx by using --with-zlib=&lt;path&gt; option. 解决办法 1apt-get install libssl-dev 参考地址：http://www.ttlsa.com/nginx/nginx-modules-ngx_lua/","categories":[{"name":"nginx","slug":"nginx","permalink":"http://www.leocook.org/blog/categories/nginx/"}],"tags":[{"name":"nginx lua","slug":"nginx-lua","permalink":"http://www.leocook.org/blog/tags/nginx-lua/"}]},{"title":"Install_hadoop_cluster_on_CM","slug":"hadoop-2016-07-23-Install-hadoop-cluster-on-CM","date":"2016-07-22T16:00:00.000Z","updated":"2019-06-09T15:52:45.244Z","comments":true,"path":"2016/07/23/hadoop-2016-07-23-Install-hadoop-cluster-on-CM/","link":"","permalink":"http://www.leocook.org/2016/07/23/hadoop-2016-07-23-Install-hadoop-cluster-on-CM/","excerpt":"对于Hadoop这个复杂的大系统，我们期望能有一个平台，可以对Hadoop的每一个部件都能够进行安装部署，以及细颗粒度的监控。Apache发行版的Hadoop可以使用Ambari；Cloudera公司的CDH版本Hadoop则可以使用Cloudera Manager（后面简称为CM）来统一管理和部署。咱们这里的操作系统使用的是ubuntu14.04.","text":"对于Hadoop这个复杂的大系统，我们期望能有一个平台，可以对Hadoop的每一个部件都能够进行安装部署，以及细颗粒度的监控。Apache发行版的Hadoop可以使用Ambari；Cloudera公司的CDH版本Hadoop则可以使用Cloudera Manager（后面简称为CM）来统一管理和部署。咱们这里的操作系统使用的是ubuntu14.04. 1.Cloudera Manager安装前准备1.1. 操作系统的优化 打开的最大文件数修改当前的session配置(临时)： 1ulimit -SHn 65535 永久修改（需要重启服务器）： 12sudo echo &quot;ulimit -SHn 65535&quot; &gt;&gt; /etc/rc.localsudo chmod +x /etc/rc.local 打开的组大文件句柄数临时配置 12# 一般可不做修改，这是临时配置sudo echo 2000000 &gt; /proc/sys/fs/file-max 永久配置 1echo &quot;echo 2000000 &gt; /proc/sys/fs/file-max&quot; &gt;&gt; /etc/rc.local 或者 12sudo echo &quot;fs.file-max = 2000000&quot; &gt;&gt;/etc/sysctl.conf #推荐#使文件生效sudo /sbin/sysctl -p 打开的最大网络连接数 12sudo echo &quot;net.core.somaxconn = 2048&quot; &gt;&gt;/etc/sysctl.confsudo /sbin/sysctl -p 关闭selinux ubuntu默认是不安装的selinux的，所以这里可以直接忽略。 配置ntp 安装 1apt-get install ntp 如果只需要保证集群内部的各个server之间时间保持同步，只需要在需要同步的机器上配置： 10 */12 * * * ntpdate dt-vt-154 如果需要时间和互联网的时间保持一致，那么就需要在提供ntp server的机器上配置上层ntpserver: 123server [ntpserver_01]server [ntpserver_02]server [ntpserver_03] 关闭防火墙 12sudo ufw disable #关闭防火墙apt-get remove iptables #卸载防火墙 配置好hosts映射文件 123456vi /etc/hosts192.168.1.151 dt-vt-151192.168.1.152 dt-vt-152192.168.1.153 dt-vt-153192.168.1.154 dt-vt-154 Java环境配置 安装 123mkdir /opt/javacd /opt/java#下载到安装文件到这个目录 环境配置 12345vi /etc/profileexport JAVA_HOME=/opt/java/jdk1.7.0_79export CLASSPATH=.:$JAVA_HOME/libexport PATH=$JAVA_HOME/bin:$PATH 执行下面命令，使当前的session生效： 1source /etc/profile 1.2. 数据的存放在使用CM来管理集群的时候，会涉及到大量的数据存储，例如Hadoop的主机列表信息，主机的配置信息，负载信息，各个模块的运行时状态等等。咱们这里使用mysql来作为CM的数据存储，这里不仅是CM，Hadoop中的Hive等模块的元数据，都来使用mysql存储。 安装mysql 1sudo apt-get install mysql-server 我这里安装完后是5.5.49. 关闭mysql，备份配置文件 1/etc/init.d/mysql stop 把/var/lib/mysql/ib_logfile0和/var/lib/mysql/ib_logfile1拷贝至某个配置目录中，例如：/var/lib/mysql/bak 配置InnoDB引擎 务必使用InnoDB引擎引擎，若是使用MyISAM引擎CM将启动不了。在Mysql的命令行中运行下面的命令，来查看你的Mysql使用了哪个引擎。 1mysql&gt; show table status; 配置mysql的innodb刷写模式 1innodb_flush_method=O_DIRECT 即：配置Innodb的刷写模式为异步写模式。 修改mysql的最大连接数 1max_connections=1550 在这里，你应该会考虑配置该数值为多少比较合适。当集群规模小于50台的时候，假设该库中有N个数据库是用来服务于Hadoop的，那么max_connections可以设置为100N+50。例如：Cloudera Manager Server, Activity Monitor, Reports Manager, Cloudera Navigator, 和 Hive metastore都是使用mysql的，那么就配置max_connections为550.当集群规模*大于50台的时候，建议每个数据库只存放在一台机器上。 配置文件样例 1234567891011121314151617181920212223242526272829303132[mysqld]transaction-isolation = READ-COMMITTED# Disabling symbolic-links is recommended to prevent assorted security risks;# to do so, uncomment this line:# symbolic-links = 0key_buffer = 16Mkey_buffer_size = 32Mmax_allowed_packet = 32Mthread_stack = 256Kthread_cache_size = 64query_cache_limit = 8Mquery_cache_size = 64Mquery_cache_type = 1max_connections = 1550#expire_logs_days = 10#max_binlog_size = 100M# InnoDB settingsinnodb_file_per_table = 1innodb_flush_log_at_trx_commit = 2innodb_log_buffer_size = 64Minnodb_buffer_pool_size = 4Ginnodb_thread_concurrency = 8innodb_flush_method = O_DIRECTinnodb_log_file_size = 512M[mysqld_safe]log-error=/var/log/mysqld.logpid-file=/var/run/mysqld/mysqld.pid 启动mysql 1/etc/init.d/mysql start 打开开机自启 12apt-get install sysv-rc-confsysv-rc-conf mysql on 安装mysql-jdbc驱动 1apt-get install libmysql-java mysql远程连接 123vi /etc/mysql/my.cnfbind-address = 0.0.0.0 給相关服务创建mysql的数据库 相关列表如下： Role Database User Password Activity Monitor amon amon amon_password Reports Manager rman rman rman_password Hive Metastore Server hive_metastore hive hive_password Sentry Server sentry sentry sentry_password Cloudera Navigator Audit Server nav nav nav_password Cloudera Navigator Metadata Server navms navms navms_password OOZIE oozie oozie oozie 建表语句格式如下： 12create database database DEFAULT CHARACTER SET utf8;grant all on database.* TO &apos;user&apos;@&apos;%&apos; IDENTIFIED BY &apos;password&apos;; 建表语句如下： 1234567891011121314151617181920create database amon DEFAULT CHARACTER SET utf8;grant all on amon.* TO &apos;amon&apos;@&apos;%&apos; IDENTIFIED BY &apos;amon&apos;;create database rmon DEFAULT CHARACTER SET utf8;grant all on rmon.* TO &apos;rmon&apos;@&apos;%&apos; IDENTIFIED BY &apos;rmon&apos;;create database hive_metastore DEFAULT CHARACTER SET utf8;grant all on hive_metastore.* TO &apos;hive&apos;@&apos;%&apos; IDENTIFIED BY &apos;hive&apos;;create database sentry DEFAULT CHARACTER SET utf8;grant all on sentry.* TO &apos;sentry&apos;@&apos;%&apos; IDENTIFIED BY &apos;sentry&apos;;create database nav DEFAULT CHARACTER SET utf8;grant all on nav.* TO &apos;nav&apos;@&apos;%&apos; IDENTIFIED BY &apos;nav&apos;;create database navms DEFAULT CHARACTER SET utf8;grant all on navms.* TO &apos;navms&apos;@&apos;%&apos; IDENTIFIED BY &apos;navms&apos;;create database oozie DEFAULT CHARACTER SET utf8;grant all on oozie.* TO &apos;oozie&apos;@&apos;%&apos; IDENTIFIED BY &apos;oozie&apos;; 2.开始安装Cloudera Manager2.1.下载CM可以下载安装最新版本的CM： 1wget http://archive.cloudera.com/cm5/installer/latest/cloudera-manager-installer.bin 当然，如果你想选择安装其它版本，可以访问下面的地址，并选择下载你所需要的版本(cm5+)： 1http://archive.cloudera.com/cm5/installer/ 我这里使用的是版本是5.4.10，从release note上看，目前cdh5.4.10是最稳当的版本。我下载到了本地路径如下： 1/opt/cm/cloudera-manager-installer.bin 修改权限，使其可以被执行： 1chmod u+x cloudera-manager-installer.bin 2.2.配置私有软件仓库(如果不使用私有仓库，这里可以直接跳过)2.2.1.创建一个临时可以使用的远程仓库这个配置是在安装cloudera-manager-server的时候才会用的。这里的仓库是使用传统的http协议，通过网络传输数据的。可以去http://archive.cloudera.com/cm5/repo-as-tarball/下载你所需要的cdh包。 解压安装包 下载完安装包后，解压到某个目录下，我这里下载的是cm5.4.10-ubuntu14-04.tar.gz这个版本。 12tar -zxvf cm5.4.10-ubuntu14-04.tar.gzchmod -R ugo+rX /opt/cm/local_resp/cm 解压后的目录是/opt/cm/local_resp/cm 启动Http server 启动一个Http服务，使可以通过网络来访问仓库中的数据。 12cd /opt/cm/python -m SimpleHTTPServer 8900 我这里使用的是8900，你可以根据需要，使用指定的端口。 验证 可在浏览器中访问地址http://server:8900/cm，如果可以正常访问，并且能看到相对应的文件列表，则表示正常启动。 2.2.2.配置安装CM所需要的私有仓库在目录/etc/apt/sources.list.d/下创建文件my-private-cloudera-repo.list，并写入配置把这个文件和上面新建的仓库关联到一起： 123vi /etc/apt/sources.list.d/my-private-cloudera-repo.listdeb [arch=amd64] http://192.168.1.154:8900/cm/ trusty-cm5.4.10 contrib 执行下面的命令，使得上面的配置生效： 1sudo apt-get update 2.2.3.配置使用CM安装节点时会用到的仓库可以下载地址：http://archive.cloudera.com/cm5/ubuntu/trusty/amd64/cm/ 里的所有内容到本地，然后使用2.2.1中的方式来启动一个Http server。 2.2.4.配置节点的JDK这一步是可选的，如果你不想每台机器都去手动安装，也可以在后边使用CM来批量安装。 2.3.开始安装CM 连接互联网安装 1sudo ./cloudera-manager-installer.bin 或者 使用本地仓库安装 1sudo ./cloudera-manager-installer.bin --skip_repo_package=1 然后就是一路的YES &amp; NEXT，最后安装完成，CM默认的端口是7180，账户名和密码都是7180. 2.4.使用CM安装集群首次登陆CM管理界面的时候，会出现一个集群安装向导。我这里选择的是免费版本。然后大概有如下几步： 使用ip或者hostname来搜索主机，搜索到之后，go to next step. 看到如下图片的时候，如果你已经在前面的主机中安装好了JDK，那么这里可以不选，如果没有，则必须选择。 选择是否使用单用户模式（Single User Mode） 不使用该模式的话，HDFS服务会使用“hdfs”账户来启动，Hbase的Region Server会使用“hbase”账户来启动。使用了该模式之后，所有的服务都是使用同一个账户去启动的。这里主要看集群的使用场景，如果其中涉及到不同的模块是由不同人员来运维管理的话，我建议还是不要使用单用户模式了。但如果集群是统一由一个人员来管理，那么选择使用单用户模式可能会方便很多。我这里没有使用单用户模式。 配置好SSH登录 配置好SSH后开始连入主机，安装jdk和cm-agent 安装完成，此时cm-agent就已经安装好了，随时都可以使用cm-server控制安装Hadoop的相关组件,完成后先不要点击进入下一步，继续看下面。 分发Hadoop的安装包 去地址： 1http://archive.cloudera.com/cdh5/parcels/5.4.10/ 下载： 123CDH-5.4.10-1.cdh5.4.10.p0.16-trusty.parcelCDH-5.4.10-1.cdh5.4.10.p0.16-trusty.parcel.sha1manifest.json 下载完成后： 1把&quot;CDH-5.4.10-1.cdh5.4.10.p0.16-trusty.parcel.sha1&quot;重命名为&quot;CDH-5.4.10-1.cdh5.4.10.p0.16-trusty.parcel.sha&quot; 并移到目录： 1/opt/cloudera/parcel-repo 然后在cm-serve点击进入下一页，将会看到如下图：因为你已经把安装包下载好，并且放入到/opt/cloudera/parcel-repo（默认的目录）里面，所以这里的Download自然就是100%，Distributed是把安装包从cm-server往集群中各个节点分发的过程，Unpacked是表示各个节点的上安装包的解压情况进度，前面都OK后，Activated自然就可以了，表示安装包已经部署好了，可以随时进行安装。 选择安装Hadoop的那些组件之后，会在这里显示各个组件部署的主机地址。 这里配置的是一些组件使用的mysql信息： 然后可以配置一些服务的的具体参数 参数配置都没问题后，开始执行安装。 安装完成后。PS：我这里的Kafka是后来安装上去的，默认Hadoop的parcel包是没有kafka的。 3.注意事项3.1.主机的Host配置不能出差错我就配置错过host，期间走了一些弯路，主要表现在添加完主机之后，Hosts下的主机名都是localhost。 3.2.cm-agent安装失败重试时如果在重试的过程中出现了一直等待，或者cm-agent端口被占用的情况，大概有下面的几种情况： 锁文件没有删除 1sudo rm /tmp/.scm_prepare_node.lock 端口被占用 9000/9001 这种情况是部分进程没有关闭成功，找到端口对应的进程号，然后然后使用kill -9停止进程 12netstat -nap | grep 9000netstat -nap | grep 9001 主要是supervisor和cm-agent进程 4.会用到的一些地址总结4.1.Mysql的相关配置http://www.cloudera.com/documentation/enterprise/5-4-x/topics/cm_ig_mysql.html 4.2.创建本地仓库 For CM http://www.cloudera.com/documentation/enterprise/5-4-x/topics/cm_ig_create_local_package_repo.html For parcel http://www.cloudera.com/documentation/enterprise/5-4-x/topics/cm_ig_create_local_parcel_repo.html 4.3.parcel的下载地址http://archive.cloudera.com/cdh5/parcels/http://archive.cloudera.com/kafka/parcels/ 4.4.cm的下载地址加压后可以直接运行http://archive.cloudera.com/cm5/cm/5/ 5.附件博文中的图片是压缩后的，清晰度比较低，原图可以访问下面的地址： 链接: http://pan.baidu.com/s/1cmiBCE 密码: rnmw","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.leocook.org/blog/categories/hadoop/"}],"tags":[{"name":"cm hadoop","slug":"cm-hadoop","permalink":"http://www.leocook.org/blog/tags/cm-hadoop/"}]},{"title":"ELK安装配置介绍","slug":"ELK-2016-07-20-ELK安装配置介绍","date":"2016-07-19T16:00:00.000Z","updated":"2019-06-12T14:04:27.975Z","comments":true,"path":"2016/07/20/ELK-2016-07-20-ELK安装配置介绍/","link":"","permalink":"http://www.leocook.org/2016/07/20/ELK-2016-07-20-ELK安装配置介绍/","excerpt":"es是接触的比较早，在13年就做过相关开发，后来使用过ELK来做一些数据统计。最近打算从头来梳理一下这块的东西，今天就先从安装和配置开始吧。","text":"es是接触的比较早，在13年就做过相关开发，后来使用过ELK来做一些数据统计。最近打算从头来梳理一下这块的东西，今天就先从安装和配置开始吧。 版本列表 project version es 2.3.4 logstash 2.3.4 kibana 4.5.3 1.logstash配置1.1.Jdk安装http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html 1.2.下载logstashhttps://www.elastic.co/downloads/logstash 1.3.启动解压后可直接启动，不增加额外的配置也是能够启动成功的。启动方式有多种，这里举例说明。 使用-e指定启动的参数 1./logstash -e &apos;input &#123; stdin &#123; &#125; &#125; output &#123; stdout &#123;&#125; &#125;&apos; 这里设定stdin为输入，stdout为输出。 使用配置文件启动 1234567cat logstash-simple.confinput &#123; stdin &#123; &#125; &#125;output &#123; stdout &#123; codec=&gt; rubydebug &#125;&#125;./logstash agent -f logstash-simple.conf 1.4.验证启动后才命令行输入”hello World”，如下： 1234567891011root@dt-vt-153:/opt/logstash/logstash-2.3.4/bin# ./logstash agent -f logstash-simple.confSettings: Default pipeline workers: 1Pipeline main startedhello World&#123; &quot;message&quot; =&gt; &quot;hello World&quot;, &quot;@version&quot; =&gt; &quot;1&quot;, &quot;@timestamp&quot; =&gt; &quot;2016-07-18T07:20:20.526Z&quot;, &quot;host&quot; =&gt; &quot;0.0.0.0&quot;&#125; 打印出来的message部分显示为输入内容。 2.ES2.1.下载https://www.elastic.co/downloads/elasticsearch 2.2.配置配置一下主机的地址,这里不配置的话，只能在安装服务的宿主机上使用localhost来访问ES了。 12vi config/elasticsearch.ymlnetwork.host: 0.0.0.0 2.3.启动1/opt/elasticsearch/elasticsearch-2.3.4/bin/elasticsearch 2.4.插件安装kopf插件安装： 1/opt/elasticsearch/elasticsearch-2.3.4/bin/plugin install lmenezes/elasticsearch-kopf 安装完之后，可以访问web页面http://[hostname]:9200/_plugin/kopf查看。 2.5.验证es默认使用的9200端口，可使用下面的命令来查看该端口是否已经被监听： 1netstat -anp |grep :9200 或者使用浏览器访问端口9200. 3. Kibana3.1.下载https://www.elastic.co/downloads/kibana 3.2.配置配置一下es的地址。 123vi config/kibana.ymlelasticsearch.url: &quot;http://[hostname]:9200&quot; 这里默认是使用ES的数据。 3.3.启动1/opt/kibana/kibana-4.5.3-linux-x64/bin/kibana 3.4.验证访问： 1http://[hostname]:5601/ 点击”create”创建索引名称。 4.使ELK整体协作起来4.1.原理 logstash logstash主要用作收集数据使用，可以自由的定义数据的入口和出口，兼容多种数据源。 elasticsearch es和solr比较类似，都是基于lucene的来提供的搜索服务。但是在高并发的表现上，ES的负载均衡效果是优于solr的。 kibana kibana是一个可以可以用来查看ES里数据的Web。在早期logstash有一个logstash-web，但是功能比较简单。咱们这里说的kibana严格意义上说是kibana4，是在2015年重构完成的一个版本。 4.2.E和L的连接其实就是把logstash收集到的数据写入es中，这里只要在logstash的启动参数上做配置就可以了，具体的配置文件如下： 12345678910111213141516vi logstash-indexer.confinput &#123; file &#123; type =&gt;&quot;syslog&quot; path =&gt; [ &quot;/var/log/syslog&quot; ] &#125; syslog &#123; type =&gt;&quot;syslog&quot; port =&gt;&quot;5544&quot; &#125;&#125;output &#123; stdout &#123; codec=&gt; rubydebug &#125; elasticsearch &#123;hosts =&gt; &quot;localhost&quot; &#125;&#125; 配置介绍 logstash的配置里，一定要有一个input和一个output。file: 这里配置输入的文件信息。syslog：把logstash配置为一个可接收syslog服务器来接收file里变化的数据。output里定义了两处输出，分别是stdout命令行和elasticsearch。 启动 1nohup ./logstash agent -f logstash-indexer.conf &gt; nohup &amp; 4.3.Kibana的配置只需要修改kibana.yml中es的地址就可以了。 4.4.ESES在这个架构中作为数据存储和索引的系统。无额外的特殊配置。 5.小结ELK架构在处理运维系统的日志分析以及一些数据量不是很大的场景还是很实用的。快速、简单、易扩展，企业中使用可以考虑使用hdfs作为es的数据存储来使用，具体性能需要根据实际的业务复杂度来衡量，复杂度不是很高的海量数据统计，可优先考虑使用elk方案。 参考地址：http://baidu.blog.51cto.com/71938/1676798https://www.gitbook.com/book/chenryn/kibana-guide-cn/details","categories":[{"name":"ELK","slug":"ELK","permalink":"http://www.leocook.org/blog/categories/ELK/"}],"tags":[{"name":"es logstash kibana ELK","slug":"es-logstash-kibana-ELK","permalink":"http://www.leocook.org/blog/tags/es-logstash-kibana-ELK/"}]},{"title":"Tez系列第三篇-Tez和oozie整合","slug":"tez-2016-05-17-Tez系列第三篇-Tez和oozie整合","date":"2016-05-16T16:00:00.000Z","updated":"2019-06-09T15:37:37.124Z","comments":true,"path":"2016/05/17/tez-2016-05-17-Tez系列第三篇-Tez和oozie整合/","link":"","permalink":"http://www.leocook.org/2016/05/17/tez-2016-05-17-Tez系列第三篇-Tez和oozie整合/","excerpt":"1.完成上一篇的基础的相关配置2.拷贝Tez的依赖Jar包到OOZIE的HDFS共享目录下12hadoop fs -copyFromLocal *.jar /user/oozie/share/lib/lib_20150722203343/hive/hadoop fs -copyFromLocal /usr/lib/tez/lib/*.jar /user/oozie/share/lib/lib_20150722203343/hive/","text":"1.完成上一篇的基础的相关配置2.拷贝Tez的依赖Jar包到OOZIE的HDFS共享目录下12hadoop fs -copyFromLocal *.jar /user/oozie/share/lib/lib_20150722203343/hive/hadoop fs -copyFromLocal /usr/lib/tez/lib/*.jar /user/oozie/share/lib/lib_20150722203343/hive/ 3.修改Jar的权限保证oozie有权限读取、使用Jar包:1hadoop fs -chown oozie:oozie /user/oozie/share/lib/lib_20150722203343/hive/*.jar 4.是配置生效12oozie admin -sharelibupdateoozie admin -shareliblist hive 或者重启oozie也可以 5.在workflow里使用Tez这里咱们只是让oozie处理hive作业时使用Tez引擎，具体配置如下. 5.1.使单个workflow里的hive都用tez在作业流的hive-site.xml中加入下面的配置，即可使整个作业里的hive都使用Tez引擎： 123456789101112&lt;property&gt; &lt;name&gt;hive.execution.engine&lt;/name&gt; &lt;value&gt;tez&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;tez.lib.uris&lt;/name&gt; &lt;value&gt;$&#123;nameNode&#125;/tmp/apps/tez-0.8.2/,$&#123;nameNode&#125;/tmp/apps/tez-0.8.2/lib&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;tez.use.cluster.hadoop-libs&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 5.2.使单个workflow里的单个hive都用tez上面的配置不用加，在workflow.xml里的hive节点添加如下配置: 123456789101112&lt;property&gt; &lt;name&gt;hive.execution.engine&lt;/name&gt; &lt;value&gt;tez&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;tez.lib.uris&lt;/name&gt; &lt;value&gt;$&#123;nameNode&#125;/tmp/apps/tez-0.8.2/,$&#123;nameNode&#125;/tmp/apps/tez-0.8.2/lib&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;tez.use.cluster.hadoop-libs&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; hive.execution.engine属性可以不添加，在hive的脚本中的第一行添加:1set hive.execution.engine=tez;","categories":[{"name":"tez","slug":"tez","permalink":"http://www.leocook.org/blog/categories/tez/"}],"tags":[{"name":"hadoop tez oozie hive","slug":"hadoop-tez-oozie-hive","permalink":"http://www.leocook.org/blog/tags/hadoop-tez-oozie-hive/"}]},{"title":"Tez系列第二篇-hive_on_tez","slug":"tez-2016-05-09-Tez系列第二篇-hive-on-tez","date":"2016-05-08T16:00:00.000Z","updated":"2019-06-09T15:37:46.088Z","comments":true,"path":"2016/05/09/tez-2016-05-09-Tez系列第二篇-hive-on-tez/","link":"","permalink":"http://www.leocook.org/2016/05/09/tez-2016-05-09-Tez系列第二篇-hive-on-tez/","excerpt":"本文主要描述Tez的安装配置，以及使用Tez作为Hive的计算引擎时的相关配置。","text":"本文主要描述Tez的安装配置，以及使用Tez作为Hive的计算引擎时的相关配置。 1.安装配置Tez1.1.环境要求 CDH5.4.4(hadoop2.6.0) 编译环境：gcc, gcc-c++, make, build Nodejs、npm (Tez-ui需要) Git pb2.5.0 maven3 Tez0.8.2 1.2.集群准备以及安装完成的cdh5.4.4集群。 1.3. 编译环境准备安装gcc, gcc-c++, make, build1yum install gcc gcc-c++ libstdc++-devel make build 1.4. Nodejs、npm下载源码:1234wget http://nodejs.org/dist/v0.8.14/node-v0.8.14.tar.gz``` 解压后编译: ./configuremake &amp;&amp; make install12查看nodejs和npm的版本: node —versionnpm —version123456笔者的安装环境里，node的版本是v0.12.9，npm的版本是2.14.9### 1.5.安装GIT下载git: https://git-scm.com/download笔者选择的是1.7.3版本12解压后编译: ./configuremakemake install123### 1.6. ProtocolBuffer2.5.0- 下载pb源码 https://github.com/google/protobuf/releases/tag/v2.5.01- 编译安装pb ./configure -prefix=/opt/protoc/make &amp;&amp; make install12- 配置环境变量 export PROTOC_HOME=/opt/protocexport PATH=$PATH:$PROTOC_HOME/bin1- 检验 protoc —version12345查看pb的版本是不是2.5.0，笔者这里显示为 **libprotoc 2.5.0**### 1.7.编译&amp;安装Tez- 下载Tez Tez所有版本列表在者： http://tez.apache.org/releases/index.html12345笔者这里下载的是0.8.2版本。- 解压修改配置 vi pom.xml 2.6.0-cdh5.4.4&lt;/hadoop.version&gt;123 vi tez-ui/pom.xml v0.12.9 2.14.912- 开始编译 mvn clean package -DskipTests=true -Dmaven.javadoc.skip=true -Dfrontend-maven-plugin.version=0.0.231234567&gt;编译的过程中可能会发生错误，我这边由于网络故障，经常会出现node.gz.tar文件下载失败。最后还是编译成功了。## 2. 开始整合Hive和Tez### 2.1. 查看编译完成的目标目录结构 [wulin@lf-R710-29 target]$ lsarchive-tmp maven-archiver tez-0.8.2 tez-0.8.2-minimal tez-0.8.2-minimal.tar.gz tez-0.8.2.tar.gz tez-dist-0.8.2-tests.jar12### 2.1. 拷贝tez-0.8.2-minimal目录至HDFS hdfs dfs -put tez-0.8.2-minimal /tmp/tez-dir/123456先拷贝到tmp目录做测试，成功运行后在拷贝到正式目录。### 2.2. 拷贝对应以来jar把hadoop-mapreduce-client-common-2.6.0-cdh5.4.4.jar到hdfs的/tmp/tez-dir/tez-0.8.2-minimal目录### 2.3. 把tez-0.8.2拷贝到服务器本地部署的目录 cp -r tez-0.8.2 /opt/12### 2.4. 进入部署的目录创建conf/tez-site.xml &lt;?xml version=”1.0” encoding=”UTF-8”?&gt;&lt;?xml-stylesheet type=”text/xsl” href=”configuration.xsl”?&gt; tez.lib.uris ${fs.defaultFS}/tmp/tez-dir/tez-0.8.2-minimal,${fs.defaultFS}/tmp/tez-dir/tez-0.8.2-minimal/lib tez.use.cluster.hadoop-libs true 123&gt;注：tez.lib.uris参数值，是之前上传到hdfs目录的tez包，它必须是tez-0.8.2-minimal目录，而不能是tez-0.8.2目录。（如果有谁使用tez-0.8.2目录部署成功的话，可以告诉我，谢谢！）。根据官网的说明，使用tez-0.8.2-minimal包的时候，务必设置tez.use.cluster.hadoop-libs属性为true。### 2.4. 把Tez加入到环境变量 export TEZ_JARS=/opt/tez-0.8.2-minimalexport TEZ_CONF_DIR=$TEZ_JARS/confexport HADOOP_CLASSPATH=${TEZ_CONF_DIR}:${TEZ_JARS}/:${TEZ_JARS}/lib/12345&gt;注：经笔者的测试，TEZ_JARS指向tez-0.8.2-minimal目录或者tez-0.8.2目录都是可以的。### 2.5. 让Hive把Tez用起来- 配置整合临时配置 hive&gt;set hive.execution.engine=tez;1或者修改hive-site.xml（长期配置） hive.execution.engine tez 1- 执行hive，验证查看 hive&gt; select count() from dual;Query ID = wulin_20160406152121_6fd704e7-a437-4345-9958-2fbd1cccb057Total jobs = 1Launching Job 1 out of 1Number of reduce tasks determined at compile time: 1In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer=In order to limit the maximum number of reducers: set hive.exec.reducers.max=In order to set a constant number of reducers: set mapreduce.job.reduces=Starting Job = job_1457012272029_352465, Tracking URL = http://lfh-R710-165:8088/proxy/application_1457012272029_352465/Kill Command = /opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/bin/hadoop job -kill job_1457012272029_352465Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 12016-04-06 15:21:29,925 Stage-1 map = 0%, reduce = 0%2016-04-06 15:21:38,274 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 2.05 sec2016-04-06 15:21:45,611 Stage-1 map = 100%, reduce = 100%, Cumulative CPU 3.86 secMapReduce Total cumulative CPU time: 3 seconds 860 msecEnded Job = job_1457012272029_352465MapReduce Jobs Launched:Stage-Stage-1: Map: 1 Reduce: 1 Cumulative CPU: 3.86 sec HDFS Read: 6138 HDFS Write: 2 SUCCESSTotal MapReduce CPU Time Spent: 3 seconds 860 msecOK1Time taken: 35.934 seconds, Fetched: 1 row(s)hive&gt; set hive.execution.engine=tez;hive&gt; select count() from dual;Query ID = wulin_20160406152222_426dd505-1f6a-4d02-ae95-5a4d0e6bbc76Total jobs = 1Launching Job 1 out of 1 Status: Running (Executing on YARN cluster with App id application_1457012272029_352467) VERTICES STATUS TOTAL COMPLETED RUNNING PENDING FAILED KILLED Map 1 ………. SUCCEEDED 1 1 0 0 0 0 Reducer 2 …… SUCCEEDED 1 1 0 0 0 0VERTICES: 02/02 [==========================&gt;&gt;] 100% ELAPSED TIME: 9.64 s OK1Time taken: 22.211 seconds, Fetched: 1 row(s)hive&gt;12345678如下图：![Hive on Tez](http://leocook-blog.test.upcdn.net/tez-ok.png &quot;Hive on Tez&quot;)到此，hive on tez，整合完毕！## 常见的错误### 1.不要使用sudo权限来编译（编译tez时的错误） [ERROR] Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.3.2:exec (Bower install) on project tez-ui: Command execution failed. Process exited with an error: 1 (Exit value: 1) -&gt; [Help 1]1234解决办法：不要使用root用户，也不要使用sudo来编译.### 2.maven插件frontend-maven-plugin的版本问题（编译tez时的错误） [ERROR] Failed to execute goal com.github.eirslett:frontend-maven-plugin:0.0.22:install-node-and-npm (install node and npm) on project tez-ui: Execution install node and npm of goal com.github.eirslett:frontend-maven-plugin:0.0.22:install-node-and-npm failed: A required class was missing while executing com.github.eirslett:frontend-maven-plugin:0.0.22:install-node-and-npm: org/slf4j/helpers/MarkerIgnoringBase12345解决办法：强制执行编译时frontend-maven-plugin插件的版本（mvn clean package -DskipTests=true -Dmaven.javadoc.skip=true -Dfrontend-maven-plugin.version=0.0.XX）如果maven的版本低于3.1，frontend-maven-plugin版本应该 &lt;= 0.0.22；如果maven的版本大于或等于3.1，frontend-maven-plugin版本应该 &gt;= 0.0.23.### 3.解压Node压缩包时错误（编译tez时的错误） [ERROR] Failed to execute goal com.github.eirslett:frontend-maven-plugin:0.0.22:install-node-and-npm (install node and npm) on project tez-ui: Could not extract the Node archive: Could not extract archive: ‘/home/…/tez/tez-ui/src/main/webapp/node_tmp/node.tar.gz’: EOFException -&gt; [Help 1]123解决版本：检查第二个问题，并重新执行。如果还失败，可以多执行几次，可能和网络有关系。### 4.node&amp;npm版本不对应（编译tez时的错误） [ERROR] npm WARN engine hoek@2.16.3: wanted: {“node”:”&gt;=0.10.40”} (current: {“node”:”v0.10.18”,”npm”:”1.3.8”})[ERROR] npm WARN engine boom@2.10.1: wanted: {“node”:”&gt;=0.10.40”} (current: {“node”:”v0.10.18”,”npm”:”1.3.8”})[ERROR] npm WARN engine cryptiles@2.0.5: wanted: {“node”:”&gt;=0.10.40”} (current: {“node”:”v0.10.18”,”npm”:”1.3.8”})123解决办法：1. 安装正确版本的nodeJs；2. 修改tez-ui/pom.xml中的nodeVersion和npmVersion标签值为系统环境的值。可使用下面命令，查看系统里的node和npm版本： node —versionnpm —version12### 5.缺少MR的依赖包（error when run hive on tez） Vertex failed, vertexName=Map 1, vertexId=vertex_1457012272029_352429_1_00, diagnostics=[Vertex vertex_1457012272029_352429_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: dual initializer failed, vertex=vertex_1457012272029_352429_1_00 [Map 1], java.lang.NoClassDefFoundError: org/apache/hadoop/mapred/MRVersion at org.apache.hadoop.hive.shims.Hadoop23Shims.isMR2(Hadoop23Shims.java:843) at org.apache.hadoop.hive.shims.Hadoop23Shims.getHadoopConfNames(Hadoop23Shims.java:914) at org.apache.hadoop.hive.conf.HiveConf$ConfVars.(HiveConf.java:356) at org.apache.hadoop.hive.ql.exec.Utilities.getBaseWork(Utilities.java:371) at org.apache.hadoop.hive.ql.exec.Utilities.getMapWork(Utilities.java:296) at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:106) at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:278) at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:269) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:269) at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:253) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.mapred.MRVersion at java.net.URLClassLoader$1.run(URLClassLoader.java:366) at java.net.URLClassLoader$1.run(URLClassLoader.java:355) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:354) at java.lang.ClassLoader.loadClass(ClassLoader.java:425) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308) at java.lang.ClassLoader.loadClass(ClassLoader.java:358) … 17 more]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1457012272029_352429_1_01, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1457012272029_352429_1_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask12345这个错误是在配置完之后，运行hive时才会出现的。解决办法：拷贝mr依赖包至tez的hdfs目录中。笔者的环境是CDH5.4.4，所以把hadoop-mapreduce-client-common-2.6.0-cdh5.4.4.jar拷贝到hdfs的/tmp/tez-dir/tez-0.8.2-minimal目录，就解决问题了。### 6.tez&amp;hive on oozie 错误 Status: Running (Executing on YARN cluster with App id application_1461470184587_0770) Map 1: -/- Reducer 2: 0/1Status: FailedVertex failed, vertexName=Map 1, vertexId=vertex_1461470184587_0770_1_00, diagnostics=[Vertex vertex_1461470184587_0770_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: wl_manager_core_assembly initializer failed, vertex=vertex_1461470184587_0770_1_00 [Map 1], java.lang.IllegalArgumentException: Illegal Capacity: -1 at java.util.ArrayList.(ArrayList.java:142) at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:330) at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:306) at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:408) at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:129) at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:278) at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:269) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671) at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:269) at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:253) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1461470184587_0770_1_01, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1461470184587_0770_1_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTaskIntercepting System.exit(2)Failing Oozie Launcher, Main class [org.apache.oozie.action.hadoop.HiveMain], exit code [2]``` 参考链接：http://m.oschina.net/blog/421764http://duguyiren3476.iteye.com/blog/2214549https://cwiki.apache.org/confluence/display/TEZ/Build+errors+and+solutions","categories":[{"name":"tez","slug":"tez","permalink":"http://www.leocook.org/blog/categories/tez/"}],"tags":[{"name":"hadoop tez hive","slug":"hadoop-tez-hive","permalink":"http://www.leocook.org/blog/tags/hadoop-tez-hive/"}]},{"title":"Tez系列第一篇-基础常识","slug":"tez-2016-04-01-Tez系列第一篇-基础常识","date":"2016-03-31T16:00:00.000Z","updated":"2019-06-09T15:37:39.898Z","comments":true,"path":"2016/04/01/tez-2016-04-01-Tez系列第一篇-基础常识/","link":"","permalink":"http://www.leocook.org/2016/04/01/tez-2016-04-01-Tez系列第一篇-基础常识/","excerpt":"本文主要围绕着这么几个问题来展开：Tez是什么？为什么要有Tez？Tez能解决什么问题？ 1.Tez是什么1.1.介绍Tez目标是用来构建复杂的有向五环图数据处理程序。Tez项目目前是构建在YARN之上的。详情可以查看Tez的官网：http://tez.apache.org/","text":"本文主要围绕着这么几个问题来展开：Tez是什么？为什么要有Tez？Tez能解决什么问题？ 1.Tez是什么1.1.介绍Tez目标是用来构建复杂的有向五环图数据处理程序。Tez项目目前是构建在YARN之上的。详情可以查看Tez的官网：http://tez.apache.org/ 1.2.两大优势用户体验 使用API来自定义数据流 灵活的Input-Processor-Output运行模式 与计算的数据类型无关 简单的部署流程 计算性能 性能高于MapReduce 资源管理更加优化 运行时配置预加载 物理数据流动态运行 举例下图是一个基于MR的Hive/Pig的DAG数据流处理过程: 下图是一个基于Tez的Hive/Pig的DAG数据流处理过程: 2.为什么要有Tez2.1.YARN的AMYARN的每个作业在执行前都会先创建一个AM，然后才会开始正真的计算。这样处理小作业的时候，会有较大的延迟，而且还会造成极大的性能浪费。 2.2.YARN的资源无法重用在MR1中，用户可以开启JVM重用，用来降低作业延迟。但是在YARN中，每个作业的AM会先向RM申请资源（Container），申请到资源之后开始运行作业，作业处理完成后释放资源，期间没有资源重新利用的环节。这样会使作业大大的延迟。 2.3.YARN的DAG中间计算结果读写效率低下可以查看1.2中的图“基于MR的Hive/Pig的DAG数据流处理过程”，可以看出图中的每一节点都是把结果写到一个中间存储（HDFS/S3）中，下个节点从中间存储读取数据，再来继续接下来的计算。可见中间存储的读写性能对整个DAG的性能影响是很大的。如果使用Tez，则可以省去中间存储的读写，上个节点的输出可以直接重定向到下个节点的输入。 3.Tez能解决什么问题3.1.使用AM缓冲池实现AM的复用，AMPoolServer使用Tez后，yarn的作业不是先提交给RM了，而是提交给AMPS。AMPS在启动后，会预先创建若干个AM，作为AM资源池，当作业被提交到AMPS的时候，AMPS会把该作业直接提交到AM上，这样就避免每个作业都创建独立的AM，大大的提高了效率。 3.2.Container预启动AM缓冲池中的每个AM在启动时都会预先创建若干个container，以此来减少因创建container所话费的时间。 3.3.Container重用每个任务运行完之后，AM不会立马释放Container，而是将它分配给其它未执行的任务。看到这里， Tez是什么？为什么要有Tez？Tez能解决什么问题？应该都知道了吧！下一篇来开始讲解正式环境中的使用。","categories":[{"name":"tez","slug":"tez","permalink":"http://www.leocook.org/blog/categories/tez/"}],"tags":[{"name":"hadoop tez","slug":"hadoop-tez","permalink":"http://www.leocook.org/blog/tags/hadoop-tez/"}]},{"title":"hadoop优化-yarn","slug":"hadoop-2016-03-13-hadoop优化-yarn","date":"2016-03-12T16:00:00.000Z","updated":"2019-06-09T15:35:57.117Z","comments":true,"path":"2016/03/13/hadoop-2016-03-13-hadoop优化-yarn/","link":"","permalink":"http://www.leocook.org/2016/03/13/hadoop-2016-03-13-hadoop优化-yarn/","excerpt":"集群优化这块一直是一个比较麻烦的事情，目前由于集群的资源分配问题，已经出现了几次作业故障，有必要好这块的东西重新梳理一下。经过３天的测试，最终找到了对于目前环境相对适合的参数，目前集群已经11*24无节点故障了，先在这里做一些简单的分享吧。","text":"集群优化这块一直是一个比较麻烦的事情，目前由于集群的资源分配问题，已经出现了几次作业故障，有必要好这块的东西重新梳理一下。经过３天的测试，最终找到了对于目前环境相对适合的参数，目前集群已经11*24无节点故障了，先在这里做一些简单的分享吧。 1.之前的集群存在的问题1.1.问题一：作业的执行速率不同步 问题表现部分任务跑的慢，部分任务跑得快。 问题的原因集群资源分配不合理，出现有低配的机器运行作业数较多，高配机器运行作业数较少的情况。 解决办法重新分配角色组，使得低配机器参与相对较少的计算，高配机器参与相对较多的计算。 1.2.问题二：资源利用倾斜 问题表现部分机器资源利用率极高，部分机器资源利用率级低； 问题的原因同【问题一】 解决办法同【问题一】 1.3.问题三：集群资源并没有真正的参与计算 问题表现作业个数较多的时候，出现集群资源分配完了，但是集群负载极低，作业执行极缓慢。 问题的原因咱们的ETL结果报表使用的是单节点mysql，大量的小文件写操作使得磁盘的IO成为了严重的性能瓶颈，所以每个导数据的任务执行的较缓慢，导数据的作业长时间占用计算资源，计算任务执行的较为缓慢。 解决办法a). 把mysql中的数据库存到不同的磁盘上的，降低单个磁盘的负载。b). 减少单个任务的资源占用，提高集群的并行度。 2.集群资源重新划分的过程2.1.拿到集群中所有机器的硬件资源列表感谢运维同学的帮助！ 2.2.根据集群资源，分组的大概情况如截图：分组命名规则：NM: NodeManager；G01: Group01；C08: cpu是8核；M48: 内存是48GB。ZK: 机器上安装了ZK，如果没有这一项，默认该机器上只安装了HbaseRegion Server、HDFS DataNode、Impala Daemon和Yarn NodeManager这四个角色。（如果某台机器上只安装了一个测试的zk，则可忽略该角色的资源占用，若该角色占用资源较多，那么就应该把这台机器单独拿出来分组） 2.3.资源划分的策略根据机器上安装的服务，大概给服务做了如下的划分： 安装有重要服务的机器，可参与计算例如安装了OOZIE、ResourcesManager、NodeManager的节点，当它们故障时，对集群来说，将可能会是一场灾难，所以不让这些机器参与计算，保证这些服务的稳定。 安装有重要服务的机器，不参与计算例如安装了FLUME、KAFKA或ZK的节点，由于它们本身就是可以配置分布式执行的，当其中一个服务出现故障时，对业务的影响是较小的，甚至没有。所以允许这些节点和参与计算的节点安装在同一台主机上。 只安装了存储和计算的机器例如HbaseRegion Server、HDFS DataNode、Impala Daemon和Yarn NodeManager，不会因为一台机器的故障导致集群出现灾难。 2.4.具体的划分策略2.4.1.内存划分策略 yarn容器可直接管理的资源主机中内存*0.8 - 7GB（Hbase）- 7GB（HDFS），具体根据集群规模，hdfs、hbase的环境来定。有的几点还安装了其它服务，具体需要观察集群环境。 单个任务可使用的任务资源map任务划分1GB，reduce任务划分2GB，JVM虚拟机分别设置为他们70%。2.4.2.内存划分策略 yarn容器可直接管理的资源对于只安装了HRS、DN、ID、NM的节点，vcore总数设置为（cpu核数-1）的2倍，具体根据cpu的计算性能来定（减一时预留给系统的）。如果节点上安装了一些会消耗CPU的服务，那么就设置vcore总数为cpu核数/2。如果安装了一些对CPU消耗不是非常大的服务，例如ZK，那么就设置vcore总是为（cpu的核数-1）。 单个任务可使用的任务资源1个vcore。3.mysql调优的过程不同的库，挂载在不同的磁盘上，减小单块盘的压力。 4.成果4.1.集群表现情况到目前为止，已超过72小时NodeManager未出现过故障了，待考察一周。 4.2.mysql表现情况优化前的负载情况入下图：优化后的负载情况入下图： 后端导数据速度有明显加快，但是SDA盘的负载还是明显略高于SDB的负载。 5.总结在摸索这个问题上花费了比较多的时间，目前的优化方案满足现在的业务场景。在集群优化这方边，在个人现在能看到的未来，还有很多可以优化的项。例如： Impala的资源管理未使用yarn，所以一直还没有开始使用； OOZIE未做HA配置； HDFS数据平衡效果不是很好； CPU一个线程做两个vcore使用，压力还是比较大的。 在接下，将会按照优先级逐一解决。","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.leocook.org/blog/categories/hadoop/"}],"tags":[{"name":"hadoop 集群优化","slug":"hadoop-集群优化","permalink":"http://www.leocook.org/blog/tags/hadoop-集群优化/"}]},{"title":"hadoop优化-概述","slug":"hadoop-2016-02-23-hadoop优化-概述","date":"2015-02-22T16:00:00.000Z","updated":"2019-06-09T15:35:54.206Z","comments":true,"path":"2015/02/23/hadoop-2016-02-23-hadoop优化-概述/","link":"","permalink":"http://www.leocook.org/2015/02/23/hadoop-2016-02-23-hadoop优化-概述/","excerpt":"这篇文章是我开始涉及做集群相关优化时的第一篇笔记，内容比较浅显易懂，适合想了解集群优化的朋友阅读。","text":"这篇文章是我开始涉及做集群相关优化时的第一篇笔记，内容比较浅显易懂，适合想了解集群优化的朋友阅读。 1.应用程序角度进行优化1.1.减少不必要的reduce任务若对于同一份数据需要多次处理，可以尝试先排序、分区，然后自定义InputSplit将某一个分区作为一个Map的输入，在Map中处理数据，将Reduce的个数设置为空。 1.2.外部文件引用如字典、配置文件等需要在Task之间共享的数据，可使用分布式缓存DistributedCache或者使用-files 1.3.使用Combinercombiner是发生在map端的，作用是归并Map端输出的文件，这样Map端输出的数据量就小了，减少了Map端和reduce端间的数据传输。需要注意的是，Combiner不能影响作业的结果;不是每个MR都可以使用Combiner的，需要根据具体业务来定;Combiner是发生在Map端的，不能垮Map来执行（只有Reduce可以接收多个Map任务的输出数据） 1.4.使用合适的Writable类型尽可能使用二进制的Writable类型，例如：IntWritable， FloatWritable等，而不是Text。因为在一个批处理系统中将数值转换为文本时低效率的。使用二进制的Writable类型可以降低cpu资源的消耗，也可以减少Map端中间数据、结果数据占用的空间。 1.5.尽可能的少创建新的Java对象a)需要注意的Writable对象，例如下面的写法： public void map(...) { … for (String word : words) { output.collect(new Text(word), new IntWritable(1)); } } 这样会冲去创建对象new Text(word)和new IntWritable(1))，这样可能会产生海量的短周期对象。更高效的写法见下： class MyMapper … { Text wordText = new Text(); IntWritable one = new IntWritable(1); public void map(...) { for (String word: words) { wordText.set(word); output.collect(wordText, one); } } } b)对于可变字符串，使用StringBuffer而不是String String类是经过final修饰的，那么每次对它的修改都会产生临时对象，而SB则不会。 2. Linux系统层面上的配置调优2.1. 文件系统的配置a) 关闭文件在被操作时会记下时间戳:noatime和nodiratimeb) 选择I/O性能较好的文件系统（Hadoop比较依赖本地的文件系统） 2.2. Linux文件系统预读缓冲区大小命令: blockdev 2.3. 去除RAID和LVM2.4. 增大同时打开的文件数和网络连接数ulimit net.core.somaxconn ulimit net.core.somaxconn 2.5. 关闭swap分区在Hadoop中，对于每个作业处理的数据量和每个Task中用到的各种缓冲，用户都是完全可控的。 /etc/sysctl.conf 2.6. I/O调度器选择详情见AMD的白皮书 3. Hadoop平台内参数调优Hadoop相关可配置参数共有几百个，但是其中只有三十个左右会对其性能产生显著影响。 3.1. 计算资源优化a) 设置合理的slot（资源槽位） mapred.tasktracker.map.tasks.maximum / mapred.tasktracker.reduce.tasks.maximum 参数说明：每个TaskTracker上可并发执行的Map Task和Reduce Task数目默认值：都是2推荐值：根据具体的节点资源来看，推荐值是(core_per_node)/2~2*(cores_per_node)单位：无 3.2. 节点间的通信优化a) TaskTracker和JobTracker之间的心跳间隔这个值太小的话，在一个大集群中会造成JobTracker需要处理高并发心跳，可能会有很大的压力。建议集群规模小于300时，使用默认值3秒，在此基础上，集群规模每增加100台，会加1秒。b) 启用带外心跳(out-of-band heartbeat) mapreduce.tasktracker.outofband.heartbeat 参数说明：主要是为了减少任务分配延迟。它与常规心跳不同，一般的心跳是一定时间间隔发送的，而带外心跳是在任务运行结束或是失败时发送，这样就能在TaskTracker节点出现空闲资源的时候能第一时间通知JobTracker。 3.3. 磁盘块的配置优化a) 作业相关的磁盘配置 mapred.local.dir 参数说明：map本地计算时所用到的目录，建议配置在多块硬盘上b) 存储相关的磁盘配置（HDFS数据存储）dfs.data.dir参数说明：HDFS的数据存储目录，建议配置在多块硬盘上，可提高整体IO性能例如： &lt;property&gt; &lt;name&gt;dfs.name.dir&lt;/name&gt; &lt;value&gt;/data1/hadoopdata/mapred/jt/,/data2/hadoopdata/mapred/jt/&lt;/value&gt; &lt;/property&gt; c) 存储相关的磁盘配置（HDFS元数据存储） dfs.name.dir 参数说明：HDFS的元数据存储目录，建议设置多目录，每个多目录都可保存元数据的一个备份注：要想提升hadoop整体IO性能，对于hadoop中用到的所有文件目录，都需要评估它磁盘IO的负载，对于IO负载可能会高的目录，最好都配置到多个磁盘上，以提示IO性能 3.4. RPC Handler个数和Http线程数优化a) RPC Handler个数 mapred.job.tracker.handler.count 参数说明：JobTracker需要并发的处理来自各个TaskTracker的RPC请求，可根据集群规模和并发数来调整RPC Handler的个数。默认值：10推荐值：60-70，最少要是TaskTracker个数的4%单位：无b) Http线程数\u0000tasktracker.http.threads 在Shuffle阶段，Reduce Task会通过Http请求从各个TaskTracker上读取Map Task的结果，TaskTracker是使用Jetty Server来提供服务的，这里可适量调整Jetty Server的工作线程以提高它的并发处理能力。默认值：40推荐值：50-80+ 3.5. 选择合适的压缩算法mapred.compress.map.output / Mapred.output.compress map输出的中间结果时需要进行压缩的，指定压缩方式（Mapred.compress.map.output.codec/ Mapred.output.compress.codec）。推荐使用LZO压缩。 3.6. 启用批量任务调度(现在新版本都默认支持了)a) Fair Scheduler mapred.fairscheduler.assignmultiple b) Capacity Scheduler 3.7. 启用预读机制(Apache暂时没有)Hadoop是顺序读，所以预读机制可以很明显的提高HDFS的读性能。HDFS预读： dfs.datanode.readahead ：true dfs.datanode.readahead.bytes ：4MB shuffle预读 mapred.tasktracker.shuffle.fadvise : true mapred.tasktracker.shuffle.readahead.bytes : 4MB 3.8.HDFS相关参数优化1) dfs.replication参数说明：hdfs文件副本数默认值：3推荐值：3-5（对于IO较为密集的场景可适量增大）单位：无2) dfs.blocksize参数说明：默认值：67108864(64MB)推荐值：稍大型集群建议设为128MB(134217728)或256MB(268435456)单位：无3) dfs.datanode.handler.count参数说明：DateNode上的服务线程数默认值：10推荐值：单位：无4) fs.trash.interval参数说明：HDFS文件删除后会移动到垃圾箱，该参数时清理垃圾箱的时间默认值：0推荐值：1440(1day)单位：无5) io.sort.factor参数说明：当一个map task执行完之后，本地磁盘上(mapred.local.dir)有若干个spill文件，map task最后做的一件事就是执行merge sort，把这些spill文件合成一个文件（partition）。执行merge sort的时候，每次同时打开多少个spill文件由该参数决定。打开的文件越多，不一定merge sort就越快，所以要根据数据情况适当的调整。默认值：10推荐值：单位：无6) mapred.child.java.opts参数说明：JVM堆的最大可用内存默认值：-Xmx200m推荐值：-Xmx1G | -Xmx4G | -Xmx8G单位：-Xmx8589934592也行，单位不固定7) io.sort.mb参数说明：Map Task的输出结果和元数据在内存中占的buffer总大小，当buffer达到一定阀值时，会启动一个后台进程来对buffer里的内容进行排序，然后写入本地磁盘，形成一个split小文件默认值：100推荐值：200 | 800单位：兆8) io.sort.spill.percent参数说明：即io.sort.mb中所说的阀值默认值：0.8推荐值：0.8单位：无9) io.sort.record参数说明：io.sort.mb中分类给元数据的空间占比默认值：0.05推荐值：0.05单位：无10) Mapred.reduce.parallel参数说明：Reduce shuffle阶段copier线程数。默认是5，对于较大集群，可调整为16~25默认值：5推荐值：16~25单位：无 4.系统实现角度调优https://www.xiaohui.org/archives/944.html 主要针对HDFS进行优化，HDFS性能低下的两个原因：调度延迟和可移植性 4.1. 调度延迟关于调度延迟主要是发生在两个阶段：a) tasktracker上出现空余的slot到该tasktracker接收到新的task；b) tasktracker获取到了新的Task后，到连接上了datanode，并且可以读写数据。之所以说这两个阶段不够高效，因为一个分布式计算系统需要解决的是计算问题，如果把过多的时间花费在其它上，就显得很不合适，例如线程等待、高负荷的数据传输。下面解释下会经历上边两个阶段发生的过程：a) 当tasktracker上出现slot时，他会调用heartbeat方法向jobtracker发送心跳包（默认时间间隔是3秒，集群很大时可适量调整）来告知它，假设此时有准备需要执行的task，那么jobtracker会采用某种调度机制（调度机制很重要，是一个可以深度研究的东东）选择一个Task，然后通过调用heartbeat方法发送心跳包告知tasktracker。在该过程中，HDFS一直处于等待状态，这就使得资源利用率不高。b) 这个过程中所发生的操作都是串行化的tasktracker会连接到namenode上获取到自己需要的数据在datanode上的存储情况，然后再从datanode上读数据，在该过程中，HDFS一直处于等待状态，这就使得资源利用率不高。若能减短hdfs的等待时间;在执行task之前就开始把数据读到将要执行该task的tasktracker上，减少数据传输时间，那么将会显得高效很多。未解决此类问题，有这样几种解决方案：重叠I/O和CPU阶段（pipelining），task预取（task prefetching），数据预取（data prefetching）等。 4.2. 可移植性Hadoop是Java写的，所以可移植性相对较高。由于它屏蔽了底层文件系统，所以无法使用底层api来优化数据的读写。在活跃度较高的集群里（例如共享集群），大量并发读写会增加磁盘的随机寻道时间，这会降低读写效率;在大并发写的场景下，还会增加大量的磁盘碎片，这样将会大大的增加了读数据的成本，hdfs更适合文件顺序读取。对于上述问题，可以尝试使用下面的解决方案： tasktracker现在的线程模型是：one thread per client，即每个client连接都是由一个线程处理的（包括接受请求、处理请求，返回结果）。那么这一块一个拆分成两个部分来做，一组线程来处理和client的通信（Client Threads），一组用于数据的读写（Disk Threads）。 想要解决上述两个问题，暂时没有十全十美的办法，只能尽可能的权衡保证调度延迟相对较低+可移植性相对较高。 4.3. 优化策略：Prefetching与preshuffling a) Prefetching包括Block-intra prefetching和Block-inter prefetchingBlock-intra prefetching：对block内部数据处理方式进行了优化，即一边进行计算，一边预读将要用到的数据。这种方式需要解决两个难题：一个是计算和预取同步，另一个是确定合适的预取率。前者可以使用进度条（processing bar）的概念，进度条主要是记录计算数据和预读数据的进度，当同步被打破时发出同步失效的通知。后者是要根据实际情况来设定，可采用重复试验的方法来确定。Block-inter prefetching：在block层面上预读数据，在某个Task正在处理数据块A1的时候，预测器能预测接下来将要读取的数据块A2、A3、A4，然后把数据块A2、A3、A4预读到Task所在的rack上。 b) preshuffling数据被map task处理之前，由预测器判断每条记录将要被哪个reduce task处理，将这些数据交给靠近reduce task的map task来处理。 参考资料： cloudera官方文档http://blog.cloudera.com/blog/2009/12/7-tips-for-improving-mapreduce-performance/ AMD白皮书(较为实用)http://www.admin-magazine.com/HPC/content/download/9408/73372/file/Hadoop_Tuning_Guide-Version5.pdf 国内博客（大部分内容都是AMD白皮书上的翻译）：http://dongxicheng.org/mapreduce/hadoop-optimization-0/http://dongxicheng.org/mapreduce/hadoop-optimization-1/","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.leocook.org/blog/categories/hadoop/"}],"tags":[{"name":"集群优化","slug":"集群优化","permalink":"http://www.leocook.org/blog/tags/集群优化/"}]},{"title":"tomcat6集群配置","slug":"WebService-2014-05-28-tomcat6集群配置","date":"2014-05-27T16:00:00.000Z","updated":"2019-06-09T15:35:38.420Z","comments":true,"path":"2014/05/28/WebService-2014-05-28-tomcat6集群配置/","link":"","permalink":"http://www.leocook.org/2014/05/28/WebService-2014-05-28-tomcat6集群配置/","excerpt":"1.概要web容器在做集群配置时，有3点需要注意 负载均衡配置； session共享； 若做的是单机集群（多个tomcat安装在同一台机器上），需要注意端口冲突问题。 环境 tomcat6 apache mod_jk-1.2.28-httpd-2.2.3.so","text":"1.概要web容器在做集群配置时，有3点需要注意 负载均衡配置； session共享； 若做的是单机集群（多个tomcat安装在同一台机器上），需要注意端口冲突问题。 环境 tomcat6 apache mod_jk-1.2.28-httpd-2.2.3.so 2.Apache配置在本案例中是使用Apache来做的，下边用到的mod模块和Apache版本需要兼容才行。文中所使用的软件，会在文章底部附上下载链接。 2.1.http.conf配置修改在apache安装目录的conf目录下，修改http.conf配置文件，在任意某一行加入 1Include conf/mod_jk.conf 修改Apache监听端口（在文件的第46行左右） 1Listen 90 2.2.mod_jk.conf配置修改在conf目录下新建文件mod_jk.conf，写入如下内容 1234567#加载mod_jk Module 把mod_jk-1.2.28-httpd-2.2.3.so放到相应的目录中LoadModule jk_module modules/mod_jk-1.2.28-httpd-2.2.3.so#指定 workers.properties文件路径JkWorkersFile conf/workers.properties#指定那些请求交给tomcat处理,&quot;controller&quot;为在workers.properties里指定的负载分配控制器JkMount /*.jsp controllerJkMount /*.action controller 其中，mod_jk-1.2.28-httpd-2.2.3.so文件更具自己实际添加的模块文件名来写。 2.3.workers.properties配置修改在conf目录下新建文件workers.properties，写入如下内容 123456789101112131415161718worker.list=controller,tomcat1,tomcat2#tomcat1worker.tomcat1.port=8109 #ajp13端口号在tomcat下server.xml配置,默认8009 默认与HTTP通信的协议worker.tomcat1.host=localhost worker.tomcat1.type=ajp13 #tomcat的主机地址，如不为本机，请填写ip地址worker.tomcat1.lbfactor = 1 #server的加权比重，值越高，分得的请求越多#tomcat2worker.tomcat2.port=9109 worker.tomcat2.host=localhost worker.tomcat2.type=ajp13worker.tomcat2.lbfactor = 1 #========controller,负载均衡控制器========worker.controller.type=lb worker.controller.balanced_workers=tomcat1,tomcat2 #指定分担请求的tomcatworker.controller.sticky_session=1 其中tomcat上ajp13协议通信端口需要根据具体的设置区修改（ajp13协议在这里就是Apache与tomcat之间的通信协议）。 3.Tomcat配置3.1 tomcat的关闭端口默认是8005，若同一台机器上配置了多个tomcat，这里必须要修改： 1&lt;Server port=&quot;8005&quot; shutdown=&quot;SHUTDOWN&quot;&gt; 3.2.tomcat的服务端口默认是8080，若同一台机器上配置了多个tomcat，这里必须要修改： 123&lt;Connector port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot; connectionTimeout=&quot;20000&quot; redirectPort=&quot;8443&quot; /&gt; 3.3.AJP13通信端口tomcat上的AJP13协议通信端口：默认是8009，这里的端口配置需要和上边文件workers.properties中的AJP13协议端口配置相对应，若同一台机器上配置了多个tomcat，这里必须要修改： 1&lt;Connector port=&quot;8009&quot; protocol=&quot;AJP/1.3&quot; redirectPort=&quot;8443&quot; /&gt; 3.4.Session共享 在Tomcat中启用标签 1&lt;Cluster className=&quot;org.apache.catalina.ha.tcp.SimpleTcpCluster&quot;/&gt; 在web项目中的web.xml中加入 1&lt;distributable/&gt; 则这个项目就支持集群了。到此，配置完毕。下载链接： http://pan.baidu.com/s/1mgLq8Dq","categories":[{"name":"WebService","slug":"WebService","permalink":"http://www.leocook.org/blog/categories/WebService/"}],"tags":[{"name":"tomcat","slug":"tomcat","permalink":"http://www.leocook.org/blog/tags/tomcat/"}]},{"title":"ubuntu常用配置","slug":"linux-2014-05-15-ubuntu常用配置","date":"2014-05-14T16:00:00.000Z","updated":"2019-06-09T15:36:57.889Z","comments":true,"path":"2014/05/15/linux-2014-05-15-ubuntu常用配置/","link":"","permalink":"http://www.leocook.org/2014/05/15/linux-2014-05-15-ubuntu常用配置/","excerpt":"在使用ubuntu作为自己的开发环境时，可能会用到下面的这些小技巧哦！","text":"在使用ubuntu作为自己的开发环境时，可能会用到下面的这些小技巧哦！ 1.ubuntu desktop接双显示器1.1.环境 ubuntu12.04 Arandr 1.2.修改系统配置 编辑/etc/X11/xorg.conf文件 1vi /etc/X11/xorg.conf 在Section “Screen”内容之内增加以下内容 123SubSection &quot;Display&quot;Virtual 4000 3000EndSubSection 重启xwindow 方法一：快捷键是Ctrl + Alt + Backspace;(这个快捷键默认是关闭的，在快捷键设置中可以更改). 方法二：1sudo init 6 1.3.使用可视化软件Arandr 安装Arandr 1sudo apt-get install arandr 打开查看各个主屏的名称 设置对应的显示设备为主屏 1sudo xrandr --output CRT1 --primary 聪明的你一定会知道要设置LVDS为主显示器需要怎么做！Enjoy it！ 2.最好用的终端Terminator1sudo apt-get install terminator 3.视频播放器SMPlay123sudo apt-get install mplayer sudo apt-get install ffmpegsudo apt-get install smplayer","categories":[{"name":"linux","slug":"linux","permalink":"http://www.leocook.org/blog/categories/linux/"}],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"http://www.leocook.org/blog/tags/ubuntu/"}]}]}