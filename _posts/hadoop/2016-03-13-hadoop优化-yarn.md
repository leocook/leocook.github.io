---
layout: post
comments: true
date: 2016-03-13
categories: hadoop
tags: hadoop 集群优化
---

* content
{:toc}

集群优化这块一直是一个比较麻烦的事情，目前由于集群的资源分配问题，已经出现了几次作业故障，有必要好这块的东西重新梳理一下。经过３天的测试，最终找到了对于目前环境相对适合的参数，目前集群已经11*24无节点故障了，先在这里做一些简单的分享吧。

# 1.之前的集群存在的问题

## 1.1.问题一：作业的执行速率不同步
- 问题表现
部分任务跑的慢，部分任务跑得快。
- 问题的原因
集群资源分配不合理，出现有低配的机器运行作业数较多，高配机器运行作业数较少的情况。
- 解决办法
重新分配角色组，使得低配机器参与相对较少的计算，高配机器参与相对较多的计算。

## 1.2.问题二：资源利用倾斜
- 问题表现
部分机器资源利用率极高，部分机器资源利用率级低；
- 问题的原因
**同【问题一】**
- 解决办法
**同【问题一】**

## 1.3.问题三：集群资源并没有真正的参与计算
- 问题表现
作业个数较多的时候，出现集群资源分配完了，但是集群负载极低，作业执行极缓慢。
- 问题的原因
咱们的ETL结果报表使用的是单节点mysql，大量的小文件写操作使得磁盘的IO成为了严重的性能瓶颈，所以每个导数据的任务执行的较缓慢，导数据的作业长时间占用计算资源，计算任务执行的较为缓慢。
- 解决办法
a). 把mysql中的数据库存到不同的磁盘上的，降低单个磁盘的负载。
b). 减少单个任务的资源占用，提高集群的并行度。

# 2.集群资源重新划分的过程

## 2.1.拿到集群中所有机器的硬件资源列表
感谢运维同学的帮助！

## 2.2.根据集群资源，分组的大概情况如截图：
![服务器分组情况](http://7xriy2.com1.z0.glb.clouddn.com/%E5%88%86%E7%BB%84%E5%88%97%E8%A1%A8.png "根据机器硬件资源情况，服务器分组情况" )
分组命名规则：
***NM***: NodeManager；    
***G01***: Group01；    
***C08***: cpu是8核；    
***M48***: 内存是48GB。    
***ZK***: 机器上安装了ZK，如果没有这一项，默认该机器上只安装了HbaseRegion Server、HDFS DataNode、Impala Daemon和Yarn NodeManager这四个角色。（如果某台机器上只安装了一个测试的zk，则可忽略该角色的资源占用，若该角色占用资源较多，那么就应该把这台机器单独拿出来分组）

## 2.3.资源划分的策略
根据机器上安装的服务，大概给服务做了如下的划分：
- 安装有重要服务的机器，可参与计算
例如安装了OOZIE、ResourcesManager、NodeManager的节点，当它们故障时，对集群来说，将可能会是一场灾难，所以不让这些机器参与计算，保证这些服务的稳定。

- 安装有重要服务的机器，不参与计算
例如安装了FLUME、KAFKA或ZK的节点，由于它们本身就是可以配置分布式执行的，当其中一个服务出现故障时，对业务的影响是较小的，甚至没有。所以允许这些节点和参与计算的节点安装在同一台主机上。

- 只安装了存储和计算的机器
例如HbaseRegion Server、HDFS DataNode、Impala Daemon和Yarn NodeManager，不会因为一台机器的故障导致集群出现灾难。

## 2.4.具体的划分策略

### 2.4.1.内存划分策略
- yarn容器可直接管理的资源
主机中内存*0.8 - 7GB（Hbase）- 7GB（HDFS），具体根据集群规模，hdfs、hbase的环境来定。有的几点还安装了其它服务，具体需要观察集群环境。
- 单个任务可使用的任务资源
map任务划分1GB，reduce任务划分2GB，JVM虚拟机分别设置为他们70%。
### 2.4.2.内存划分策略
- yarn容器可直接管理的资源
对于只安装了HRS、DN、ID、NM的节点，vcore总数设置为（cpu核数-1）的2倍，具体根据cpu的计算性能来定（减一时预留给系统的）。如果节点上安装了一些会消耗CPU的服务，那么就设置vcore总数为cpu核数/2。如果安装了一些对CPU消耗不是非常大的服务，例如ZK，那么就设置vcore总是为（cpu的核数-1）。
- 单个任务可使用的任务资源
1个vcore。
# 3.mysql调优的过程
不同的库，挂载在不同的磁盘上，减小单块盘的压力。

# 4.成果

## 4.1.集群表现情况
到目前为止，已超过72小时NodeManager未出现过故障了，待考察一周。
## 4.2.mysql表现情况
优化前的负载情况入下图：
![优化前](http://7xriy2.com1.z0.glb.clouddn.com/mysql_befor.png "优化前负载情况")
优化后的负载情况入下图：
![优化后](http://7xriy2.com1.z0.glb.clouddn.com/mysql.png "优化后负载情况")

>后端导数据速度有明显加快，但是SDA盘的负载还是明显略高于SDB的负载。

    
# 5.总结
在摸索这个问题上花费了比较多的时间，目前的优化方案满足现在的业务场景。在集群优化这方边，在个人现在能看到的未来，还有很多可以优化的项。例如：
- Impala的资源管理未使用yarn，所以一直还没有开始使用；
- OOZIE未做HA配置；
- HDFS数据平衡效果不是很好；
- CPU一个线程做两个vcore使用，压力还是比较大的。

在接下，将会按照优先级逐一解决。

